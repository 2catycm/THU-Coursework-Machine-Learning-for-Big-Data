[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "THU-Coursework-Machine-Learning-for-Big-Data",
    "section": "",
    "text": "This file will become your README and also the index of your documentation.",
    "crumbs": [
      "理论作业 Theory Assignments",
      "THU-Coursework-Machine-Learning-for-Big-Data"
    ]
  },
  {
    "objectID": "index.html#developer-guide",
    "href": "index.html#developer-guide",
    "title": "THU-Coursework-Machine-Learning-for-Big-Data",
    "section": "Developer Guide",
    "text": "Developer Guide\n如果你想加入我们一起开源作业，请阅读以下指南。\nIf you are new to using nbdev here are some useful pointers to get you started.\n\n关于Quarto和nbdev一些需要配置的地方\nnbdev_install_quarto\nquarto install tinytex\nquarto install chromium\nsudo apt-get install librsvg2-bin\n\n\n关于nbdev、quarto+pandoc 这一套系统支持和不支持的markdown与latex语法\n\nlatex公式：\n\n不能用””\n对于align公式,似乎都失败了 align, aligned和aligned*, 参考\n\n\n\n\nInstall THU_Coursework_Machine_Learning_for_Big_Data in Development mode\n# make sure THU_Coursework_Machine_Learning_for_Big_Data package is installed in development mode\n$ pip install -e .\n\n# make changes under nbs/ directory\n# ...\n\n# compile to have changes apply to THU_Coursework_Machine_Learning_for_Big_Data\n$ nbdev_prepare",
    "crumbs": [
      "理论作业 Theory Assignments",
      "THU-Coursework-Machine-Learning-for-Big-Data"
    ]
  },
  {
    "objectID": "index.html#usage",
    "href": "index.html#usage",
    "title": "THU-Coursework-Machine-Learning-for-Big-Data",
    "section": "Usage",
    "text": "Usage\n我们在学习《大数据机器学习》课程做作业的同时，也形成了一个简单的机器学习库，对李航书上的部分代码做了实现和可视化，你可以通过安装我们的库来复用我们写的代码逻辑。\n\nInstallation\nInstall latest from the GitHub repository:\n$ pip install git+https://github.com/Open-Book-Studio/THU-Coursework-Machine-Learning-for-Big-Data.git\nor from conda\n$ conda install -c yecanming6666 thu_big_data_ml\nor from pypi\n$ pip install thu_big_data_ml\n\n\nDocumentation\nDocumentation can be found hosted on this GitHub repository’s pages. Additionally you can find package manager specific guidelines on conda and pypi respectively.",
    "crumbs": [
      "理论作业 Theory Assignments",
      "THU-Coursework-Machine-Learning-for-Big-Data"
    ]
  },
  {
    "objectID": "index.html#how-to-use",
    "href": "index.html#how-to-use",
    "title": "THU-Coursework-Machine-Learning-for-Big-Data",
    "section": "How to use",
    "text": "How to use\nFill me in please! Don’t forget code examples:\n\n1+1\n\n2",
    "crumbs": [
      "理论作业 Theory Assignments",
      "THU-Coursework-Machine-Learning-for-Big-Data"
    ]
  },
  {
    "objectID": "core.html",
    "href": "core.html",
    "title": "core",
    "section": "",
    "text": "source\n\nfoo\n\n foo ()",
    "crumbs": [
      "理论作业 Theory Assignments",
      "core"
    ]
  },
  {
    "objectID": "theory_assignments/A2/p_assignment2_yecanming.html#sec-1",
    "href": "theory_assignments/A2/p_assignment2_yecanming.html#sec-1",
    "title": "大数据机器学习课程第二次作业",
    "section": "1 第一题",
    "text": "1 第一题\n题目如下\n\nMinsky与Papert指出：感知机因为是线性模型，所以不能表示复杂的函数，如异或(XOR)。验证感知机为什么不能表示异或。\n\n\n\n\n\n\n\n注记\n\n\n\nTL; DR 前面审题内容较长，学习了一些这道题的一些背景知识方便理解。 对于题目的证明，可以直接跳到解题部分@sec-proof。\n\n\n\n\n\n\n\ngraph LR\n    A[马文-明斯基]\n    B[美国科学家]\n    C[人工智能之父]\n    D[专长于认知科学]\n    E[与麦卡锡共同发起达特茅斯会议]\n    F[提出人工智能概念]\n    G[框架理论创立者]\n    H[出生-1927年8月9日]\n    I[逝世-2016年1月24日]\n    J[西蒙-派珀特]\n    K[美国麻省理工学院终身教授]\n    L[教育信息化奠基人]\n    M[数学家-计算机科学家-心理学家-教育家]\n    N[近代人工智能领域先驱者之一]\n    O[逝世-2016年7月31日]\n\n    A---B\n    A---C\n    A---D\n    A---E\n    A---F\n    A---G\n    A---H\n    A---I\n    J---K\n    J---L\n    J---M\n    J---N\n    J---O\n\n\n\n\n\n\n\n\n\n他们在什么时候，什么地方指出了上述观点？\n\n在1969年，Minsky和Papert发表了《Perceptrons》一书，探讨了单层感知器的局限性，特别是无法解决线性不可分问题，例如异或问题。 (Marvin 和 Seymour 1969)。这本书从理论上否定了神经网络的研究价值，并对神经网络的发展产生了深远的影响。\n\n\n\n\n\n\n\n\n注记\n\n\n\nMinsky和Papert的观点导致了神经网络研究进入低迷期。而实际上Minsky本人并没有看衰神经网络，只是他的书被人误解以为神经网络一无是处。实际上他写书的时候，MLP的训练算法已经出现了，但是他在书中没有提及(人民邮电出版社 2020)。\n\n\n\n1.1 解题\n\n1.1.1 感知机是什么？感知机是线性模型吗？\n根据李航课本内容(李航 2019)，感知机是一种简单的线性二分类模型，由Rosenblatt在1957年提出。它的数学形式可以表示为：\n\\[ f(x) = sign(w \\cdot x + b)\n\\tag{1}\\]\n在 式 1 中，\\(w\\) 和 \\(b\\) 是模型参数，\\(x\\) 是输入向量，\\(sign\\) 是符号函数，\\(f(x)\\) 是模型输出, f就是感知机。\n\\(f(x)\\) 本身当然不是一个线性函数，因为他输出的是一个决策。 我们换一个角度思考，我们有线性方程式 2 \\[  w \\cdot x + b =0\n\\tag{2}\\] 式 2表示了输入空间中的一个超平面， \\(w\\) 和 \\(b\\) 是该超平面的法向量和截距。 这个超平面能把输入空间划分为两部分，分为正类和负类，称为分离超平面。 线性方程表示了超平面，所以我们说感知机是线性模型。\n\n\n代码\nfrom sklearn.datasets import make_blobs\nfrom sklearn.linear_model import Perceptron\nimport numpy as np\nimport matplotlib.pyplot as plt\nplt.style.use('default')\nfrom mpl_toolkits.mplot3d import Axes3D\n\n# 生成线性可分的数据集\nX, y = make_blobs(n_samples=100, centers=2, n_features=3, random_state=42)\n\n# 使用感知机模型进行学习\nclf = Perceptron(random_state=42, max_iter=1000)\nclf.fit(X, y)\n\n# 获取感知机的权重和偏置\nw = clf.coef_[0]\nb = clf.intercept_[0]\n\n# 创建一个3D图形来展示数据和感知机超平面\nfig = plt.figure()\nax = fig.add_subplot(111, projection='3d')\n\n# 绘制数据点\nax.scatter(X[y == 0][:, 0], X[y == 0][:, 1], X[y == 0][:, 2], color='red', label='Class 0')\nax.scatter(X[y == 1][:, 0], X[y == 1][:, 1], X[y == 1][:, 2], color='blue', label='Class 1')\n\n# 计算超平面上的两个点用于绘制超平面\nxx, yy = np.meshgrid(range(-10, 11), range(-10, 11))\nzz = (-b - w[0] * xx - w[1] * yy) / w[2]\n\n# 绘制感知机超平面\nax.plot_surface(xx, yy, zz, alpha=0.2)\n\n# 设置图形属性\nax.set_xlabel('X1')\nax.set_ylabel('X2')\nax.set_zlabel('X3')\nax.legend()\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n1.1.2 异或函数是什么呢？\n异或（XOR）是一个布尔函数，它返回两个输入值不同则为真（1），相同则为假（0）。 数理逻辑中分为模型论和证明论，我们从模型论的角度来说异或函数可以直接真值表表示如下：\n\n\n\n\n\n\n\n\nA\nB\nXOR(A, B)\n\n\n\n\n0\n0\n0\n\n\n0\n1\n1\n\n\n1\n0\n1\n\n\n1\n1\n0\n\n\n\n\n\n1.1.3 异或函数和分类问题的关系？\n这里我们在关注感知机能不能表示异或函数。所以我们要先把异或问题广义化为二分类问题，输入的A、B两个特征，原本AB的类型是bool，现在我们扩展一下认为异或的输入AB可以是任何实数。而原本输出的0和1正好对应正例和负例，感知机的\\(sign\\)也是输出0或者1，所以我们可以把异或问题转化为感知机的二分类问题。 其他实数上异或的结果仍然是0或者1，但是无论模型输出什么都认为是正确的就好。 我们可以可视化一下现在的异或问题\n\n\n代码\n# 绘制异或（XOR）函数的图像\n\n# XOR函数的数据点\nx = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\ny = np.array([0, 1, 1, 0])\n\n# 创建图形和轴\nfig, ax = plt.subplots()\n\n# 绘制数据点\nax.scatter(x[y == 0][:, 0], x[y == 0][:, 1], color='red', label='0')\nax.scatter(x[y == 1][:, 0], x[y == 1][:, 1], color='blue', label='1')\n\n# 设置图形属性\nax.set_xlim(-0.5, 1.5)\nax.set_ylim(-0.5, 1.5)\nax.set_xlabel('X1')\nax.set_ylabel('X2')\nax.legend()\nax.set_title('XOR Function')\n\n# 显示图形\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n1.1.4 验证感知机为什么不能表示异或\n要验证感知机不能表示异或，我们需要证明： &gt; 不\\(\\exists\\)一个\\(w\\)和\\(b\\), 使得\\(f(x) = \\text{sign}(w^Tx + b)\\)可以完美分离异或二分类问题的正负实例。\n我们可以使用反证法来证明这个结论。 假设存在权重 $ w_1, w_2 $ 和偏置 $ b $ 可以表示异或函数，那么对于异或函数的每个输入组合，感知机的输出应该满足以下条件：\n\n$ w_1 + w_2 + b $ （对应于点（0,0），输出应为0，sign里面的值应该小于0）\n$ w_1 + w_2 + b &gt; 0 $ （对应于点（0,1），输出应为+1）\n$ w_1 + w_2 + b &gt; 0 $ （对应于点（1,0），输出应为+1）\n$ w_1 + w_2 + b $ （对应于点（1,1），输出应为0）\n\n我们来化简一下这些不等式，我们使用sympy库来进行符号运算：\n\n\n代码\nimport sympy as sp\nfrom IPython.display import display, Latex\n\n# 定义变量\nw1, w2, b, x1, x2 = sp.symbols('w_1 w_2 b x_1 x_2')\n\n# XOR函数的四个点\npoints = [(0, 0, -1), (0, 1, 1), (1, 0, 1), (1, 1, -1)]\n\n# 建立不等式\ninequalities = []\nfor x1_val, x2_val, y_val in points:\n    # 感知机模型: sign(w1*x1 + w2*x2 + b)\n    # 对于每个点，根据y_val的正负建立不等式\n    if y_val &gt; 0:\n        inequalities.append(w1*x1_val + w2*x2_val + b &gt; 0)\n    else:\n        inequalities.append(w1*x1_val + w2*x2_val + b &lt;= 0)\n\n# 化简不等式\nsimplified_inequalities = [sp.simplify(ineq) for ineq in inequalities]\n# for ineq in simplified_inequalities:\n#     # display(Latex(sp.latex(ineq)))\n#     print(sp.latex(ineq))\nsimplified_inequalities\n\n\n[b &lt;= 0, b &gt; -w_2, b &gt; -w_1, b &lt;= -w_1 - w_2]\n\n\n我们进一步化简，观察b这个变量\n\n\n代码\nsolutions = sp.solve(simplified_inequalities, b)\nsolutions\n\n\n\\(\\displaystyle b \\leq 0 \\wedge b \\leq - w_{1} - w_{2} \\wedge b &gt; - w_{1} \\wedge b &gt; - w_{2} \\wedge -\\infty &lt; b\\)\n\n\n而由于\n\n\n代码\nfrom sympy.logic.boolalg import simplify_logic\ntherom = (simplified_inequalities[0] & simplified_inequalities[1] & simplified_inequalities[2])&gt;&gt;(~simplified_inequalities[3])\ntherom\n\n\n\\(\\displaystyle \\left(b \\leq 0 \\wedge b &gt; - w_{1} \\wedge b &gt; - w_{2}\\right) \\Rightarrow b &gt; - w_{1} - w_{2}\\)\n\n\n与 \\(b \\leq -w_1-w_2\\) 矛盾，所以假设不成立，原命题成立，即感知机不能表示异或。\n注意上面的逻辑表达式需要一定的推导，直接看是不一定能发现前3式和第4式的矛盾。 1. 首先发现\\(w_1, w_2 \\geq 0\\) 2. 然后发现\\(w_1+b \\gt 0\\) 3. 所以 \\(w_1+w_2+b \\gt 0\\) 4. 所以矛盾。\n因为人工观察不等式其实是启发式地去找矛盾，如果我们一开始不知道异或问题表达不了，我们可能一时看不出来这个问题。 我们可以使用人工智能的一个分支——逻辑智能体（Logical Agent）来进行自动定理发现，能够在形式系统中发现矛盾。\n具体来说，我们要把问题转化为约束可满足问题，然后使用forward/backward chaining、DPLL等算法求解。\n我们先试一下sympy的satisfiable函数，看看能不能求解出这个逻辑表达式。\n\n\n代码\nfrom sympy import symbols, And, Not, Implies\nfrom sympy.logic.boolalg import to_cnf\nfrom sympy.logic.inference import satisfiable\n# 将表达式转换为 CNF\ncnf_expr = to_cnf(And(*[Not(x) for x in simplified_inequalities]))\n\n# 检查逻辑表达式的可满足性\nresult = satisfiable(cnf_expr)\nprint(result)  # 如果表达式不可满足，输出 False\n\n\n{Q.gt(b, -w_1 - w_2): True, Q.le(b, -w_1): True, Q.le(b, -w_2): True, Q.gt(b, 0): True}\n\n\n看来不行，我们换一个库Z3 Theorem Prover，它由Microsoft开发，可以处理多种逻辑，包括线性不等式。\n首先要安装一下这个库，非常简单\n$ pip install z3-solver\n\n\n代码\nfrom z3 import *\nset_param(proof=True)\nctx = Context()\ns = Solver(ctx=ctx, logFile=\"log_theorem_prover.txt\")\nb, w_1, w_2 = Real('b', ctx=ctx), Real('w_1', ctx=ctx), Real('w_2', ctx=ctx)\ns.add(b &lt;= 0)  \ns.add(b &gt; -w_2)  \ns.add(b &gt; -w_1)  \ns.add(b &lt;= -w_1-w_2)\nresult = s.check()\nif result == sat: # 三种可能，sat, unsat, unknown\n    model = s.model()\n    print(model[x])  # 打印出 x 的解\nelse:\n    print(result)\n    print(\"No solution\")\n\n\nunsat\nNo solution\n\n\n\n\n代码\ns.proof()\n\n\nth-lemma(mp(mp(asserted(b &gt; -w_2),\n               trans(monotonicity(rewrite(-w_2 = -1·w_2),\n                                  (b &gt; -w_2) = (b &gt; -1·w_2)),\n                     rewrite((b &gt; -1·w_2) = ¬(b ≤ -1·w_2)),\n                     (b &gt; -w_2) = ¬(b ≤ -1·w_2)),\n               ¬(b ≤ -1·w_2)),\n            rewrite(¬(b ≤ -1·w_2) = ¬(b + w_2 ≤ 0)),\n            ¬(b + w_2 ≤ 0)),\n         mp(mp(asserted(b ≤ -w_1 - w_2),\n               rewrite((b ≤ -w_1 - w_2) =\n                       (b ≤ -1·w_1 + -1·w_2)),\n               b ≤ -1·w_1 + -1·w_2),\n            rewrite((b ≤ -1·w_1 + -1·w_2) =\n                    (b + w_1 + w_2 ≤ 0)),\n            b + w_1 + w_2 ≤ 0),\n         asserted(b ≤ 0),\n         mp(mp(asserted(b &gt; -w_1),\n               trans(monotonicity(rewrite(-w_1 = -1·w_1),\n                                  (b &gt; -w_1) = (b &gt; -1·w_1)),\n                     rewrite((b &gt; -1·w_1) = ¬(b ≤ -1·w_1)),\n                     (b &gt; -w_1) = ¬(b ≤ -1·w_1)),\n               ¬(b ≤ -1·w_1)),\n            rewrite(¬(b ≤ -1·w_1) = ¬(b + w_1 ≤ 0)),\n            ¬(b + w_1 ≤ 0)),\n         False)\n\n\n\n\n代码\n# s.proof()\ns.unsat_core()\n\n\n[]\n\n\n我们可以看到中间的过程\n\n\n代码\nwith open('log_theorem_prover.txt', 'r') as f:\n    content = f.read()\n    print(content)\n\n\n(declare-fun b () Real)\n(assert (&lt;= b 0.0))\n(declare-fun w_2 () Real)\n(assert (&gt; b (- w_2)))\n(declare-fun w_1 () Real)\n(assert (&gt; b (- w_1)))\n(assert (&lt;= b (- (- w_1) w_2)))\n(check-sat)\n\n\n\n\n\n\n1.2 题目扩展问题\n\n1.2.1 异或的“高级版”：奇偶校验问题 Parity Check\n刚才我们只说明了2维情况下感知机有局限性，现在假如我们是当年的Minsky和Papert，我们想要说明更高维度上线性不可分函数有多重要， 而感知机无法解决，我们就能崭露头角，告诉大家神经网络不行。 那么，我们就来看看异或的“高级版”：奇偶校验 Parity Check，这个问题非常重要，比如在早期的神经架构搜索研究中会经常使用(Yao 和 Liu 1997)。 奇偶校验问题的定义是根据被传输的一组二进制代码中“1”的个数是奇数或偶数来进行校验的一种方法。奇偶校验问题本质上是一个非线性问题，因为它涉及到对二进制数据中“1”的个数进行奇偶判断，这超出了感知机模型的线性决策边界能力。\n刚才我们一个个地去看不等式推导矛盾，很低效，现在我们引入线性代数的视角去看待这个问题。 我们要说明的问题其实是， 对于 \\[\nAx \\leq b\n\\] 其中 \\(A\\) 是一个 \\(m \\times n\\) 的矩阵，\\(x\\) 是一个 \\(n \\times 1\\) 的向量，\\(b\\) 是一个 \\(m \\times 1\\) 的向量。这个不等式组表示的是 \\(x\\) 需要满足的所有线性不等式条件。 在\\(A\\)和\\(b\\)满足什么条件的时候不等式不可能成立，而Parity check是否满足这个条件呢？\n实际上，这是简单版的线性规划问题，可以使用单纯形法来求解。",
    "crumbs": [
      "理论作业 Theory Assignments",
      "theory_assignments",
      "A2",
      "大数据机器学习课程第二次作业"
    ]
  },
  {
    "objectID": "theory_assignments/A2/p_assignment2_yecanming.html#sec-2",
    "href": "theory_assignments/A2/p_assignment2_yecanming.html#sec-2",
    "title": "大数据机器学习课程第二次作业",
    "section": "2 第二题",
    "text": "2 第二题\n题目如下\n\n利用课本例题3.2构造的kd树求点\\(x=(3,4.5)^{T}\\) 的最近邻点。\n\n\n2.1 审题\n例题3.2的内容如下 &gt; 给定二维空间的数据集T，构造一个平衡kd树，并给出其构造过程。\n\n\n代码\nimport numpy as np\nT = np.array([\n    [2, 3],\n    [5, 4],\n    [9, 6],\n    [4, 7],\n    [8, 1],\n    [7, 2],\n])\nT\n\n\narray([[2, 3],\n       [5, 4],\n       [9, 6],\n       [4, 7],\n       [8, 1],\n       [7, 2]])\n\n\n我们使用代码来表示一下构造的过程，并且可视化出来分界线。 首先我们要定义节点类，并且写一个验证函数，维持KD树的性质。\n\n\n代码\nimport numpy as np\nimport matplotlib.pyplot as plt\nplt.style.use('default')\n\n# 定义KD树的节点类和构建函数\nclass Node:\n    # 一个简单的二叉树\n    def __init__(self, point:np.ndarray, # 点的坐标，是k维向量\n                 left:'Node'=None, # 左子树\n                 right:'Node'=None, # 右子树\n                 axis:int=None # 划分的轴\n                 ):\n        # 维持的性质：在axis这个轴上，左子树的点坐标都小于等于point，右子树的点坐标都大于等于point\n        self.point = point\n        self.left = left\n        self.right = right\n        self.axis = axis\n    def validate(self):\n        # 检验性质：左子树的点坐标都小于等于point，右子树的点坐标都大于等于point\n        if self.left is not None:\n            if not self.left.point[self.axis] &lt;= self.point[self.axis]:\n                return False\n        if self.right is not None:\n            if not self.right.point[self.axis] &gt;= self.point[self.axis]:\n                return False\n        return True\nNode\n\n\n__main__.Node\n\n\n我们直接使用递归的方式来构建KD树\n\n\n代码\ndef build_kdtree(points, depth=0):\n    if not points:\n        return None\n\n    k = len(points[0])  # 假设所有点都有相同的维度\n    axis = depth % k # 轮着划分轴\n\n    points.sort(key=lambda x: x[axis])\n    median = len(points) // 2\n    print(\"\\t\"*depth+f\"我在构建第{depth}层的KD树， 我需要处理的数据量是{len(points)}。我对第{axis}维进行划分，在{points[median][axis]}处分割。\")\n\n    node = Node(\n        point=points[median],\n        left=build_kdtree(points[:median], depth + 1),\n        right=build_kdtree(points[median + 1:], depth + 1),\n        axis=axis\n    )\n    return node\n\n# 定义可视化函数\ndef draw_kdtree(ax, node, depth, min_x, max_x, min_y, max_y, more=0.1):\n    if node is None:\n        return\n\n    # 绘制分割线\n    if node.axis == 0:  # x轴分割\n        ax.plot([node.point[node.axis], node.point[node.axis]], [min_y, max_y], color='black', alpha=0.5 - 0.1 * depth)\n        draw_kdtree(ax, node.left, depth + 1, min_x, node.point[node.axis], min_y, max_y)\n        draw_kdtree(ax, node.right, depth + 1, node.point[node.axis], max_x, min_y, max_y)\n    else:  # y轴分割\n        ax.plot([min_x, max_x], [node.point[node.axis], node.point[node.axis]], color='black', alpha=0.5 - 0.1 * depth)\n        draw_kdtree(ax, node.left, depth + 1, min_x, max_x, min_y, node.point[node.axis])\n        draw_kdtree(ax, node.right, depth + 1, min_x, max_x, node.point[node.axis], max_y)\n    # 设置绘图限制\n    if depth == 0:\n        ax.set_xlim(min_x - more, max_x + more)\n        ax.set_ylim(min_y - more, max_y + more)\n        \n# 数据准备\npoints = T\n\n# 构建KD树\nroot = build_kdtree(points.tolist())\n\n# 可视化KD树\nfig, ax = plt.subplots()\nax.scatter(points[:, 0], points[:, 1], color='blue', zorder=10, s=50)\n\n# 绘制KD树的分割线\ndraw_kdtree(ax, root, 0, np.min(points[:, 0]), np.max(points[:, 0]), np.min(points[:, 1]), np.max(points[:, 1]), more = 1)\n\n\n我在构建第0层的KD树， 我需要处理的数据量是6。我对第0维进行划分，在7处分割。\n    我在构建第1层的KD树， 我需要处理的数据量是3。我对第1维进行划分，在4处分割。\n        我在构建第2层的KD树， 我需要处理的数据量是1。我对第0维进行划分，在2处分割。\n        我在构建第2层的KD树， 我需要处理的数据量是1。我对第0维进行划分，在4处分割。\n    我在构建第1层的KD树， 我需要处理的数据量是2。我对第1维进行划分，在6处分割。\n        我在构建第2层的KD树， 我需要处理的数据量是1。我对第0维进行划分，在8处分割。\n\n\n\n\n\n\n\n\n\n\n2.1.1 解题\n例题已经告诉我们KD树的建立过程，现在本题我们来学习一下KD树的搜索是怎么做的。\n\n\n代码\nx = np.array([3, 4.5])\nx\n\n\narray([3. , 4.5])\n\n\n\n\n代码\n# 可视化KD树\nfig, ax = plt.subplots()\nax.scatter(points[:, 0], points[:, 1], color='blue', zorder=10, s=50)\n\n# 绘制KD树的分割线\ndraw_kdtree(ax, root, 0, np.min(points[:, 0]), np.max(points[:, 0]), np.min(points[:, 1]), np.max(points[:, 1]), more = 1)\nax.scatter(x[0], x[1], color='red', zorder=10, s=50)\nax.text(x[0], x[1], f'({x[0]}, {x[1]})', verticalalignment='bottom', horizontalalignment='right')\n\n\nText(3.0, 4.5, '(3.0, 4.5)')\n\n\n\n\n\n\n\n\n\n李航书上的KD树的搜索算法我理解如下：\n\n首先我们肯定能快速找到目标点所在的最小超矩形区域。这一步就是二叉搜索树，我们按照axis去分割。\n刚才那个区域的那个点认为是目前最近的点。\n我们再考虑那个点的父节点，这个父节点有两个子树，一个是刚才的刚才的目前最近点。\n\n这个父节点和当前最近点可以比较，如果更近可以替代掉。\n检查另一个子树的时候很复杂，需要看球是否相交。\n\n\n实际上我们有更好理解的思路，书上讲了球的相交非常复杂，实际上没那么难。\n\n我们本来的目标是遍历整个树，找到所有点里面和目标点最小的。\n现在我们本来可以从root开始递归所有节点，通过先序遍历之类的方法来找到所有点。\n然后我们就发现，左子树和右子树，有一个子树整体来说是更加有希望的，那就是被划分点划分到左边还是右边。\n所以我们优先考虑被搜索点被划分的那个子树，然后再考虑另一个子树。\n父树的最短距离就是左子树的最短距离和右子树的最短距离的最小值。如果我们发现第一次查看的子树的最短距离已经比当前坐标轴上的最短距离要小，我们偶尔就可以跳过这个子树，从而提高效率。\n\n这个思路和书上的是等价的。 我们据此写Python代码\n\n\n代码\n# 定义最近邻点搜索函数\ndef find_nearest_neighbor(node, point, depth=0, best=None):\n    if node is None:\n        return best\n\n    # 初始化最佳点\n    if best is None:\n        best = node.point, float('inf')\n    # 计算当前点到查询点的距离\n    dist = np.linalg.norm(np.array(point) - np.array(node.point))\n    if dist &lt; best[1]:\n        best = (node.point, dist)\n\n    # 确定分割轴\n    axis = depth % 2\n    \n    log = \"\\t\"*depth+f\"我在第{depth}层，我现在在{node.point}，对比我现在的位置之后，我目前发现最好的点是{best[0]}，距离是{best[1]:.2f}。\"\n    # 沿着正确的子树进行搜索\n    next_branch = None\n    opposite_branch = None\n    if point[axis] &lt; node.point[axis]:\n        next_branch = node.left\n        opposite_branch = node.right\n        log += f\"我觉得左子树更有希望, 因为在{axis}维度上，我们被划分到左边了。\"\n    else:\n        next_branch = node.right\n        opposite_branch = node.left\n        log += f\"我觉得右子树更有希望, 因为在{axis}维度上，我们被划分到右边了。\"\n        \n    print(log)\n          \n    # 递归搜索\n    best = find_nearest_neighbor(next_branch, point, depth + 1, best)\n    print(\"\\t\"*depth+f\"我在第{depth}层，我现在在{node.point}，对比第一个子树之后，我目前发现最好的点是{best[0]}，距离是{best[1]:.2f}\")\n\n    # 如果分割线与查询点的距离小于最佳距离，则搜索另一边的子树\n    if abs(point[axis] - node.point[axis]) &lt; best[1]:\n        best = find_nearest_neighbor(opposite_branch, point, depth + 1, best)\n        print(\"\\t\"*depth+f\"我在第{depth}层，我现在在{node.point}，对比第二个子树之后，我目前发现最好的点是{best[0]}，距离是{best[1]:.2f}\")\n    else:\n        print(f\"另一边的子树在现在这个维度{axis}上的距离{abs(point[axis] - node.point[axis])}大于最佳距离{best[1]:.2f}，我不用继续搜索。\")\n\n    return best\n\n\n\n# 找到点 (3, 4.5) 的最近邻点\ninput_point = [3, 4.5]\nnearest_point, distance = find_nearest_neighbor(root, input_point)\nnearest_point\n\n\n我在第0层，我现在在[7, 2]，对比我现在的位置之后，我目前发现最好的点是[7, 2]，距离是4.72。我觉得左子树更有希望, 因为在0维度上，我们被划分到左边了。\n    我在第1层，我现在在[5, 4]，对比我现在的位置之后，我目前发现最好的点是[5, 4]，距离是2.06。我觉得右子树更有希望, 因为在1维度上，我们被划分到右边了。\n        我在第2层，我现在在[4, 7]，对比我现在的位置之后，我目前发现最好的点是[5, 4]，距离是2.06。我觉得左子树更有希望, 因为在0维度上，我们被划分到左边了。\n        我在第2层，我现在在[4, 7]，对比第一个子树之后，我目前发现最好的点是[5, 4]，距离是2.06\n        我在第2层，我现在在[4, 7]，对比第二个子树之后，我目前发现最好的点是[5, 4]，距离是2.06\n    我在第1层，我现在在[5, 4]，对比第一个子树之后，我目前发现最好的点是[5, 4]，距离是2.06\n        我在第2层，我现在在[2, 3]，对比我现在的位置之后，我目前发现最好的点是[2, 3]，距离是1.80。我觉得右子树更有希望, 因为在0维度上，我们被划分到右边了。\n        我在第2层，我现在在[2, 3]，对比第一个子树之后，我目前发现最好的点是[2, 3]，距离是1.80\n        我在第2层，我现在在[2, 3]，对比第二个子树之后，我目前发现最好的点是[2, 3]，距离是1.80\n    我在第1层，我现在在[5, 4]，对比第二个子树之后，我目前发现最好的点是[2, 3]，距离是1.80\n我在第0层，我现在在[7, 2]，对比第一个子树之后，我目前发现最好的点是[2, 3]，距离是1.80\n另一边的子树在现在这个维度0上的距离4大于最佳距离1.80，我不用继续搜索。\n\n\n[2, 3]\n\n\n上述算法的流程图如下：\n\n\n\n\n\ngraph TD\n    A[开始] --&gt; B[检查node是否为None]\n    B --&gt;|是| C[返回best]\n    B --&gt;|否| D[初始化best]\n    D --&gt; E[计算dist]\n    E --&gt; F[更新best]\n    F --&gt; G[确定分割轴axis]\n    G --&gt; H[判断搜索方向]\n    H --&gt;|左子树| I[递归搜索左子树]\n    H --&gt;|右子树| J[递归搜索右子树]\n    I --&gt; K[检查是否需要搜索另一边子树]\n    J --&gt; K\n    K --&gt;|需要| L[递归搜索另一边子树]\n    K --&gt;|不需要| M[结束递归]\n    L --&gt; M\n    M --&gt; N[返回best]\n    N --&gt; C\n\n    style A fill:#bbf,stroke:#f66,stroke-width:2px\n    style C fill:#bbf,stroke:#f66,stroke-width:2px\n    style N fill:#bbf,stroke:#f66,stroke-width:2px\n\n\n\n\n\n\n\n\n\n\n2.2 题目扩展问题\n\n2.2.1 在K=1时，KD树和红黑树、AVL树等平衡搜索树的关系是什么?\n关系在于KD树没有考虑新增加节点的算法复杂度，每次新增加节点都要重新构造。 KD树建树的时候直接算中位数，强行平衡。\n\n\n2.2.2 怎么证明刚才的过程就找到了最近邻点？\n\n\n2.2.3 刚才我们只找到了最近邻点，如果需要返回q个最近邻点，应该如何修改代码？\n\n\n2.2.4 KD树构建过程、搜索过程的平均算法复杂度、最坏情况算法复杂度分别都是多少？\n平均复杂度 搜索是O(logN)， N是点的数量(李航 2019)。\n\n\n2.2.5 目前前沿的向量数据库中实际上做KNN是用什么数据结构？支持GPU加速吗？\n目前前沿的向量数据库中，实际上做KNN（K-Nearest Neighbor，最近邻搜索）通常使用的数据结构除了KD-Tree，还有HNSW（Hierarchical Navigable Small World） 以及 Ball Tree、 FLANN、局部敏感哈希（LSH）。\nNVIDIA cuVS库用于GPU加速的向量搜索和聚类，使用CAGRA（CUDA-Accelerated Graph Index for Vector Retrieval）技术。",
    "crumbs": [
      "理论作业 Theory Assignments",
      "theory_assignments",
      "A2",
      "大数据机器学习课程第二次作业"
    ]
  },
  {
    "objectID": "theory_assignments/A3/p_assignment3_yecanming.html#sec-1",
    "href": "theory_assignments/A3/p_assignment3_yecanming.html#sec-1",
    "title": "朴素贝叶斯与决策树的深入理解",
    "section": "1 第一题——朴素贝叶斯法概率估计公式推导",
    "text": "1 第一题——朴素贝叶斯法概率估计公式推导\n题目如下\n\n用贝叶斯估计法推出朴素贝叶斯法中的概率估计公式(4.10)和(4.11) \\(\\begin{aligned}P_{\\lambda}(X^{(j)}=a_{jl}|Y=C_{k})=& \\frac{\\sum_{i=1}^{N}I(x_{i}^{(j)}=a_{jl},y_{i}=c_{k})+\\lambda}{\\sum_{i=1}^{N}I(y_{i}=c_{k})+S_{j}\\lambda}\\end{aligned}\\)\n\n\n\n\n\n\n\n注记\n\n\n\nTL; DR 前面审题内容较长，学习了一些这道题的一些背景知识方便理解。 对于题目的证明，可以直接跳到解题部分@sec-proof。\n\n\n\n1.1 审题\n\n1.1.1 4.10和4.11式是什么？题目所给的式子是另一个公式吗？\n根据李航机器学习方法课本内容(李航 2019) 54 页，\n4.10 式就是题目汇中给的式子，即： \\[\n\\boldsymbol{P_{\\lambda}(X^{(j)}=a_{jl}|Y=c_{k})=\\frac{(\\sum_{i=1}^{N}I(x_{i}^{(j)}=a_{jl},y_{i}=c_{k}))+\\lambda}{(\\sum_{i=1}^{N}I(y_{i}=c_{k}))+S_{j}\\lambda}}\n\\tag{1}\\]\n书中说，用极大似然估计可能会出现所要估计的概率值为0的情况。这时会影响后验概率的计算 结果，使分类产生偏差。而上式就是为了解决这个问题而提出的，上式是对条件概率的贝叶斯估计，引入了一个新的参数\\(\\lambda \\ge 0\\)，等价于在每一个随机变量的取值频数都增加\\(\\lambda\\)次。当 \\(\\lambda=0\\) 时，等价于使用极大似然估计， 当 \\(\\lambda = 1\\) 时，称为拉普拉斯平滑。\n朴素贝叶斯需要知道怎么估计先验概率和条件概率（似然），刚才4.10估计了条件概率，接下来4.11式是估计先验概率怎么用上拉普拉斯平滑，即： \\[\nP_{\\lambda}\\left(Y=c_{k}\\right)=\\frac{(\\sum_{i=1}^{N} I\\left(y_{i}=c_{k}\\right))+\\lambda}{N+K \\lambda}\n\\tag{2}\\]\n\n\n\n\n\n\n警告\n\n\n\n李航书上的原式对括号的位置不清晰，让人分不清楚\\(+\\lambda\\) 是在求和操作\\(\\sum\\)的里面还是外面。为了避免歧义，我们上面已经修改，加上了括号，即\\(+\\lambda\\)的操作是在求和操作\\(\\sum\\)结束之后。\n\n\n\n\n1.1.2 本题要从什么推导到什么？\n4.10和4.11都是推导的结果，即贝叶斯估计的结果，但是李航书上没有说具体为什么加上\\(\\lambda\\) 就是贝叶斯估计了，具体是怎么贝叶斯估计的，所以本题需要我们推导。\n\n\n1.1.3 以上公式中的符号是什么含义？\n我们需要了解 式 1 和 式 2 中的符号含义。\n\n首先最基础的\n\n$ X R^n $：表示输入空间是n维向量集合。\n$ Y = { c_1, c_2,…, c_K } $：输出空间为类别标签集合，共有K个不同的类。\n输入特征向量 $ x X \\(，输出类别标签（class label）\\) y Y $。X是在输入空间上的随机变量，Y是在输出空间上的随机变量。\nP(X,Y) 是X和Y的联合概率分布。\n训练数据集T由N对独立的同分布产生的样本组成，每对包括一个来自X的特征向量和一个来自Y的类别标签。\n\n实际上为了方便推导，这里\\(X^{(j)}\\) 的取值是离散的，并不是\\(R^n\\)上的连续随机变量。\n\n这里假设设第j个特征\\(x^{(i)}\\)可能取值的集合为\\(\\{a_{j_1},a_{j_2},…,a_{j_{S_j}}\\}\\), 其中\\(S_j\\)为第j个特征的取值个数。\\(a_{jl}\\)用下标\\(l\\)来表示是第几个取值。\n\\(I\\)为指示函数。\n\n\n\n\n\n\n\n\n注记\n\n\n\n李航书上推导的是Categorical Naive Bayes，输入的是离散型随机变量。如果是连续型随机变量，则需要使用Gaussian Naive Bayes。\n\n\n\n\n1.1.4 一个问题，式子式 1 和 式子式 2中的\\(\\lambda\\)是同一个参数吗？\n\n如果按照李航书上说的“在随机变量各个取值的频数上赋予一个正数\\(\\lambda\\)”，那么比较难理解，到底是在\\(X^{(j)}\\)的每个取值上面加一共\\(S_j\\)次经验，还是在Y的每个类别上一共加上\\(K\\)次经验？\n这两个都取1不会矛盾吗？\n根据例题4.2，实际上就是都是取1。\n\n\n\n\n1.2 解题\n\n1.2.1 总体推导原则\n朴素贝叶斯法需要学习 \\(P(Y=c_k)\\) 和 \\(P(X^{(j)}|Y=c_k)\\) 这两系列的参数，所谓学习就是通过统计学方法对参数进行估计。 给定一个具体的数据集，极大似然估计的意思是计算出上面这两个参数每一个具体取值下，数据集如此出现的概率（似然），通过最大化这个似然，我们可以得到最有可能的模型参数。\n而贝叶斯估计法，根据我们PPT课件中学习的知识，则是认为\\(P(Y=c_k)\\) 和 \\(P(X^{(j)}|Y=c_k)\\)参数本身作为随机变量，具有先验分布，在我们观察到数据集\\(T = \\{(x_1,y_1), ..., (x_N,y_N)\\}\\)之后，我们后验地更新我们对这两个参数分布的认知，最后可能以参数分布的期望或者使得后验最大的值作为我们对参数的估计。\n根据课件上的内容，最后决策的时候，也可以不使用参数的估计，而是把整个参数的后验分布拿过来，去积分来对预测值求一个分布，称为后验预测分布。 #### 符号简化\n我们记\\(P(Y=c_k)\\)这k个参数为随机向量\\(\\theta_k\\)\n记\\(P(X^{(j)}=a_{jl}|Y=c_k)\\)这j个参数为随机向量\\(\\theta_{kl}\\)\n\n\n1.2.2 取什么共轭先验分布？\n\\(\\theta_k=P(Y=c_k)\\) 中有K个参数，需要满足加起来等于1，而且每个值非负的约束条件。\n\\(\\theta_{kl}=P(X^{(j)}|Y=c_k)\\) 我们不妨只管第j个特征，其他的特征是独立的，推导起来是一样的，那么j是已知了，这里需要估计\\(S_j \\times K\\) 个参数。同样也需要满足约束条件，对于每个具体的\\(c_k\\), \\(\\boldsymbol{P(X^{(j)}=a_{jl}|Y=c_{k})}\\) 所有\\(X^{(j)}\\)取值加起来等于1，以及取值非负。\n对于这种要求加起来等于1的约束条件，可以选择狄利克雷分布（Dirichlet Distribution）作为先验分布。狄利克雷分布又称多元Beta分布(multivariate Beta distribution)。\n设随机变量\\(\\theta_k\\)组成的随机向量\\(\\theta_{1:k}\\) 服从 \\(Dir(\\alpha_{1:k})\\)，其中\\(\\alpha_{1:k}\\) 是先验分布的超参数，是一个K维向量，且每个元素\\(\\alpha_k&gt;0\\)。 \\[\nP(\\theta_{1:k} | \\alpha_{1:k}) = \\frac{1}{B(\\alpha_{1:k})} \\prod_{k=1}^K \\theta_k^{\\alpha_k-1}\n\\]\n为了进一步简化问题，我们不妨认为这个分布是对称狄利克雷分布，即\\(\\forall k, \\alpha_k=\\lambda\\)。\\(\\lambda\\)被称为浓度参数。\n\n\n1.2.3 推导4.11式（朴素贝叶斯的”类别先验分布”参数的贝叶斯估计后验分布）\n根据贝叶斯公式，我们有： \\[\n\\begin{aligned}\nP(\\theta_{1:k} | \\alpha_{1:k}, T_{1:N}) &= \\frac{P(T_{1:N} | \\theta_{1:k}, \\alpha_{1:k}) P(\\theta_{1:k}|\\alpha_{1:k})}{P(T_{1:N}| \\alpha_{1:k})} \\\\\n&\\propto P(T_{1:N} | \\theta_{1:k}, \\alpha_{1:k}) P(\\theta_{1:k}|\\alpha_{1:k}) \\\\\n&= P(T_{1:N} | \\theta_{1:k}) P(\\theta_{1:k}|\\alpha_{1:k}) \\\\\n&= likelihood \\times prior \\\\\n\\end{aligned}\n\\]\n\n其中数据集的似然函数，我们有： \\[\n\\begin{aligned}\nP(T_{1:N} | \\theta_{1:k}) &= \\prod_{n=1}^N P(t_n|\\theta_{1:k}) \\\\\n\\end{aligned}\n\\] 其中 \\(P(t_n|\\theta_{1:k})\\) 的值是，如果\\(t_n = (x_n, y_n)\\)的\\(y_n = c_k\\), 则值为 \\(\\theta_k\\), 也就是 \\(P(t_n|\\theta_{1:k}) = \\prod_{k=1}^K I(y_n = c_k) \\times \\theta_{k}\\)。\n\n据此我们可以换一下整个式子求积的方式，原本是对每一个样本求积，现在我们只需要对每个类别求积即可：\n\\[\n\\begin{aligned}\nP(T_{1:N} | \\theta_{1:k}) &= \\prod_{k=1}^K \\theta_{k}^{N_k} \\\\\n\\end{aligned}\n\\]\n\n而对于先验分布，代入@sec-prior的狄利克雷分布公式： \\[\n\\begin{aligned}\nP_{\\lambda}(\\theta_{1:k}|\\alpha_{1:k}) &= \\frac{1}{B(\\lambda)} \\prod_{k=1}^K \\theta_k^{\\lambda-1}\n\\end{aligned}\n\\]\n根据1.和2.，我们有后验分布：\n\n\\[\n\\begin{aligned}\nP(\\theta_{1:k} | \\alpha_{1:k}, T_{1:N}) &\\propto likelihood \\times prior \\\\\n&\\propto \\prod_{k=1}^K \\theta_k^{N_k + \\lambda-1}\n\\end{aligned}\n\\]\n\\(\\theta_{1:k}\\) 在已知\\(\\alpha_{1:k}, T_{1:N}\\)的情况下，服从对称Dirichlet分布 \\(Dir(\\alpha_{N_k + \\lambda})\\)。\n而Dirichlet分布每一个\\(\\theta_k\\)的期望为其对应的参数\\(\\alpha_k\\)的占比。\n因此，我们就可以用后验分布的期望来估计\\(\\theta_{1:k}\\)。\n$$\n\\[\\begin{aligned}\nP_{\\lambda}\\left(Y=c_{k}\\right) &= \\hat{\\theta_k} \\\\\n    &= E(\\theta_k| \\lambda, T_{1:N}) \\\\\n    & = \\frac{N_k + \\lambda}{\\sum_{k=1}^K (N_k + \\lambda)} \\\\\n    &=\\frac{(\\sum_{i=1}^{N} I\\left(y_{i}=c_{k}\\right))+\\lambda}{N+K \\lambda}\n\\end{aligned}\\]\n$$ 即 式 2 式。\n\n\n1.2.4 推导4.10式（朴素贝叶斯的”条件概率”参数的贝叶斯估计后验分布）\n类似于@sec-prior，我们首先要定义条件概率\\(\\theta_{kl}\\) 的先验分布\n设随机变量\\(\\theta_{kl}\\)组成的随机向量\\({\\theta_k}_{1:l}\\) 服从 \\(Dir({\\alpha_k}_{1:l})\\)，其中\\({\\alpha_k}_{1:l}\\) 是先验分布的超参数，是一个\\(S_j\\)维向量，且每个元素\\(\\alpha_{kl}&gt;0\\)。 \\[\nP(\\theta_{k1:l} | \\alpha_{k1:l}) = \\frac{1}{B(\\alpha_{k1:l})} \\prod_{l=1}^{S_j} \\theta_{kl}^{\\alpha_{kl}-1}\n\\]\n同理，为了进一步简化问题，我们不妨认为这个分布是对称狄利克雷分布，即\\(\\forall l, \\alpha_{kl}=\\lambda\\)。\\(\\lambda\\)被称为浓度参数。 &gt; 这里回答了上面我提出的问题，实际上两个式子的\\(\\lambda\\)可以不一样，这取决于使用朴素贝叶斯的人的偏好和经验，由于正好我们都要做拉普拉斯平滑，也就是\\(\\lambda=1\\)，所以正好两个\\(\\lambda\\)相等，但是也可以是不同的。\n与@sec-proof类似，我们可以做一样的推导，只是把符号替换 - K替换为\\(S_j\\) - \\(\\theta_{k}\\)替换为\\({\\theta_kl}\\) - \\(\\alpha_{k}\\)替换为\\({\\alpha_kl}\\)\n\\[\n\\begin{aligned}\nP_{\\lambda} (X^{(j)}=a_{jl}|Y=c_{k})\n  &= \\hat{\\theta_{kl}} \\\\\n   &= E(\\theta_{kl}|\\lambda, T_{1:N}) \\\\\n    &=\\frac{(\\sum_{i=1}^{N}I(x_{i}^{(j)}=a_{jl},y_{i}=c_{k}))+\\lambda}{(\\sum_{i=1}^{N}I(y_{i}=c_{k}))+S_{j}\\lambda}\n\\end{aligned}\n\\]\n\n\n\n1.3 题目扩展问题\n\n1.3.1 贝叶斯估计的先验是否可以被任意地操控，让我们的后验分布变成任意的分布？如果是这样，那么我们如何保证先验分布的合理性？\n\n\n1.3.2 贝叶斯估计参数的后验分布的期望能否代替后验分布本身来预测新的数据的分布？",
    "crumbs": [
      "理论作业 Theory Assignments",
      "theory_assignments",
      "A3",
      "朴素贝叶斯与决策树的深入理解"
    ]
  },
  {
    "objectID": "theory_assignments/A3/p_assignment3_yecanming.html#sec-2",
    "href": "theory_assignments/A3/p_assignment3_yecanming.html#sec-2",
    "title": "朴素贝叶斯与决策树的深入理解",
    "section": "2 第二题",
    "text": "2 第二题\n题目如下\n\n已知如表5.2所示的训练数据，试用平方误差损失准则生成一个二叉回归树。\n\n5.2 训练数据是\n\n\n代码\nimport numpy as np\nx = np.arange(1, 11)\ny = np.array([4.5, 4.75, 4.91, 5.34, 5.80, 7.05, 7.90, 8.23, 8.70, 9.00])\nx, y\n\n\n(array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10]),\n array([4.5 , 4.75, 4.91, 5.34, 5.8 , 7.05, 7.9 , 8.23, 8.7 , 9.  ]))\n\n\n\n2.1 审题\n数据可视化如下\n\n\n代码\nimport matplotlib.pyplot as plt\nplt.plot(x, y, 'o-')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.title('y vs x')\nplt.show()\n\n\n\n\n\n\n\n\n\n平方误差损失准则是什么？二叉回归树是什么？\nCART（Classification and Regression Tree）方法就是”分类与回归树”方法，CART对于输入X构建的决策树是二叉树，这里的分类与回归指的是输出变量Y是离散的还是连续的。CART认为回归树是要用平方误差作为损失函数，而分类树是要用基尼系数作为损失函数，所谓“准则”是指构建树的时候用损失函数来决定用哪个输入的属性来划分节点。\n具体来说，就是划分前后损失函数的变化。\n\n2.1.1 解题\n我们直接通过写代码（注意这里有参考一些网上的代码，但是我们每一行都认真审查过了）来加深自己的理解，实现书上算法5.5。 首先我们要定义二叉树的节点类，包括左右子树指针。\n对于决策树而言，二叉树的节点还需要记录 - 特征 feature：记录当前节点划分的特征（用属性的编号j来表示） - 切分值 split：是如何划分左右子树的 - 估计值 value：被划分到当前节点的输入可以用什么输出值（通过平均值）来估计\n\n\n代码\nimport numpy as np\n\nclass Node:\n    def __init__(self, feature=None, # 切分的特征\n                 split=None,  # 切分的值\n                 left:'Node'=None, # 左子树\n                 right:'Node'=None, # 右子树\n                 value=None # 叶子节点的预测值\n                 ):\n        self.feature = feature \n        self.split = split\n        self.left:'Node' = left\n        self.right:'Node' = right\n        self.value = value\n\n    def is_leaf_node(self):\n        return self.value is not None\n\n\n下面我们来写回归树类。我们一个一个方法的写（得益于fastcore库的patch函数，我们可以渐进式地给Python类添加方法），首先是构造函数，指定算法的超参数。\n原始的书上算法5.5 没有max_depth, 而 min_samples_split=2\n\n\n代码\nfrom fastcore.utils import patch\nclass RegressionTree:\n    def __init__(self, min_samples_split=2, # 最少需要多少个样本才能分裂\n                 max_depth=float('inf') # 最大深度，太深的话容易过拟合\n                 ):\n        self.min_samples_split = min_samples_split \n        self.max_depth = max_depth \n        self.root:'Node' = None # root 也就是不划分，用所有值的y的均值作为预测值\n\n\n接下来我们可以看看怎么写回归树的训练算法。这里的关键是对输入空间如何进行划分。 CART是启发式（贪心）算法，每一次就按照最优特征和最优切分点来划分子空间。具体来说，CART算法的训练过程如下：\n\n\n代码\n@patch\ndef fit(self:RegressionTree, X, y):\n    # 这里是对外的接口， 里面的_grow_tree是递归的过程\n    self.root = self._grow_tree(X, y)\n\n@patch\ndef _grow_tree(self:RegressionTree, \n                X, # 是划分后的子集\n                y, # 是对应的标签\n                depth=0 # 当前深度\n    ):\n    n_samples, n_features = X.shape\n    n_labels = len(np.unique(y)) \n\n    # stopping criteria\n    if (depth &gt;= self.max_depth\n            or n_labels == 1 # 只有一个值，就算有很多样本也不划分了。\n            or n_samples &lt; self.min_samples_split):\n        leaf_value = np.mean(y)\n        self._print_leaf(depth, leaf_value)\n        return Node(value=leaf_value)\n\n    # find the best split 这里是最难的地方，效率也比较低，需要对整个X的每一个特征都遍历一次。\n    best_feature, best_split, best_score = self._best_split(X, y)\n    self._print_split(depth, best_feature, best_split, best_score)\n    \n    # grow the children that result from the split\n    left_indices, right_indices = self._split(X[:, best_feature], best_split)\n    left_child = self._grow_tree(X[left_indices], y[left_indices], depth + 1)\n    right_child = self._grow_tree(X[right_indices], y[right_indices], depth + 1)\n    return Node(best_feature, best_split, left_child, right_child)\n\n@patch\ndef _print_split(self:RegressionTree, depth, feature, split, score):\n    print(f\"{' ' * depth}Depth {depth}: Split on feature {feature} at value {split} with score {score}\")\n\n@patch\ndef _print_leaf(self:RegressionTree, depth, value):\n    print(f\"{' ' * depth}Depth {depth}: Leaf with value {value}\")\n\n@patch\ndef _best_split(self:RegressionTree, X, y):\n    # 需要遍历所有的特征和所有可能的切分点，找到最佳切分点\n    best_score = float('inf') # 这里我们假设越小越好。注意我们不需要算增益，其实只需要算划分之后的loss就行\n    best_feature, best_split = None, None\n    for feature_index in range(X.shape[1]):\n        feature_values = X[:, feature_index]\n        possible_splits = np.unique(feature_values) # 如果输入没有那么多就可以少算一点\n        for split in possible_splits:\n            left_indices, right_indices = self._split(feature_values, split)\n            if len(left_indices) == 0 or len(right_indices) == 0: # 划分之后有一边没了，那就不考虑这个划分点。因为增益肯定是0。\n                continue\n            left_score = self._calculate_mse(y[left_indices])\n            right_score = self._calculate_mse(y[right_indices])\n            score = left_score + right_score\n            if score &lt; best_score: # 改进了\n                best_score, best_feature, best_split = score, feature_index, split\n    return best_feature, best_split, best_score\n@patch\ndef _split(self:RegressionTree, feature_values, split):\n    left_indices = np.where(feature_values &lt;= split)\n    right_indices = np.where(feature_values &gt; split)\n    return left_indices, right_indices\n\n@patch\ndef _calculate_mse(self:RegressionTree, y):\n    # 这里是y的子集，这个子集使用np.mean作为预测值，所以会有误差。  \n    return np.mean((y - np.mean(y)) ** 2)\n\nX = x.reshape(-1, 1)\nreg_tree = RegressionTree(min_samples_split=2, max_depth=float('inf'))\nreg_tree.fit(X, y)\n\n\nDepth 0: Split on feature 0 at value 5 with score 0.6717439999999998\n Depth 1: Split on feature 0 at value 3 with score 0.08136666666666667\n  Depth 2: Split on feature 0 at value 1 with score 0.006400000000000012\n   Depth 3: Leaf with value 4.5\n   Depth 3: Split on feature 0 at value 2 with score 0.0\n    Depth 4: Leaf with value 4.75\n    Depth 4: Leaf with value 4.91\n  Depth 2: Split on feature 0 at value 4 with score 0.0\n   Depth 3: Leaf with value 5.34\n   Depth 3: Leaf with value 5.8\n Depth 1: Split on feature 0 at value 6 with score 0.17891874999999977\n  Depth 2: Leaf with value 7.05\n  Depth 2: Split on feature 0 at value 8 with score 0.049725000000000116\n   Depth 3: Split on feature 0 at value 7 with score 0.0\n    Depth 4: Leaf with value 7.9\n    Depth 4: Leaf with value 8.23\n   Depth 3: Split on feature 0 at value 9 with score 0.0\n    Depth 4: Leaf with value 8.7\n    Depth 4: Leaf with value 9.0\n\n\n/home/ai_pitch_perfector/program_files/managers/conda/lib/python3.10/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n  return _methods._mean(a, axis=axis, dtype=dtype,\n/home/ai_pitch_perfector/program_files/managers/conda/lib/python3.10/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n  ret = ret.dtype.type(ret / rcount)\n\n\n\n\n代码\n!pip install graphviz\n\n\nLooking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\nRequirement already satisfied: graphviz in /home/ai_pitch_perfector/program_files/managers/conda/lib/python3.10/site-packages (0.20.3)\n\n\n\n\n代码\nfrom graphviz import Digraph\n\ndef visualize_node(node:Node, graph_label=\"Regression Tree\"):\n    # Create Digraph object\n    dot = Digraph(comment=graph_label)\n\n    # Helper function to recursively add nodes and edges\n    def add_nodes_edges(node, parent_node_id=None):\n        # If node is a leaf, add it to the graph\n        if node.is_leaf_node():\n            node_id = str(id(node))\n            dot.node(node_id, f\"Leaf\\nValue: {node.value:.2f}\")\n        else:\n            # Add current node to the graph\n            node_id = str(id(node))\n            dot.node(node_id, f\"X_{node.feature} &lt;= {node.split:.2f}\")\n\n            # Recursively add left and right children\n            left_child_id = add_nodes_edges(node.left, node_id)\n            right_child_id = add_nodes_edges(node.right, node_id)\n\n            # Add edges from current node to its children\n            dot.edge(node_id, left_child_id, label=\"True\")\n            dot.edge(node_id, right_child_id, label=\"False\")\n\n        return node_id\n\n    # Start from the root node\n    add_nodes_edges(node)\n\n    return dot\n\n@patch\ndef visualize(self:RegressionTree):\n    return visualize_node(self.root)\n\n# Visualize the tree\ndot = reg_tree.visualize()\ndot.render('regression_tree', format='png', cleanup=True)  # Save the tree as an image\n'(regression_tree.png)'\ndot\n\n\n\n\n\n\n\n\n\n接下来是第二步，有了这个决策树，怎么预测新的数据呢？\n\n\n代码\n@patch\ndef predict(self:RegressionTree, X):\n    # 对于新的输入，预测值怎么计算呢，对每一个值去做。\n    self.decision_paths = [] \n    return np.array([self._predict_sample(x) for x in X])\n\n@patch\ndef _predict_sample(self:RegressionTree, inputs):\n        node:Node = self.root\n        path = []  # Track the decision path\n        while not node.is_leaf_node():\n            path.append(node)\n            if inputs[node.feature] &lt;= node.split:\n                node = node.left\n            else:\n                node = node.right\n        path.append(node)  # Add the leaf node to the path\n        self.decision_paths.append(path)\n        return node.value\n\npredictions = reg_tree.predict(X)\npredictions\n\n\narray([4.5 , 4.75, 4.91, 5.34, 5.8 , 7.05, 7.9 , 8.23, 8.7 , 9.  ])\n\n\n因为我们没有限制深度，所以predictions应该与真实值相同。\n\n\n代码\npredictions - y\n\n\narray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n\n\n如果我们有最大深度设为2，那么可以画出下面的图\n\n\n代码\nimport matplotlib.pyplot as plt\n\nreg_tree = RegressionTree(max_depth=2)\nreg_tree.fit(X, y)\npredictions = reg_tree.predict(X)\n\n# Scatter plot for actual values and predictions\nplt.figure(figsize=(10, 6))\nplt.scatter(range(len(y)), y, color='blue', label='Actual Values')\nplt.scatter(range(len(predictions)), predictions, color='red', label='Predictions')\n\n# Highlight the difference\nfor i, (act, pred) in enumerate(zip(y, predictions)):\n    plt.plot([i, i], [act, pred], color='green' if act == pred else 'orange')\n\nplt.title('Actual Values vs. Predictions')\nplt.xlabel('Data Points')\nplt.ylabel('Values')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n/home/ai_pitch_perfector/program_files/managers/conda/lib/python3.10/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n  return _methods._mean(a, axis=axis, dtype=dtype,\n/home/ai_pitch_perfector/program_files/managers/conda/lib/python3.10/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n  ret = ret.dtype.type(ret / rcount)\n\n\nDepth 0: Split on feature 0 at value 5 with score 0.6717439999999998\n Depth 1: Split on feature 0 at value 3 with score 0.08136666666666667\n  Depth 2: Leaf with value 4.72\n  Depth 2: Leaf with value 5.57\n Depth 1: Split on feature 0 at value 6 with score 0.17891874999999977\n  Depth 2: Leaf with value 7.05\n  Depth 2: Leaf with value 8.4575\n\n\n\n\n\n\n\n\n\n\n\n\n2.2 题目扩展问题\n这里我们产生一个疑问，划分一定能改进训练损失吗？有没有可能信息增益是负的，划分之后反而loss更大？\n我们其实想问，\\[MSE(E(l_1), l_1) + MSE(E(l_2), l_2) \\le MSE(E(l_1+l_2), l_1+l_2)\\] 这个不等式是否恒成立？ 其中 \\(l_1\\) 和 \\(l_2\\) 是两个任意实数列表， \\(l_1+l_2\\) 是两个列表的拼接。",
    "crumbs": [
      "理论作业 Theory Assignments",
      "theory_assignments",
      "A3",
      "朴素贝叶斯与决策树的深入理解"
    ]
  }
]