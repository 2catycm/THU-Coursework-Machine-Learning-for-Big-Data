# AUTOGENERATED! DO NOT EDIT! File to edit: ../../notebooks/coding_projects/big_data_analytics/P2_Matrix-Decomposition/00matrix_decomposition.ipynb.

# %% auto 0
__all__ = ['fixed_meta_params', 'frozen_rvs', 'study_path', 'sqlite_url', 'study', 'get_rating_matrix', 'get_similarities',
           'compute_weighted_sum_on_matrix', 'get_X_train_weighted', 'ensure_tensor', 'masked_rmse_loss',
           'MatrixFactorization', 'JaxMatrixFactorization', 'masked_mse_loss', 'jax_masked_mse_loss',
           'train_matrix_factorization', 'train_step', 'compilable_jax_masked_mse_loss',
           'train_matrix_factorization_jax', 'draw_metrics_df', 'MatrixFactorizationSetting', 'objective',
           'test_normality_small_sample']

# %% ../../notebooks/coding_projects/big_data_analytics/P2_Matrix-Decomposition/00matrix_decomposition.ipynb 57
# @joblib_memory.cache # data 哈希本身需要很大的开销，不宜 cache
def get_rating_matrix(data:pd.DataFrame)-> csr_matrix:
    rows = data['user_id'].map(user_to_row)
    cols = data['movie_id'].map(movie_to_col)
    values = data['rating']
    
    return csr_matrix((values, (rows, cols)), shape=(len(user_to_row), len(movie_to_col)))

# %% ../../notebooks/coding_projects/big_data_analytics/P2_Matrix-Decomposition/00matrix_decomposition.ipynb 66
from sklearn.metrics.pairwise import cosine_similarity

# %% ../../notebooks/coding_projects/big_data_analytics/P2_Matrix-Decomposition/00matrix_decomposition.ipynb 67
@joblib_memory.cache
def get_similarities():
    simularities = cosine_similarity(X_train)
    return simularities

# %% ../../notebooks/coding_projects/big_data_analytics/P2_Matrix-Decomposition/00matrix_decomposition.ipynb 78
from copy import deepcopy
from tqdm import tqdm

# %% ../../notebooks/coding_projects/big_data_analytics/P2_Matrix-Decomposition/00matrix_decomposition.ipynb 81
def compute_weighted_sum_on_matrix(cosine_sim, X_train_dense_nan):
# 创建一个与 X_train_dense 相同大小的矩阵，用于存储加权平均数
    X_train_weighted = np.zeros_like(X_train_dense_nan)

    # 遍历 X_train_dense 的每一行
    for i in tqdm(range(X_train_dense_nan.shape[0])):
        # 获取第 i 行的权重
        weights = cosine_sim[i, :]
        
        # 复制这个权重到整个矩阵的维度，方便后面掩码操作
        weights = np.repeat(weights, X_train_dense_nan.shape[1]).reshape(X_train_dense_nan.shape[0], X_train_dense_nan.shape[1])
        
        
        # 创建一个掩码，是 nan的就是True
        mask = np.isnan(X_train_dense_nan)
        
        
        # 将权重中的对应位置设置为 np.nan
        weights = np.where(mask, np.nan, weights)
        
        # 计算加权平均数，忽略 np.nan 值
        X_train_weighted[i, :] = np.nansum(X_train_dense_nan * weights, axis=0) / np.nansum(weights, axis=0)

    # X_train_weighted 现在是一个 mxn 的矩阵，其中每一行是忽略 np.nan 的加权平均数
    return X_train_weighted

@joblib_memory.cache
def get_X_train_weighted():
    return compute_weighted_sum_on_matrix(cosine_sim, X_train_dense_nan)

# %% ../../notebooks/coding_projects/big_data_analytics/P2_Matrix-Decomposition/00matrix_decomposition.ipynb 100
import torch

# %% ../../notebooks/coding_projects/big_data_analytics/P2_Matrix-Decomposition/00matrix_decomposition.ipynb 101
def ensure_tensor(x:np.ndarray|list, device:str="cuda:2"):
    # 确保输入loss函数的数据是张量
    return torch.tensor(x).to(device)
def masked_rmse_loss(reconstructed:torch.Tensor, matrix:torch.Tensor, verbose:bool=False, do_ensure_tensor:bool=True)->torch.Tensor:
    # 首先确保类型是tensor，而且device想同，这样才能在GPU上算Loss
    if do_ensure_tensor:
        reconstructed = ensure_tensor(reconstructed)
        matrix = ensure_tensor(matrix)
    # 获得一个 mask，在老师给的文档中也称为“指示矩阵”
    test_mask = (matrix!= 0)
    n = test_mask.sum()
    if verbose:
        print(f"Number of non-zero elements in the matrix: {n}")
    # 只对mask后的元素做Loss计算
    masked_matrix = matrix[test_mask]
    masked_reconstructed = reconstructed[test_mask]
    return torch.sqrt(torch.mean((masked_reconstructed - masked_matrix)**2))

# %% ../../notebooks/coding_projects/big_data_analytics/P2_Matrix-Decomposition/00matrix_decomposition.ipynb 111
import torch
import torch.nn as nn
import torch.optim as optim

# %% ../../notebooks/coding_projects/big_data_analytics/P2_Matrix-Decomposition/00matrix_decomposition.ipynb 113
class MatrixFactorization(nn.Module):
    def __init__(self, n_users:int, n_items:int, # 定义分解矩阵的大小
                 k # 隐向量维度
                 ):
        super().__init__()
        self.U = nn.Parameter(torch.randn(n_users, k))
        self.V = nn.Parameter(torch.randn(n_items, k))
    
    def forward(self):
        return torch.matmul(self.U, self.V.t())

# %% ../../notebooks/coding_projects/big_data_analytics/P2_Matrix-Decomposition/00matrix_decomposition.ipynb 116
# 再尝试一下jax
import jax
import jax.numpy as jnp
from jax import grad, jit, vmap # 这三个函数在 jax 中叫做 "transformations", 意思是对函数进行操作的函数（也可以说是泛函、算子），这三个函数分别作用是  求导，即时编译，向量化。
import jax.random as jrandom # 为了和 torch.random 做区分，我们导入叫做 jrandom

# %% ../../notebooks/coding_projects/big_data_analytics/P2_Matrix-Decomposition/00matrix_decomposition.ipynb 119
from flax import linen as jnn # 为了和 torch.nn 做区分，我们导入叫做 jnn，和flax官方的写法不同
from flax import nnx # 导入 nnx 库，里面包含了一些常用的网络层
from fastcore.all import store_attr # 导入 fastcore 基础库的 store_attr 函数，用来方便地存储类的属性，这样Python面向对象写起来不那么冗长。 请 pip install fastcore。
import treescope # flax 的 可视化

# %% ../../notebooks/coding_projects/big_data_analytics/P2_Matrix-Decomposition/00matrix_decomposition.ipynb 120
# 定义 MatrixFactorization 模型
# 注意 flax 使用了 Python标准库 `dataclasses`， 因为面向对象的定义风格更加简洁，使用类变量specify了init的参数。
class JaxMatrixFactorization(nnx.Module):
    def __init__(self, n_users:int, n_items:int, # 定义分解矩阵的大小
                 k:int, # 隐向量维度
                 *, rngs: nnx.Rngs # 在 jax 中随机种子非常重要。
                 ):
        super().__init__()
        # store_attr()
        key = rngs.params()
        self.U = nnx.Param(jrandom.normal(key, (n_users, k)))
        self.V = nnx.Param(jrandom.normal(key, (n_items, k)))
        # 如果我们还有子模块，flax要求把 rngs 传递下去。
    def __call__(self):
        # return jnp.dot(self.U, self.V.T) # 不能使用这个，因为 nnx.Param 和 jnp.array 不一样，会导致 jax 编译错误，这个是jax的bug。
        return self.U @ self.V.T

# %% ../../notebooks/coding_projects/big_data_analytics/P2_Matrix-Decomposition/00matrix_decomposition.ipynb 124
# 定义PyTorch的损失函数
def masked_mse_loss(reconstructed:torch.Tensor, matrix:torch.Tensor)->torch.Tensor:
    observed_indices = torch.where(matrix != 0) # A 矩阵，表示哪里是有评分的，只在有评分的地方算loss。
    return 0.5*torch.mean((reconstructed[observed_indices] - matrix[observed_indices])**2)

# 同理，定义 jax 损失函数
def jax_masked_mse_loss(reconstructed:jnp.ndarray, matrix:jnp.ndarray)->jnp.ndarray:
    observed_indices = jnp.where(matrix != 0) # A 矩阵，表示哪里是有评分的，只在有评分的地方算loss。
    return 0.5*jnp.mean((reconstructed[observed_indices] - matrix[observed_indices])**2)

# %% ../../notebooks/coding_projects/big_data_analytics/P2_Matrix-Decomposition/00matrix_decomposition.ipynb 131
import lightning as L # PyTorch Lightning库，这里我们只是用它来固定随机数种子

# %% ../../notebooks/coding_projects/big_data_analytics/P2_Matrix-Decomposition/00matrix_decomposition.ipynb 132
def train_matrix_factorization(X_train_dense:np.array, X_test_dense:np.array, k:int = 50, 
                    lmd:float = 2e-2, lr:float = 5e-3, max_epochs:int = 100000, 
                    required_delta_loss:float = 1e-2,
                    random_state=42, device = 'cuda:4'):
    # 设置 PyTorch 随机种子，防止结果不一致。
    L.seed_everything(random_state) 
    # 输入数据转为 PyTorch 张量
    X_train_torch = torch.from_numpy(X_train_dense).to(device)
    X_test_torch = torch.from_numpy(X_test_dense).to(device)
    # 模型定义
    m, n = X_train_torch.shape
    model = MatrixFactorization(m, n, k).to(device)
    model = torch.compile(model)
    # 优化器定义
    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=lmd)
    # 指标
    metrics = []
    # 优化循环
    bar = tqdm(range(max_epochs))
    previous_loss = 0
    for epoch in bar:
        optimizer.zero_grad()
        pred_matrix = model()
        loss = masked_mse_loss(pred_matrix, X_train_torch)
        loss.backward()
        optimizer.step()
        # 计算指标
        with torch.no_grad():
            # RMSE
            train_rmse = masked_rmse_loss(pred_matrix, X_train_torch, do_ensure_tensor=False)
            test_rmse = masked_rmse_loss(pred_matrix, X_test_torch, do_ensure_tensor=False)
            # loss 变化
            loss_item = loss.item()
            delta_loss = abs(loss_item - previous_loss)
            previous_loss = loss_item
            
            metric = dict(
                loss=loss_item,
                train_rmse = train_rmse.item(),
                test_rmse = test_rmse.item(), 
                delta_loss = delta_loss
            )
            # 指标记录
            metrics.append(metric)
            bar.set_postfix(**metric)
            # 中止条件
            if delta_loss<required_delta_loss:
                break
            
    return model, metrics

# %% ../../notebooks/coding_projects/big_data_analytics/P2_Matrix-Decomposition/00matrix_decomposition.ipynb 137
# 训练函数
# @nnx.jit  # nnx 提供的 算子，进行自动状态管理
@partial(nnx.jit, static_argnums=(3, ))  # 静态参数，用于固定模型和优化器参数, 这个是为了解决一些不能被 jit 但是可以hash的参数，这里不能这样解决。参考 https://jax.readthedocs.io/en/latest/errors.html#jax.errors.ConcretizationTypeError
def train_step(jmodel:JaxMatrixFactorization, joptimizer:nnx.Optimizer, X_train_jnp:jnp.ndarray, criterion=jax_masked_mse_loss):
    def loss_fn(jmodel:JaxMatrixFactorization):
        pred_matrix = jmodel()
        return criterion(pred_matrix, X_train_jnp)
    loss, grads = nnx.value_and_grad(loss_fn)(jmodel) # 这是另一个nnx算子，自动求导函数、
    joptimizer.update(grads)  # In place 的更新操作，更新优化器参数和模型参数。
    return loss

# %% ../../notebooks/coding_projects/big_data_analytics/P2_Matrix-Decomposition/00matrix_decomposition.ipynb 140
@jit
def compilable_jax_masked_mse_loss(reconstructed:jnp.ndarray, matrix:jnp.ndarray)->jnp.ndarray:
    mask = (matrix!=0)  # 获得一个 mask，在老师给的文档中也称为“指示矩阵”
    masked_reconstructed = mask * reconstructed # 把 matrix 没有评分的地方的 reconstructed 也变成0 （这里通过乘法，乘1乘0实现，也可以通过赋值实现）
    n = mask.sum() # 计算有评分的数量
    rmse = jnp.sqrt(((matrix-masked_reconstructed)**2).sum() / n) # 计算 RMSE，因为没有评分的地方大家都是0，所以计算出来是对的。
    return rmse

# %% ../../notebooks/coding_projects/big_data_analytics/P2_Matrix-Decomposition/00matrix_decomposition.ipynb 143
import optuna

# %% ../../notebooks/coding_projects/big_data_analytics/P2_Matrix-Decomposition/00matrix_decomposition.ipynb 144
# @jit
def train_matrix_factorization_jax(X_train_dense:np.array, X_test_dense:np.array, k:int = 50, 
                    lmd:float = 2e-2, lr:float = 5e-3, max_epochs:int = 100000, required_delta_loss:float = 1e-3,
                    random_state=42,
                    trial:optuna.Trial=None, critical_metric="test_rmse" # 用于下一章节的调优的 Pruning 
                    ):
    X_train_jnp = jnp.array(X_train_dense) # 从numpy array 转换为 jax array
    X_test_jnp = jnp.array(X_test_dense) # 从numpy array 转换为 jax array
    # 模型定义
    m, n = X_train_jnp.shape
    rngs=nnx.Rngs(params=random_state)
    jmodel = JaxMatrixFactorization(m, n, k, rngs=rngs)
    # 优化器
    joptimizer = nnx.Optimizer(jmodel, optax.adamw(learning_rate=lr, weight_decay=lmd))
    jmetrics = [] # 指标
    bar = tqdm(range(max_epochs))
    previous_loss:float = 0.0
    for epoch in bar:
        pred_matrix = jmodel()
        loss = train_step(jmodel, joptimizer, X_train_jnp, criterion=compilable_jax_masked_mse_loss)
        
        # 指标记录
        train_rmse = jnp.sqrt(loss)
        test_rmse = jnp.sqrt(compilable_jax_masked_mse_loss(pred_matrix, X_test_jnp))
        # 有关收敛
        loss_item = float(loss) # 把单个 scalar 转化为 float， 但是需要注意这个会让jit无法在外层编译
        delta_loss:float = abs(loss_item - previous_loss)
        previous_loss = loss_item
        
        metric = dict(
            loss = loss_item,  
            train_rmse = float(train_rmse),
            test_rmse = float(test_rmse), 
            delta_loss = delta_loss
        )
        # 指标的记录
        jmetrics.append(metric)
        bar.set_postfix(**metric)
        # optuna调参记录
        if trial is not None:
            for k, v in metric.items():
                trial.set_user_attr(k, v)
            trial.report(metric[critical_metric], step=epoch)
            if trial.should_prune():
                raise optuna.TrialPruned()

        # 收敛条件
        if delta_loss<required_delta_loss:
            break
    return jmodel, jmetrics

# %% ../../notebooks/coding_projects/big_data_analytics/P2_Matrix-Decomposition/00matrix_decomposition.ipynb 154
import pandas as pd
import plotly.graph_objects as go
from plotly.subplots import make_subplots

# %% ../../notebooks/coding_projects/big_data_analytics/P2_Matrix-Decomposition/00matrix_decomposition.ipynb 156
def draw_metrics_df(df:pd.DataFrame, title:str='Metrics'):
    # 使用 plotly 创建图表
    fig = go.Figure()
    # 需要支持第二纵轴
    fig = make_subplots(specs=[[{"secondary_y": True}]], figure=fig)

    # 绘制 loss 曲线
    fig.add_trace(go.Scatter(x=df.index, y=df['loss'], name='Loss', mode='lines'), secondary_y=False)
    fig.add_trace(go.Scatter(x=df.index[1:], y=df['delta_loss'][1:], name='Delta Loss', mode='lines'), secondary_y=False)

    # 绘制 train_rmse 和 test_rmse 曲线
    fig.add_trace(go.Scatter(x=df.index, y=df['train_rmse'], name='Train RMSE', mode='lines'), secondary_y=True)
    fig.add_trace(go.Scatter(x=df.index, y=df['test_rmse'], name='Test RMSE', mode='lines'), secondary_y=True)
    
    # 更新坐标轴
    fig.update_yaxes(title_text="Loss", titlefont_color="red", tickfont_color="red", secondary_y=False)
    fig.update_yaxes(title_text="RMSE", titlefont_color="blue", tickfont_color="blue", secondary_y=True)
    
    # 标题
    fig.update_layout(title=dict(
        text=title
    ),)
    return fig

# %% ../../notebooks/coding_projects/big_data_analytics/P2_Matrix-Decomposition/00matrix_decomposition.ipynb 164
from scholarly_infrastructure.rv_args.nucleus import RandomVariable, experiment_setting
from optuna.distributions import IntDistribution, FloatDistribution, CategoricalDistribution
from typing import Optional, Union
from dataclasses import asdict
import optuna

# %% ../../notebooks/coding_projects/big_data_analytics/P2_Matrix-Decomposition/00matrix_decomposition.ipynb 165
@experiment_setting
class MatrixFactorizationSetting:
    # 隐因子数量 k
    k: int = ~RandomVariable( # ~ 表示 服从 这个分布，这是我写的库实现的语法糖
        default=20,
        description="Number of latent factors.",
        # distribution=IntDistribution(low=1, high=128, log=True) # 我们假设 k 和 RMSE满足某种“scaling law”，所以使用 log 分布。
        distribution=CategoricalDistribution([1, 4, 16, 64]) 
        )
    
    # 正则化参数 λ
    lmd: float = ~RandomVariable(
        default=0.001,
        description="Regularization parameter.",
        distribution=FloatDistribution(0.001, 0.1, log=True)
    )
    
    # 学习率
    lr: float = ~RandomVariable(
        default = 5e-3, 
        description="Learning rate.",
        distribution=FloatDistribution(1e-4, 1e-1, log=True) # 虽然本次我们实验不调，但是其固有分布也是要写清楚的，下次实验或许要调。
    )
    
    # 精度要求
    required_delta_loss: float = ~RandomVariable(
        default = 1e-3, 
        description="Required Delta Loss, a.k.a. tolerance",
        distribution=FloatDistribution(1e-8, 1e-1, log=True)
    )      

# %% ../../notebooks/coding_projects/big_data_analytics/P2_Matrix-Decomposition/00matrix_decomposition.ipynb 168
fixed_meta_params = MatrixFactorizationSetting(lr=5e-3, # surprise库的值
                                               required_delta_loss=1e-4)
frozen_rvs = {"lr", "required_delta_loss"}

# %% ../../notebooks/coding_projects/big_data_analytics/P2_Matrix-Decomposition/00matrix_decomposition.ipynb 170
def objective(trial:optuna.Trial, critical_metric="test_rmse"):
    # experiment_setting 类具有 optuna_suggest 方法，可以自动推荐一个设置出来。
    config:MatrixFactorizationSetting = MatrixFactorizationSetting.optuna_suggest(
        trial, fixed_meta_params, frozen_rvs=frozen_rvs)
    # 调用上面写好的 train_matrix_factorization_jax 函数
    jmodel, jmetrics = train_matrix_factorization_jax(X_train_dense, X_test_dense, 
                                              trial=trial, critical_metric=critical_metric, # 为了进行搜索剪枝，传入这两个参数
                                              **asdict(config))
    # 假如采取最后的
    # best_metric = jmetrics[-1][critical_metric]
    # 假如允许 Early Stop
    best_metric = min(map(lambda m: m[critical_metric], jmetrics))
    return best_metric

# %% ../../notebooks/coding_projects/big_data_analytics/P2_Matrix-Decomposition/00matrix_decomposition.ipynb 172
from ..help import runs_path
from optuna.samplers import *
from optuna.pruners import *
import json

# %% ../../notebooks/coding_projects/big_data_analytics/P2_Matrix-Decomposition/00matrix_decomposition.ipynb 173
study_path = runs_path / "optuna_studies.db"
sqlite_url = f"sqlite:///{study_path}"

# %% ../../notebooks/coding_projects/big_data_analytics/P2_Matrix-Decomposition/00matrix_decomposition.ipynb 174
study = optuna.create_study(
    study_name="matrix factorization hpo 11.27 4.0", # 3.0 使用 1e-3
    storage=sqlite_url, 
    load_if_exists=True, 
    # sampler=QMCSampler(seed=42), # 谷歌建议
    sampler=TPESampler(seed=42), # 谷歌建议
    pruner=HyperbandPruner(), # 通过中间结果来决定是否停止搜索
    direction="minimize")
study.set_user_attr("contributors", ["Ye Canming"])
study.set_user_attr("fixed_meta_parameters", json.dumps(asdict(fixed_meta_params)))

# %% ../../notebooks/coding_projects/big_data_analytics/P2_Matrix-Decomposition/00matrix_decomposition.ipynb 201
from .anova import test_normality_group, homogeneity_of_variance
from scipy import stats
from statsmodels.stats.diagnostic import lilliefors
from .anova import auto_anova_for_df, auto_kruskal_for_df

# %% ../../notebooks/coding_projects/big_data_analytics/P2_Matrix-Decomposition/00matrix_decomposition.ipynb 202
# 根据上次我们ANOVA作业调研的结果，
def test_normality_small_sample(df, interesting_col, hue_col='群类别', transform=None):
    if transform is None:
        transform = lambda x: x
    grouped_data = df.groupby(hue_col)
    normality_results = {}
    for name, group in grouped_data:
        normality_result = {}
        data_column = group[interesting_col].to_numpy()
        data_column = transform(data_column)
        
        # Shapiro-Wilk test
        res = stats.shapiro(data_column)
        normality_result['Shapiro-Wilk'] = "Not Normal" if res.pvalue < 0.05 else "Normal"
        
        # Lilliefors test 
        res = lilliefors(data_column)
        normality_result['Lilliefors'] = "Not Normal" if res[1] < 0.05 else "Normal"
        
        normality_results[name] = normality_result
    return pd.DataFrame(normality_results)

# %% ../../notebooks/coding_projects/big_data_analytics/P2_Matrix-Decomposition/00matrix_decomposition.ipynb 212
import scikit_posthocs as sp
from scikit_posthocs import posthoc_dunn
