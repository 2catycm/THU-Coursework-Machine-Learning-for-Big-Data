# AUTOGENERATED! DO NOT EDIT! File to edit: ../../notebooks/coding_projects/big_data_analytics/P2_Matrix-Decomposition/00matrix_decomposition.ipynb.

# %% auto 0
__all__ = ['get_rating_matrix', 'compute_weighted_sum_on_matrix', 'get_X_train_weighted', 'MatrixFactorization',
           'masked_mse_loss']

# %% ../../notebooks/coding_projects/big_data_analytics/P2_Matrix-Decomposition/00matrix_decomposition.ipynb 56
# @joblib_memory.cache # data 哈希本身需要很大的开销，不宜 cache
def get_rating_matrix(data:pd.DataFrame)-> csr_matrix:
    rows = data['user_id'].map(user_to_row)
    cols = data['movie_id'].map(movie_to_col)
    values = data['rating']
    
    return csr_matrix((values, (rows, cols)), shape=(len(user_to_row), len(movie_to_col)))

# %% ../../notebooks/coding_projects/big_data_analytics/P2_Matrix-Decomposition/00matrix_decomposition.ipynb 79
def compute_weighted_sum_on_matrix(cosine_sim, X_train_dense_nan):
# 创建一个与 X_train_dense 相同大小的矩阵，用于存储加权平均数
    X_train_weighted = np.zeros_like(X_train_dense_nan)

    # 遍历 X_train_dense 的每一行
    for i in tqdm(range(X_train_dense_nan.shape[0])):
        # 获取第 i 行的权重
        weights = cosine_sim[i, :]
        
        # 复制这个权重到整个矩阵的维度，方便后面掩码操作
        weights = np.repeat(weights, X_train_dense_nan.shape[1]).reshape(X_train_dense_nan.shape[0], X_train_dense_nan.shape[1])
        
        
        # 创建一个掩码，是 nan的就是True
        mask = np.isnan(X_train_dense_nan)
        
        
        # 将权重中的对应位置设置为 np.nan
        weights = np.where(mask, np.nan, weights)
        
        # 计算加权平均数，忽略 np.nan 值
        X_train_weighted[i, :] = np.nansum(X_train_dense_nan * weights, axis=0) / np.nansum(weights, axis=0)

    # X_train_weighted 现在是一个 mxn 的矩阵，其中每一行是忽略 np.nan 的加权平均数
    return X_train_weighted

@joblib_memory.cache
def get_X_train_weighted():
    return compute_weighted_sum_on_matrix(cosine_sim, X_train_dense_nan)

# %% ../../notebooks/coding_projects/big_data_analytics/P2_Matrix-Decomposition/00matrix_decomposition.ipynb 107
class MatrixFactorization(nn.Module):
    def __init__(self, n_users, n_items, k):
        super().__init__()
        self.U = nn.Parameter(torch.randn(n_users, k))
        self.V = nn.Parameter(torch.randn(n_items, k))
    
    def forward(self):
        return torch.matmul(self.U, self.V.t())

# %% ../../notebooks/coding_projects/big_data_analytics/P2_Matrix-Decomposition/00matrix_decomposition.ipynb 109
# 定义PyTorch的损失函数
def masked_mse_loss(reconstructed:torch.Tensor, matrix:torch.Tensor)->torch.Tensor:
    observed_indices = torch.where(matrix != 0) # A 矩阵，表示哪里是有评分的，只在有评分的地方算loss。
    return 0.5*torch.mean((reconstructed[observed_indices] - matrix[observed_indices])**2)

