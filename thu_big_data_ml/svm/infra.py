# AUTOGENERATED! DO NOT EDIT! File to edit: ../../notebooks/coding_projects/P2_SVM/00svm.ipynb.

# %% auto 0
__all__ = ['ReturnType', 'Strategy', 'scheduler_lmd_leon_bottou_sgd', 'scheduler_lmd_leon_bottou_asgd', 'search_space', 'verbose',
           'sklearn_to_X_y_categories', 'make_train_val_test', 'get_torch_dataset', 'process_sklearn_dataset_dict',
           'BinaryHingeLoss', 'MultiClassHingeLoss', 'get_max_values_without_true', 'HingeSupportVectorClassifier',
           'separate_weight_decay', 'SupportVectorClassifierConfig', 'SupportVectorClassifier', 'evaluate_knn',
           'objective', 'regplot', 'fast_build_kd_tree', 'fast_search_kd_tree', 'FastKDTree']

# %% ../../notebooks/coding_projects/P2_SVM/00svm.ipynb 15
import pandas as pd
import numpy as np

# %% ../../notebooks/coding_projects/P2_SVM/00svm.ipynb 16
def sklearn_to_X_y_categories(dataset_dict):
    X = dataset_dict['data']
    y = dataset_dict['target']
    if isinstance(X, pd.DataFrame):
        X:np.array = X.values
    if isinstance(y, pd.Series):
        y:np.array = y.values
    # if y.dtype.name == 'category':
    #     categories = y.dtype.categories
    # else:
    X = X.astype(np.float32)
    y = y.astype(np.int64)
    categories = np.unique(y)
    # print(str((X.shape, X.dtype, y.shape, y.dtype, categories)))
    print(X.shape, X.dtype, y.shape, y.dtype, categories)
    return X, y, categories


# %% ../../notebooks/coding_projects/P2_SVM/00svm.ipynb 19
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# %% ../../notebooks/coding_projects/P2_SVM/00svm.ipynb 20
def make_train_val_test(X, y, val_size=0.1, test_size=0.2, random_state=42, normalize=True):
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, 
                                                        stratify=y)
    # print(len(X_train), len(X_test))
    if normalize:
        scaler = StandardScaler()
        X_train = scaler.fit_transform(X_train)
        X_test = scaler.transform(X_test)
    # 进一步划分出验证集，用于调参、early stopping等。
    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=42, 
                                                    stratify=y_train)
    print(len(X_train), len(X_val), len(X_test))
    return X_train, X_val, X_test, y_train, y_val, y_test


# %% ../../notebooks/coding_projects/P2_SVM/00svm.ipynb 23
import torch
import lightning as L

# %% ../../notebooks/coding_projects/P2_SVM/00svm.ipynb 24
def get_torch_dataset(X, y):
    X_tensor = torch.tensor(X, dtype=torch.float32)
    y_tensor = torch.tensor(y, dtype=torch.long)
    dataset = torch.utils.data.TensorDataset(X_tensor, y_tensor)
    return dataset

# %% ../../notebooks/coding_projects/P2_SVM/00svm.ipynb 27
from typing import Literal

# %% ../../notebooks/coding_projects/P2_SVM/00svm.ipynb 28
ReturnType = Literal['numpy', 'torch', 'lightning', 'pandas']

def process_sklearn_dataset_dict(dataset_dict:dict, return_type:ReturnType):
    X, y, categories = sklearn_to_X_y_categories(dataset_dict)
    X_train, X_val, X_test, y_train, y_val, y_test = make_train_val_test(X, y)
    train_set = get_torch_dataset(X_train, y_train)
    val_set = get_torch_dataset(X_val, y_val)
    test_set = get_torch_dataset(X_test, y_test)
    data_module = L.LightningDataModule.from_datasets(
        train_dataset=train_set, 
            val_dataset=val_set, 
            test_dataset=test_set, 
            predict_dataset=test_set, 
            batch_size=128,  
            num_workers=4
        )
    if return_type == 'numpy':
        return X_train, X_val, X_test, y_train, y_val, y_test
    elif return_type == 'torch':
        return train_set, val_set, test_set
    elif return_type == 'lightning':
        return data_module  
    elif return_type == 'pandas':
        raise NotImplementedError("Pandas not implemented yet") # 这里可以用 dataset_dict 的 frame, 但是 train test split 还有预处理。

# %% ../../notebooks/coding_projects/P2_SVM/00svm.ipynb 33
from namable_classify.metrics import compute_classification_metrics

# %% ../../notebooks/coding_projects/P2_SVM/00svm.ipynb 42
import torch.nn as nn
from fastcore.all import store_attr

# %% ../../notebooks/coding_projects/P2_SVM/00svm.ipynb 43
class BinaryHingeLoss(nn.Module):
    """
    Binary Hinge Loss. 
    For SVM, 
    $$
    \min_{w, b}  \frac{1}{2} \lVert w \rVert^2 + C \sum_{i=1}^N \left[ 1 - y_i(w \cdot x_i + b) \right]_+
    $$
    we compute 
    $$
    C \sum_{i=1}^N \left[ 1 - y_i(w \cdot x_i + b) \right]_+
    $$
    """
    def __init__(self, C=1.0, 
                 squared = False, 
                 margin = 1.0,
                 ):
        super().__init__()
        store_attr() # 保存参数到实例变量中

    def forward(self, y_pred_logits:torch.Tensor, y_true:torch.Tensor)->torch.Tensor:
        functional_margin = y_true * y_pred_logits # 函数间隔
        how_small_than_required_margin = self.margin - functional_margin
        xi = torch.clamp(how_small_than_required_margin, min=0) # 计算 xi 也就是 松弛变量
        if self.squared:
            xi = xi ** 2
        return self.C * xi.sum()


# %% ../../notebooks/coding_projects/P2_SVM/00svm.ipynb 49
from fastcore.all import patch

# %% ../../notebooks/coding_projects/P2_SVM/00svm.ipynb 50
Strategy = Literal['crammer_singer', 'one_vs_all']
class MultiClassHingeLoss(nn.Module):
    """MultiClassHingeLoss"""
    def __init__(self, C=1.0, 
                 squared = False, 
                 margin = 1.0,
                 strategy: Strategy = 'crammer_singer',
                #  *args, **kwargs
                 ):
        super().__init__()
        store_attr()
        self.binary_critieria = None
    def forward(self, y_pred_logits:torch.Tensor, 
                y_true:torch.Tensor # 并非 one hot 编码，而是 int/long 类型 的 label
                )->torch.Tensor:
        if self.strategy == 'crammer_singer':
            return self.forward_crammer_singer(y_pred_logits, y_true)
        elif self.strategy == 'one_vs_all':
            return self.forward_one_vs_all(y_pred_logits, y_true)
        else:
            raise ValueError(f"Invalid strategy: {self.strategy}")
    def forward_one_vs_all(self, y_pred_logits:torch.Tensor, y_true:torch.Tensor)->torch.Tensor:
        num_of_classes = y_pred_logits.size(1)
        if self.binary_critieria is None:
            self.binary_critieria = nn.ModuleList([
                BinaryHingeLoss(C=self.C, squared=self.squared, margin=self.margin)
                for _ in range(num_of_classes)
                ])
        losses = []
        for k, critierion in enumerate(self.binary_critieria):
            y_true_binary = 2 * (y_true == k) - 1 # 转换为 -1/1 编码、
            y_pred_that_class = y_pred_logits[:, k]
            loss = critierion(y_pred_that_class, y_true_binary)
            losses.append(loss)
        return sum(losses)
    def forward_crammer_singer(self, y_pred_logits:torch.Tensor, y_true:torch.Tensor)->torch.Tensor: ...

# %% ../../notebooks/coding_projects/P2_SVM/00svm.ipynb 52
import torch

# %% ../../notebooks/coding_projects/P2_SVM/00svm.ipynb 53
def get_max_values_without_true(y_pred_logits, y_true):
    """
    获取去掉y_true对应元素后，y_pred_logits每行的最大值。

    参数:
    y_pred_logits: torch.Tensor, 形状为 (N, K)
    y_true: torch.Tensor, 形状为 (N,)

    返回:
    torch.Tensor, 形状为 (N,), 去掉y_true对应元素后每行的最大值
    """
    # 将y_true转换为适当的索引格式
    indices = y_true.unsqueeze(1).expand_as(y_pred_logits)

    # 创建一个与y_pred_logits形状相同的掩码，真实标签位置为False，其余为True
    mask = torch.ones_like(y_pred_logits, dtype=torch.bool)
    mask.scatter_(1, indices, False)

    # 使用掩码来排除y_true对应的列，并计算每一行的最大值
    max_values = y_pred_logits[mask].view(y_pred_logits.size(0), -1).max(dim=1)[0]

    return max_values

# %% ../../notebooks/coding_projects/P2_SVM/00svm.ipynb 59
@patch
def forward_crammer_singer(self:MultiClassHingeLoss, y_pred_logits:torch.Tensor, y_true:torch.Tensor)->torch.Tensor:
    """L_i = \left[ 1 - (\hat{y_i}^{t_i} - \max_{k \neq t_i} \hat{y_i}^k)) \right]_+"""
    batch_size, num_classes = y_pred_logits.size()
    y_true_one_hot = torch.eye(num_classes).to(y_pred_logits.device)[y_true]
    
    # 计算真实类别的预测值
    y_true_logits = (y_pred_logits * y_true_one_hot).sum(dim=1)
    # y_true_logits = y_pred_logits[:, y_true]

    max_other_logits = get_max_values_without_true(y_pred_logits, y_true)
    
    functional_margin_differences = (y_true_logits - max_other_logits)
    
    # 计算hinge loss
    xi = torch.clamp(self.margin - functional_margin_differences, min=0)
    
    if self.squared:
        xi = xi ** 2
    return self.C * xi.sum()

# %% ../../notebooks/coding_projects/P2_SVM/00svm.ipynb 67
from overrides import override
from lightning.pytorch.utilities.types import EVAL_DATALOADERS, TRAIN_DATALOADERS, STEP_OUTPUT, OptimizerLRScheduler
import torch.optim as optim

# %% ../../notebooks/coding_projects/P2_SVM/00svm.ipynb 68
class HingeSupportVectorClassifier(L.LightningModule):
    def __init__(self, 
                #  model related
                 input_dim, num_classes,  
                #  optimization related
                 learning_rate=0.01, weight_decay=0.5, 
                 optimizer_type = optim.SGD, 
                #  loss related
                 C: float = 1,
                    squared: bool = False,
                    margin: float = 1,
                    strategy: Strategy = 'crammer_singer',
                #  experiment related
                 experiment_index=0,  
                 ):
        super().__init__()
        self.save_hyperparameters()
        L.seed_everything(experiment_index)
        self.model = nn.Linear(input_dim, num_classes)
        self.loss_fn = MultiClassHingeLoss()
        
        self.example_input_array = torch.randn(1, input_dim)
        self.dummy_inputs = dict(input_ids=self.example_input_array) # for opendelta and huggingface
        self.automatic_optimization = True
        # 评价策略
        self.evaluation_steps_outputs = dict()
        
    @override
    def forward(self, image_tensor:torch.Tensor, *args, **kwargs)-> torch.Tensor:
        """
        Returns:
            torch.Tensor: the predicted functional margin to each class's decision hyperplane   
        """
        return self.model(image_tensor)
    
    def predict_geometric_margin(self, image_tensor:torch.Tensor)->torch.Tensor:
        w_norm_each_line = torch.norm(self.model.weight, dim=1)
        return torch.clamp(self(image_tensor) / w_norm_each_line, min=0)
    
    def predict_class(self, image_tensor:torch.Tensor)->torch.Tensor:
        return torch.argmax(self(image_tensor), dim=1)

    def forward_loss(self, image_tensor: torch.Tensor, label_tensor:torch.Tensor)->torch.Tensor:
        logits = self(image_tensor)
        return self.loss_fn(logits, label_tensor)
        
    @override
    def training_step(self, batch, batch_idx=None, *args, **kwargs)-> STEP_OUTPUT:
        loss = self.forward_loss(*batch)
        self.log("train_loss", loss, prog_bar=True)
        return loss

# %% ../../notebooks/coding_projects/P2_SVM/00svm.ipynb 72
scheduler_lmd_leon_bottou_sgd = lambda epoch, init_lr=0.1, lmd=0.001: init_lr / (1+ lmd*init_lr*epoch)
scheduler_lmd_leon_bottou_asgd = lambda epoch, init_lr=0.1, lmd=0.001: init_lr / (1+ lmd*init_lr*epoch)**0.75


# %% ../../notebooks/coding_projects/P2_SVM/00svm.ipynb 74
def separate_weight_decay(model:nn.Module, weight_decay: float, otherwise_set_to:float=0.0, verbose:bool=False):
    decay = list() # 不能使用 set，由于Pytorch优化器需要顺序
    no_decay = list()
    for name, param in model.named_parameters():
        do_weight_decay = 'weight' in name
        if verbose:
            print(f'{name} should do weight decay? {do_weight_decay}')
        if do_weight_decay:
            decay.append(param)
        else:
            no_decay.append(param)
    # return decay, no_decay
    return [
                dict(params=decay, weight_decay=weight_decay), 
                dict(params=no_decay, weight_decay=otherwise_set_to)
            ]

# %% ../../notebooks/coding_projects/P2_SVM/00svm.ipynb 76
@override    
@patch
def configure_optimizers(self:HingeSupportVectorClassifier) -> OptimizerLRScheduler:
    init_lr = self.hparams.learning_rate
    # lmd = self.hparams.weight_decay / self.hparams.C 
    lmd = self.hparams.weight_decay # 需要考证，Leon Bottou的lamda 是什么情况下推导的。

    weight_decay = self.hparams.weight_decay
    
    if self.hparams.optimizer_type == torch.optim.ASGD:
        optimizer = torch.optim.ASGD(
            # self.parameters(), 
            separate_weight_decay(self, weight_decay), 
                                    lr=init_lr,  # 刚才 save_hyperparameters() 保存了，这是为了方便是使用 Lightning 调学习率
                                    # weight_decay = self.hparams.weight_decay, # 李航书上的 hinge loss的第一项
                                    ) 
        scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer,
                lr_lambda=lambda epoch: scheduler_lmd_leon_bottou_asgd(epoch, init_lr, lmd))
    elif self.hparams.optimizer_type == torch.optim.SGD:
        optimizer = torch.optim.SGD(
            # self.parameters(), 
            separate_weight_decay(self, weight_decay), 
                                    lr=init_lr,
                                    # weight_decay=self.hparams.weight_decay
                                    )
        scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer,
                lr_lambda=lambda epoch: scheduler_lmd_leon_bottou_sgd(epoch, init_lr, lmd))
    else:
        return self.hparams.optimizer_type(self.parameters(), lr=init_lr, weight_decay=self.hparams.weight_decay) # 只 return optimizer
        
    return ([optimizer], [scheduler])


# %% ../../notebooks/coding_projects/P2_SVM/00svm.ipynb 78
from namable_classify.utils import append_dict_list, ensure_array
from typing import Any
import numpy as np

# %% ../../notebooks/coding_projects/P2_SVM/00svm.ipynb 79
@patch
def on_evaluation_epoch_start(self:HingeSupportVectorClassifier, stage:str=""):
    self.evaluation_steps_outputs = dict()
    self.evaluation_steps_outputs[f'{stage}_batch_probs'] = []
    self.evaluation_steps_outputs[f'{stage}_label_tensor'] = []
    # self.evaluation_steps_outputs[f'{stage}_batch_preds'] = []
        
@patch
def evaluation_step(self:HingeSupportVectorClassifier, batch, batch_idx=None, stage:str="", *args: Any, **kwargs: Any) -> STEP_OUTPUT:
    image_tensor, label_tensor = batch
    batch_probs = self(image_tensor)
    # batch_preds = self.predict_class(image_tensor)
    append_dict_list(self.evaluation_steps_outputs, f'{stage}_batch_probs', ensure_array(batch_probs))
    append_dict_list(self.evaluation_steps_outputs, f'{stage}_label_tensor', ensure_array(label_tensor))
    # append_dict_list(self.evaluation_steps_outputs, f'{stage}_batch_preds', ensure_array(batch_preds))
    batch_loss = self.loss_fn(batch_probs, label_tensor)
    self.log(f"{stage}_loss", batch_loss, prog_bar=True)
    return batch_loss
        
@patch
def on_evaluation_epoch_end(self:HingeSupportVectorClassifier, stage:str=""):
    # https://github.com/Lightning-AI/pytorch-lightning/discussions/9845
    # labels = self.lit_data.classes
    # labels = list(range(self.lit_data.num_of_classes))
    labels = list(range(self.hparams.num_classes))
    # labels = None
    # print(labels)
    # stack 是 new axis， concat是existing axis
    all_pred_probs = np.concatenate(self.evaluation_steps_outputs[f'{stage}_batch_probs'])
    all_label_tensor = np.concatenate(self.evaluation_steps_outputs[f'{stage}_label_tensor'])
    # all_preds = np.concatenate(self.evaluation_steps_outputs[f'{stage}_batch_preds'])
    # logger.debug(self.evaluation_steps_outputs[f'{stage}_label_tensor'])
    # logger.debug(all_label_tensor)
    eval_dict = compute_classification_metrics(all_label_tensor, all_pred_probs, 
                                            #    logits_to_prob=False, 
                                                logits_to_prob=True, 
                                            labels=labels)
    eval_dict = {f"{stage}_{k}": v for k,v in eval_dict.items()}
    self.log_dict(eval_dict)
    self.evaluation_steps_outputs.clear()

@override
@patch
def on_validation_epoch_start(self:HingeSupportVectorClassifier):
    return self.on_evaluation_epoch_start(stage="val")

@override
@patch
def on_test_epoch_start(self:HingeSupportVectorClassifier):
    return self.on_evaluation_epoch_start(stage="test")

@override
@patch
def on_validation_epoch_end(self:HingeSupportVectorClassifier):
    return self.on_evaluation_epoch_end(stage="val")

@override
@patch
def on_test_epoch_end(self:HingeSupportVectorClassifier):
    return self.on_evaluation_epoch_end(stage="test")

@override
@patch
def validation_step(self:HingeSupportVectorClassifier, batch, batch_idx=None, *args, **kwargs):
    return self.evaluation_step(batch, batch_idx, stage="val")

@override
@patch
def test_step(self:HingeSupportVectorClassifier, batch, batch_idx=None, *args, **kwargs):
    return self.evaluation_step(batch, batch_idx, stage="test")


# %% ../../notebooks/coding_projects/P2_SVM/00svm.ipynb 102
from scholarly_infrastructure.rv_args.nucleus import RandomVariable, experiment_setting
from optuna.distributions import IntDistribution, FloatDistribution, CategoricalDistribution
from typing import Optional, Union

# %% ../../notebooks/coding_projects/P2_SVM/00svm.ipynb 103
@experiment_setting
class SupportVectorClassifierConfig:
    # 惩罚系数 C
    C: float = ~RandomVariable(
        default=1.0,
        description="Regularization parameter. The strength of the regularization is inversely proportional to C.",
        distribution=FloatDistribution(1e-5, 1e2, log=True)
    )
    
    # 核函数类型
    kernel: str = ~RandomVariable(
        default="rbf",
        description="Kernel type to be used in the algorithm.",
        distribution=CategoricalDistribution(choices=["linear", "poly", "rbf", "sigmoid", "precomputed"])
    )
    
    # 多项式核函数的度数
    degree: int = ~RandomVariable(
        default=3,
        description="Degree of the polynomial kernel function ('poly').",
        distribution=IntDistribution(1, 10, log=False)
    )
    
    # 核函数系数 gamma
    gamma: Union[str, float] = ~RandomVariable(
        default="scale",
        description="Kernel coefficient for 'rbf', 'poly' and 'sigmoid'.",
        distribution=CategoricalDistribution(choices=["scale", "auto"])  # 可以添加浮点数分布视需求
    )
    
    # 核函数独立项 coef0
    coef0: float = ~RandomVariable(
        default=0.0,
        description="Independent term in kernel function. It is significant in 'poly' and 'sigmoid'.",
        distribution=FloatDistribution(0, 1)
    )
    
    # 收缩启发式算法
    shrinking: bool = ~RandomVariable(
        default=True,
        description="Whether to use the shrinking heuristic.",
        distribution=CategoricalDistribution(choices=[True, False])
    )
    
    # 是否启用概率估计
    probability: bool = ~RandomVariable(
        default=False,
        description="Whether to enable probability estimates. Slows down fit when enabled.",
        distribution=CategoricalDistribution(choices=[True, False])
    )
    
    # 停止准则的容差 tol
    tol: float = ~RandomVariable(
        default=1e-3,
        description="Tolerance for stopping criterion.",
        distribution=FloatDistribution(1e-6, 1e-1, log=True)
    )
    
    # 内核缓存的大小（MB）
    cache_size: float = ~RandomVariable(
        default=200,
        description="Specify the size of the kernel cache (in MB).",
        distribution=FloatDistribution(50, 500, log=False)
    )
    
    # 类别权重 class_weight
    class_weight: Optional[Union[dict, str]] = ~RandomVariable(
        default=None,
        description="Set C of class i to class_weight[i]*C or use 'balanced' to adjust weights inversely to class frequencies.",
        distribution=CategoricalDistribution(choices=[None, "balanced"])
    )
    
    # 是否启用详细输出
    verbose: bool = ~RandomVariable(
        default=False,
        description="Enable verbose output (may not work properly in a multithreaded context).",
        distribution=CategoricalDistribution(choices=[True, False])
    )
    
    # 最大迭代次数
    max_iter: int = ~RandomVariable(
        default=-1,
        description="Hard limit on iterations within solver, or -1 for no limit.",
        distribution=IntDistribution(-1, 1000, log=False)
    )
    
    # 决策函数形状
    decision_function_shape: str = ~RandomVariable(
        default="ovr",
        description="Whether to return a one-vs-rest ('ovr') decision function or original one-vs-one ('ovo').",
        distribution=CategoricalDistribution(choices=["ovo", "ovr"])
    )
    
    # 是否打破决策函数平局
    break_ties: bool = ~RandomVariable(
        default=False,
        description="If True, break ties according to the confidence values of decision_function when decision_function_shape='ovr'.",
        distribution=CategoricalDistribution(choices=[True, False])
    )
    
    # 随机种子 random_state
    random_state: Optional[int] = ~RandomVariable(
        default=None,
        description="Controls random number generation for probability estimates. Ignored when probability=False.",
        distribution=IntDistribution(0, 100)  # 根据需求设置范围
    )

# %% ../../notebooks/coding_projects/P2_SVM/00svm.ipynb 109
from fastcore.all import patch, store_attr
class SupportVectorClassifier:
    """自己实现的支持向量机分类器。需要支持多分类。
    """
    def __init__(self, config:SupportVectorClassifierConfig):
        store_attr()
        

# %% ../../notebooks/coding_projects/P2_SVM/00svm.ipynb 113
from sklearn.metrics.pairwise import distance_metrics
from ray import train, tune
# https://docs.ray.io/en/latest/tune/tutorials/tune-search-spaces.html
search_space = dict(
    # weights = tune.grid_search(["uniform", "distance"]) 
    weights = tune.choice(["uniform", "distance"]) # 目标元参数， 我们的零假设是这两个distance不优于uniform，备择假设是distance更好。
    ,n_neighbors = tune.randint(1, 20)  # 随机取整数。 TODO 我们还可以用左偏正态分布来建模这个参数的先验分布。
    # ray tune也能处理条件分布，但是太复杂了，我们避免`p`参数依赖于`metric`参数生效的问题，我们换成choice来处理。
    ,distance_metric = tune.choice([k for k in distance_metrics().keys() if k not in ['precomputed', 'haversine']]) # grid_search 是要求必须遍历的，而choice是随机选择。
)

# %% ../../notebooks/coding_projects/P2_SVM/00svm.ipynb 116
# from sklearn.model_selection import cross_val_score
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import KFold
def evaluate_knn(weights:str, n_neighbors:int, distance_metric:str, random_seed:int = 42):
    knn = KNeighborsClassifier(n_neighbors=n_neighbors, weights=weights, metric=distance_metric)

    # 使用k fold交叉验证，相当于做了5次独立实验。
    kf = KFold(n_splits=5, shuffle=True, random_state=random_seed)
    
    # 初始化存储每次交叉验证的分数
    scores = []
    
    # 进行5折交叉验证
    for train_index, test_index in kf.split(X_train):
        # 分割训练集和测试集
        X_train_fold, X_test_fold = X_train[train_index], X_train[test_index]
        y_train_fold, y_test_fold = y_train[train_index], y_train[test_index]
        
        # 创建KNN分类器实例
        knn = KNeighborsClassifier(n_neighbors=n_neighbors, weights=weights, metric=distance_metric)
        
        # 训练模型
        knn.fit(X_train_fold, y_train_fold)
        
        # 预测测试集
        y_pred = knn.predict(X_test_fold)
        
        # 计算准确率
        score = accuracy_score(y_test_fold, y_pred)
        scores.append(score)
    return scores
# 测试下函数能不能跑
evaluate_knn(random_seed=43, weights='uniform', n_neighbors=5, distance_metric='euclidean')

# %% ../../notebooks/coding_projects/P2_SVM/00svm.ipynb 117
# 符合optuna接口
def objective(meta_parameters):
    scores = evaluate_knn(**meta_parameters)
    return dict(
        mean_score=sum(scores)/len(scores),
        std_score=np.std(scores),
                )|{f"score_{i}":score for i,score in enumerate(scores)}

# %% ../../notebooks/coding_projects/P2_SVM/00svm.ipynb 144
# 为了解决seaborn开发者不愿意支持用户看到拟合曲线参数的问题，我们查找到了下面的解决方案
# 本代码参考 https://stackoverflow.com/questions/22852244/how-to-get-the-numerical-fitting-results-when-plotting-a-regression-in-seaborn
def regplot(
    *args,
    line_kws=None,
    marker=None,
    scatter_kws=None,
    **kwargs
):
    # this is the class that `sns.regplot` uses
    plotter = sns.regression._RegressionPlotter(*args, **kwargs)

    # this is essentially the code from `sns.regplot`
    ax = kwargs.get("ax", None)
    if ax is None:
        ax = plt.gca()

    scatter_kws = {} if scatter_kws is None else copy.copy(scatter_kws)
    scatter_kws["marker"] = marker
    line_kws = {} if line_kws is None else copy.copy(line_kws)

    plotter.plot(ax, scatter_kws, line_kws)

    # unfortunately the regression results aren't stored, so we rerun
    grid, yhat, err_bands = plotter.fit_regression(plt.gca())

    # also unfortunately, this doesn't return the parameters, so we infer them
    slope = (yhat[-1] - yhat[0]) / (grid[-1] - grid[0])
    intercept = yhat[0] - slope * grid[0]
    return slope, intercept

# %% ../../notebooks/coding_projects/P2_SVM/00svm.ipynb 155
from typing import Callable

# %% ../../notebooks/coding_projects/P2_SVM/00svm.ipynb 156
verbose = False
def fast_build_kd_tree(X, axis_order_list:list, strategy = "median", depth=0):
    if len(X) == 0:
        return None
    k = X.shape[1]
    # 根据当前深度，选择划分的维度
    # 原本是 
    # axis = depth % k 
    # axis = axis_order(depth, k)
    assert len(axis_order_list)==k, f"axis_order_list length should be equal to k, but got {len(axis_order_list)} with {axis_order_list}"
    axis = axis_order_list[depth % k]
    if verbose:
        print(" "*4*depth + f"Building kd-tree of depth {depth} with {len(X)} points, axis is {axis}")
    
    # 中位数策略
    if strategy == "median":
        X = X[X[:, axis].argsort()]
        median = X.shape[0] // 2 #将当前结点数据一分为二
        # assert len(np.where((X_train == X[median]).all(axis=1)))>0
        return Node(data=X[median], left=fast_build_kd_tree(X[:median], axis_order_list, strategy, depth + 1),
                    right=fast_build_kd_tree(X[median + 1:],axis_order_list, strategy,  depth + 1))
    
    # 中点策略
    else:
        x_axis = X[:, axis] # n个数
        middle_point_value = (x_axis.max()-x_axis.min()) / 2
        left_points = X[x_axis <= middle_point_value]
        right_points = X[x_axis > middle_point_value]
        distances_on_axis_to_middle = abs(x_axis - middle_point_value)
        closest_point_to_middle = distances_on_axis_to_middle.argmin()
        
        return Node(data=X[closest_point_to_middle], left=fast_build_kd_tree(left_points, axis_order_list,strategy, depth + 1), right=fast_build_kd_tree(right_points, axis_order_list, strategy, depth + 1))
    

# %% ../../notebooks/coding_projects/P2_SVM/00svm.ipynb 157
from queue import PriorityQueue

# %% ../../notebooks/coding_projects/P2_SVM/00svm.ipynb 158
def fast_search_kd_tree(tree, target, axis_order_list:list, k=3):
    if tree is None:
        return []
    # k_nearest = [] #list用于储存target当前遍历到的k个k近邻
    # 我们使用优先队列来储存k_nearest，从而提高效率，优先队列中的元素为(-距离, 节点)的元组，距离远的先取出来
    # k_nearest_pq = PriorityQueue(maxsize=k)
    k_nearest_pq = PriorityQueue()
    entry_count = 0
    stack = [(tree, 0)] #用于储存待遍历节点的stack
    while stack:
        node, depth = stack.pop() # 节点出栈
        if node is None:
            continue
        # print(" "*4*depth + f"node: {node.data}, depth: {depth}")
        distance = euclidean_distance(target, node.data) #计算需要分类的目标点与节点的距离
        
        # 调换到前面
        axis = depth % target.shape[0] #计算当前深度对应的划分维度
        axis = axis_order_list[axis] # 新增加
        # print(axis)
        
        axis_diff = target[axis] - node.data[axis] #计算该维度下目标点与当前节点的差
        
        #如果k_nearest未装满或k_nearest中相距目标点最远的点与目标点的距离大于axis_diff的绝对值时，则另一边的子树也入栈
        can_omit_another_side = True
        # if len(k_nearest) < k: # 当k_nearest未装满时，直接将节点放入
        # if k_nearest.qsize() < k: # 当k_nearest未装满时，直接将节点放入
        if k_nearest_pq.qsize() < k: # 当k_nearest未装满时，直接将节点放入
        # if not k_nearest_pq.full(): # 当k_nearest未装满时，直接将节点放入
            # print(f"not full, put {(-distance, node)}")
            pass #BLANK_1
            # k_nearest.append((node, distance))
            k_nearest_pq.put((-distance, entry_count, node))
            entry_count+=1
            can_omit_another_side = False
        else: #当k_nearest装满时，对比该节点与k_nearest中与目标点距离最远的节点的距离，如果小于则替换，如果大于则不替换
            pass #BLANK_2
            farthest = k_nearest_pq.get()
            farthest_distance = -farthest[0]
            # print(f"full, farthest: {farthest}")
            if distance < farthest_distance:
                # print(f"closer, put {(-distance, node)}")
                # assert isinstance(distance, float)
                # print(k_nearest_pq.queue)
                k_nearest_pq.put((-distance, entry_count, node))
                entry_count+=1
            else:
                k_nearest_pq.put(farthest)
            

            if farthest_distance > abs(axis_diff): 
                can_omit_another_side = False
        
        if axis_diff <= 0: #当差小于0时则，该节点的左子树入栈 #如果k_nearest未装满或k_nearest中相距目标点最远的点与目标点的距离大于axis_diff的绝对值时，则右子树也入栈
            pass #BLANK_3
            stack.append((node.left, depth+1))
            if not can_omit_another_side:
                stack.append((node.right, depth+1))
        else:#当差大于0时则，该节点的右子树入栈，#如果k_nearest未装满或k_nearest中相距目标点最远的点与目标点的距离大于axis_diff的绝对值时，则左子树也入栈
            pass #BLANK_4
            stack.append((node.right, depth+1))
            if not can_omit_another_side:
                stack.append((node.left, depth+1))
    # return [data for data, _ in k_nearest] #返回遍历完的kd树后的k_nearest
    # return [data for _, data in k_nearest] #返回遍历完的kd树后的k_nearest
    return [k_nearest_pq.get()[-1].data for i in range(k_nearest_pq.qsize())] #返回遍历完的kd树后的k_nearest

# %% ../../notebooks/coding_projects/P2_SVM/00svm.ipynb 159
# 由于我们需要记录axis_order， 所以要写成类
class FastKDTree:
    def __init__(self, X, split_value_strategy='median', axis_order_strategy='range') -> None:
        n, k = X.shape
        # 决定一下划分维度的顺序
        # 方差最大原则
        if axis_order_strategy == 'variance':
            self.axis_order_list = np.argsort(np.var(X, axis=0))
        # 极差最大原则
        elif axis_order_strategy == 'range':
            self.axis_order_list = np.argsort(np.max(X, axis=0) - np.min(X, axis=0))
        else:
            self.axis_order_list = np.arange(k)
        print(self.axis_order_list)
        self.root = fast_build_kd_tree(X, self.axis_order_list, split_value_strategy, 0)
    def search_kd_tree(self, point, k):
        return fast_search_kd_tree(self.root, point, self.axis_order_list, k)
    
    # 使用KNN算法分类
    def knn_classifier(self, X_train, y_train, X_test, k=3):
        y_pred = []
        for i, test_point in enumerate(X_test):
            k_nearest = self.search_kd_tree(test_point, k)
            # print(k_nearest)
            # print(k_nearest[0])
            # print(np.where((X_train == k_nearest[0]).all(axis=1)))
            labels = [y_train[np.where((X_train == point).all(axis=1))[0][0]] for point in k_nearest]
            counts = np.bincount(labels)#计算k_nearest中样本最多的标签，预测目标样本为该标签
            y_pred.append(np.argmax(counts))
        return y_pred
try:
    tree = FastKDTree(X_train, split_value_strategy='middle')
except Exception as e:
    print(e)
