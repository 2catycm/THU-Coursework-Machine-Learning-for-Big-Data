{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../../thu_sigs_logo.png\" alt=\"清华深研院-横\" style=\"zoom:50%;\" />"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: \"更高效的支持向量机算法实现及其在手写数字识别中的应用——00绪论\"\n",
    "subtitle: \"大数据机器学习课程第二次实验项目\"\n",
    "author: \"叶璨铭 (2024214500) \\n ycm24@mails.tsinghua.edu.cn\"\n",
    "date: \"2024-11-11\"\n",
    "toc: true\n",
    "number-sections: true\n",
    "highlight-style: pygments\n",
    "date-format: full\n",
    "lang: zh\n",
    "bibliography: [../../references.bib]\n",
    "format: \n",
    "  html:\n",
    "    code-fold: false\n",
    "  gfm: default\n",
    "  docx: default\n",
    "jupyter: python3\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T15:30:39.870282Z",
     "start_time": "2024-11-12T15:30:39.864209Z"
    }
   },
   "outputs": [],
   "source": [
    "#| default_exp svm.infra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T15:30:40.141007Z",
     "start_time": "2024-11-12T15:30:40.108437Z"
    }
   },
   "outputs": [],
   "source": [
    "#|hide\n",
    "# autoreload\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from thu_big_data_ml.help import plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 绪论\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 代码与文档格式说明\n",
    "\n",
    "本文档使用Jupyter Notebook编写，所以同时包括了实验文档和实验代码。\n",
    "\n",
    "本次实验项目采用了 Quarto + nbdev 的系统来发布Jupyter Notebook, 因而我们的实验文档导出为pdf和html格式可以进行阅读，而我们的代码也导出为python模块形式，可以作为代码库被其他项目使用。\n",
    "\n",
    "我们这样做的好处是，避免单独管理一堆 .py 文件，防止代码冗余和同步混乱，py文件和pdf文件都是从.ipynb文件导出的，可以保证实验文档和代码的一致性。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "::: {.callout-important}\n",
    "可以通过以下命令安装我们实验的代码：\n",
    "\n",
    "```shell\n",
    "pip install git+https://github.com/Open-Book-Studio/THU-Coursework-Machine-Learning-for-Big-Data.git\n",
    "```\n",
    "我们的代码导出为了python模块形式，通过以下命令导入：\n",
    "```python\n",
    "from thu_big_data_ml.svm import *\n",
    "```\n",
    ":::\n",
    "\n",
    "https://github.com/Open-Book-Studio/THU-Coursework-Machine-Learning-for-Big-Data.git 是我们本次大数据机器学习课程实验的代码仓库地址，\n",
    "\n",
    "而这次作业中，我开发的另一个用于图像分类科研的开源项目[有名的分类框架 (NamableClassify)](https://github.com/2catycm/NamableClassify.git)也相应地进行了代码更新，把我们这次作业实现的SVM算法加入到了其中。接下来我们也会用到这个项目中的一些代码（比如分类评测指标）来完成本次作业。我还构建了我们课题组的基础依赖库[ScholarlyInfrastructure](https://github.com/THU-CVML/ScholarlyInfrastructure)，对标`fastcore`库，对AI科研经常会用到的一些基础性地、和Python语言的表达力有关的代码进行了整理，比如PyTorch模型检查、清晰的日志、实验参数管理、异常处理、argmax自动函数优化等。这里我们用到了实验参数管理功能，把SVM的超参数表达为随机变量，随即使用元参数优化算法进行搜索。\n",
    "```shell\n",
    "pip install git+https://github.com/2catycm/NamableClassify.git\n",
    "pip install git+https://github.com/THU-CVML/ScholarlyInfrastructure.git\n",
    "```\n",
    "\n",
    "```python\n",
    "from namable_classify import *\n",
    "from scholarly_infrastructure import *\n",
    "```\n",
    "\n",
    "以上代码库开源在github，欢迎各位同学、老师们提出宝贵意见，或者加入我们的开发一起完善，构建更加优质的科研工具。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "::: {.callout-important}\n",
    "本文档具有一定的交互性，建议使用浏览器打开html文件，这样比pdf文件阅读体验更佳。\n",
    "\n",
    "由于这次项目作业内容太多，为了便于管理，我们将项目文档和代码分为了不同几个部分。\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实验目的与项目要求\n",
    "> 老师给我们的要求是\n",
    "> 1. ⼿动实现一个 SVM 分类器，熟悉 SVM 的原理与优化求解 SVM 分类器的算法的过程。\n",
    ">\n",
    "> 2. MNIST 数据集分类，报告你在测试集上的准确率，与已有SVM库进行对比。\n",
    ">\n",
    "> 3. 对 SVM 分类器训练的超参数(包括收敛终止条件，学习率等)进行调优\n",
    ">\n",
    "> 4. 构建使用 kernel 方法的 SVM 分类器\n",
    ">\n",
    "> 5. 对比不同 SVM 方法\n",
    "\n",
    "\n",
    "~~作为Top1大学的学生~~，我们不仅需要完成以上内容，还需要进行一些深入的思考和探索。\n",
    "1. 现有最流行的SVM分类的实现是sklearn以及其背后的libsvm，由于C++编写，确实在CPU上运行很快。但是，如果我们需要在GPU上运行，是否可以考虑用CUDA等方法来加速呢？我们决定尝试一下实现速度更快的SVM分类器。\n",
    "2. sklearn自带的调参GridSearchCV和RandomizedSearchCV都具有一定的局限性。参考谷歌调参手册，我们使用科学的实验设计来对SVM分类算法的元参数进行搜索，从而实现更高的分类精度，并且获得一些insight，便于我们后续科研中使用SVM。\n",
    "3. 实现自己的kernel方法。实现之后，不仅像2那样参考谷歌调参手册和假设检验来比较数值上的性能，还使用可视化工具来对不同核函数的效果进行比较。\n",
    "4. 除了李航《统计学习方法》的介绍逻辑，也补充阅读周志华的《机器学习》西瓜书，加深对SVM的理解。进一步阅读深度学习时代的SVM前沿论文，探索一些更加新的SVM策略，为后续科研提供参考。\n",
    "\n",
    "\n",
    "事不宜迟，我们开始动手吧！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实验数据\n",
    "> MNIST 数据库是由 Yann et. al. 提供的⼿写数字数据库⽂件, 官网地址为 http://yann.lecun.com/exdb/mnist/。\n",
    "> 主要包含了 60000 张的训练图像和 10000 张的测试图像\n",
    "\n",
    "类似于上一次Project（KD树实现KNN），我们使用 sklearn CI（持续集成）测试用例的load_digits数据集，而不只是使用原始的MNIST数据集，来加快实验的效率。并且在划分数据集时，train_test_split应当使用stratify参数，以确保每一类样本的比例相同。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# from namable_classify.data import ClassificationDataConfig, ClassificationDataModule, MNISTDataModule\n",
    "# # ClassificationDataConfig?\n",
    "# mnist_data = MNISTDataModule.from_config(ClassificationDataConfig(dataset_name='MNIST'))\n",
    "# mnist_data.prepare_data()\n",
    "# mnist_data.hparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext rich\n",
    "from scholarly_infrastructure.logging.nucleus import logger, print\n",
    "from sklearn.datasets import load_digits, fetch_openml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "\u001b[1m(\u001b[0m\n",
       "    \u001b[1;35mdict_keys\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[32m'data'\u001b[0m, \u001b[32m'target'\u001b[0m, \u001b[32m'frame'\u001b[0m, \u001b[32m'feature_names'\u001b[0m, \u001b[32m'target_names'\u001b[0m, \u001b[32m'images'\u001b[0m, \u001b[32m'DESCR'\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m,\n",
       "    \u001b[1;35mdict_keys\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[32m'data'\u001b[0m, \u001b[32m'target'\u001b[0m, \u001b[32m'frame'\u001b[0m, \u001b[32m'categories'\u001b[0m, \u001b[32m'feature_names'\u001b[0m, \u001b[32m'target_names'\u001b[0m, \u001b[32m'DESCR'\u001b[0m, \u001b[32m'details'\u001b[0m, \u001b[32m'url'\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[1m)\u001b[0m"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dataset_dict_uci_digits = load_digits(as_frame=True)\n",
    "dataset_dict_uci_digits = load_digits(as_frame=False)\n",
    "dataset_dict_full_mnist = fetch_openml(\"mnist_784\", as_frame=True)\n",
    "dataset_dict_uci_digits.keys(), dataset_dict_full_mnist.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1;35marray\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m0\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m2\u001b[0m, \u001b[1;36m3\u001b[0m, \u001b[1;36m4\u001b[0m, \u001b[1;36m5\u001b[0m, \u001b[1;36m6\u001b[0m, \u001b[1;36m7\u001b[0m, \u001b[1;36m8\u001b[0m, \u001b[1;36m9\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_dict_uci_digits.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "def sklearn_to_X_y_categories(dataset_dict):\n",
    "    X = dataset_dict['data']\n",
    "    y = dataset_dict['target']\n",
    "    if isinstance(X, pd.DataFrame):\n",
    "        X:np.array = X.values\n",
    "    if isinstance(y, pd.Series):\n",
    "        y:np.array = y.values\n",
    "    # if y.dtype.name == 'category':\n",
    "    #     categories = y.dtype.categories\n",
    "    # else:\n",
    "    X = X.astype(np.float32)\n",
    "    y = y.astype(np.int64)\n",
    "    categories = np.unique(y)\n",
    "    # print(str((X.shape, X.dtype, y.shape, y.dtype, categories)))\n",
    "    print(X.shape, X.dtype, y.shape, y.dtype, categories)\n",
    "    return X, y, categories\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">Sat 2024-11-16 22:11:02.268075</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36mSat 2024-11-16 22:11:02.268075\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> <span style=\"font-weight: bold\">((</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1797</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">64</span><span style=\"font-weight: bold\">)</span>, <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">dtype</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'float32'</span><span style=\"font-weight: bold\">)</span>, <span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1797</span>,<span style=\"font-weight: bold\">)</span>, <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">dtype</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'int64'</span><span style=\"font-weight: bold\">)</span>, <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">array</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span>,    <a href=\"file:///home/ye_canming/repos/novelties/cv/ScholarlyInfrastructure/scholarly_infrastructure/logging/nucleus.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">nucleus.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/ye_canming/repos/novelties/cv/ScholarlyInfrastructure/scholarly_infrastructure/logging/nucleus.py#55\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">55</span></a>\n",
       "         <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">9</span><span style=\"font-weight: bold\">]))</span>                                                                                         <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">             </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[34mINFO    \u001b[0m \u001b[1m(\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m1797\u001b[0m, \u001b[1;36m64\u001b[0m\u001b[1m)\u001b[0m, \u001b[1;35mdtype\u001b[0m\u001b[1m(\u001b[0m\u001b[32m'float32'\u001b[0m\u001b[1m)\u001b[0m, \u001b[1m(\u001b[0m\u001b[1;36m1797\u001b[0m,\u001b[1m)\u001b[0m, \u001b[1;35mdtype\u001b[0m\u001b[1m(\u001b[0m\u001b[32m'int64'\u001b[0m\u001b[1m)\u001b[0m, \u001b[1;35marray\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m0\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m2\u001b[0m, \u001b[1;36m3\u001b[0m, \u001b[1;36m4\u001b[0m, \u001b[1;36m5\u001b[0m, \u001b[1;36m6\u001b[0m, \u001b[1;36m7\u001b[0m, \u001b[1;36m8\u001b[0m,    \u001b]8;id=855392;file:///home/ye_canming/repos/novelties/cv/ScholarlyInfrastructure/scholarly_infrastructure/logging/nucleus.py\u001b\\\u001b[2mnucleus.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=96894;file:///home/ye_canming/repos/novelties/cv/ScholarlyInfrastructure/scholarly_infrastructure/logging/nucleus.py#55\u001b\\\u001b[2m55\u001b[0m\u001b]8;;\u001b\\\n",
       "         \u001b[1;36m9\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\u001b[1m)\u001b[0m                                                                                         \u001b[2m             \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">Sat 2024-11-16 22:11:02.537849</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36mSat 2024-11-16 22:11:02.537849\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> <span style=\"font-weight: bold\">((</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">70000</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">784</span><span style=\"font-weight: bold\">)</span>, <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">dtype</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'float32'</span><span style=\"font-weight: bold\">)</span>, <span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">70000</span>,<span style=\"font-weight: bold\">)</span>, <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">dtype</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'int64'</span><span style=\"font-weight: bold\">)</span>, <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">array</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span>, <a href=\"file:///home/ye_canming/repos/novelties/cv/ScholarlyInfrastructure/scholarly_infrastructure/logging/nucleus.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">nucleus.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/ye_canming/repos/novelties/cv/ScholarlyInfrastructure/scholarly_infrastructure/logging/nucleus.py#55\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">55</span></a>\n",
       "         <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">9</span><span style=\"font-weight: bold\">]))</span>                                                                                         <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">             </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[34mINFO    \u001b[0m \u001b[1m(\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m70000\u001b[0m, \u001b[1;36m784\u001b[0m\u001b[1m)\u001b[0m, \u001b[1;35mdtype\u001b[0m\u001b[1m(\u001b[0m\u001b[32m'float32'\u001b[0m\u001b[1m)\u001b[0m, \u001b[1m(\u001b[0m\u001b[1;36m70000\u001b[0m,\u001b[1m)\u001b[0m, \u001b[1;35mdtype\u001b[0m\u001b[1m(\u001b[0m\u001b[32m'int64'\u001b[0m\u001b[1m)\u001b[0m, \u001b[1;35marray\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m0\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m2\u001b[0m, \u001b[1;36m3\u001b[0m, \u001b[1;36m4\u001b[0m, \u001b[1;36m5\u001b[0m, \u001b[1;36m6\u001b[0m, \u001b[1;36m7\u001b[0m, \u001b[1;36m8\u001b[0m, \u001b]8;id=682506;file:///home/ye_canming/repos/novelties/cv/ScholarlyInfrastructure/scholarly_infrastructure/logging/nucleus.py\u001b\\\u001b[2mnucleus.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=20885;file:///home/ye_canming/repos/novelties/cv/ScholarlyInfrastructure/scholarly_infrastructure/logging/nucleus.py#55\u001b\\\u001b[2m55\u001b[0m\u001b]8;;\u001b\\\n",
       "         \u001b[1;36m9\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\u001b[1m)\u001b[0m                                                                                         \u001b[2m             \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X, y, categories = sklearn_to_X_y_categories(dataset_dict_uci_digits)\n",
    "X_full, y_full, categories_full = sklearn_to_X_y_categories(dataset_dict_full_mnist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "划分数据集为训练集和测试集。\n",
    "注意这里与官方的mnist划分有所不同，但是是合理而且科学的，因为正确使用了`stratify`参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "def make_train_val_test(X, y, val_size=0.1, test_size=0.2, random_state=42, normalize=True):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, \n",
    "                                                        stratify=y)\n",
    "    # print(len(X_train), len(X_test))\n",
    "    if normalize:\n",
    "        scaler = StandardScaler()\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "    # 进一步划分出验证集，用于调参、early stopping等。\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=42, \n",
    "                                                    stratify=y_train)\n",
    "    print(len(X_train), len(X_val), len(X_test))\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">Sat 2024-11-16 22:11:02.748631</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36mSat 2024-11-16 22:11:02.748631\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> <span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1293</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">144</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">360</span><span style=\"font-weight: bold\">)</span>                                                                             <a href=\"file:///home/ye_canming/repos/novelties/cv/ScholarlyInfrastructure/scholarly_infrastructure/logging/nucleus.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">nucleus.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/ye_canming/repos/novelties/cv/ScholarlyInfrastructure/scholarly_infrastructure/logging/nucleus.py#55\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">55</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[34mINFO    \u001b[0m \u001b[1m(\u001b[0m\u001b[1;36m1293\u001b[0m, \u001b[1;36m144\u001b[0m, \u001b[1;36m360\u001b[0m\u001b[1m)\u001b[0m                                                                             \u001b]8;id=204713;file:///home/ye_canming/repos/novelties/cv/ScholarlyInfrastructure/scholarly_infrastructure/logging/nucleus.py\u001b\\\u001b[2mnucleus.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=81860;file:///home/ye_canming/repos/novelties/cv/ScholarlyInfrastructure/scholarly_infrastructure/logging/nucleus.py#55\u001b\\\u001b[2m55\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">Sat 2024-11-16 22:11:07.325810</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36mSat 2024-11-16 22:11:07.325810\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> <span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">50400</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5600</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">14000</span><span style=\"font-weight: bold\">)</span>                                                                         <a href=\"file:///home/ye_canming/repos/novelties/cv/ScholarlyInfrastructure/scholarly_infrastructure/logging/nucleus.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">nucleus.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/ye_canming/repos/novelties/cv/ScholarlyInfrastructure/scholarly_infrastructure/logging/nucleus.py#55\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">55</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[34mINFO    \u001b[0m \u001b[1m(\u001b[0m\u001b[1;36m50400\u001b[0m, \u001b[1;36m5600\u001b[0m, \u001b[1;36m14000\u001b[0m\u001b[1m)\u001b[0m                                                                         \u001b]8;id=196832;file:///home/ye_canming/repos/novelties/cv/ScholarlyInfrastructure/scholarly_infrastructure/logging/nucleus.py\u001b\\\u001b[2mnucleus.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=371490;file:///home/ye_canming/repos/novelties/cv/ScholarlyInfrastructure/scholarly_infrastructure/logging/nucleus.py#55\u001b\\\u001b[2m55\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_train, X_val, X_test, y_train, y_val, y_test = make_train_val_test(X, y)\n",
    "X_train_full, X_val_full, X_test_full, y_train_full, y_val_full, y_test_full = make_train_val_test(X_full, y_full)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "获得 PyTorch 格式 的Dataset，\n",
    "进一步得到 PyTorch Lightning 的 DataModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "import torch\n",
    "import lightning as L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "def get_torch_dataset(X, y):\n",
    "    X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "    y_tensor = torch.tensor(y, dtype=torch.long)\n",
    "    dataset = torch.utils.data.TensorDataset(X_tensor, y_tensor)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = get_torch_dataset(X_train, y_train)\n",
    "val_set = get_torch_dataset(X_val, y_val)\n",
    "test_set = get_torch_dataset(X_test, y_test)\n",
    "train_set_full = get_torch_dataset(X_train_full, y_train_full)\n",
    "val_set_full = get_torch_dataset(X_val_full, y_val_full)\n",
    "test_set_full = get_torch_dataset(X_test_full, y_test_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightning as L\n",
    "data_module = L.LightningDataModule.from_datasets(\n",
    "    train_dataset=train_set, \n",
    "    val_dataset=val_set, \n",
    "    test_dataset=test_set, \n",
    "    predict_dataset=test_set, \n",
    "    batch_size=128,  \n",
    "    num_workers=4\n",
    ")\n",
    "data_module_full = L.LightningDataModule.from_datasets(\n",
    "    train_dataset=train_set_full, \n",
    "    val_dataset=val_set_full, \n",
    "    test_dataset=test_set_full, \n",
    "    predict_dataset=test_set_full, \n",
    "    batch_size=128,  \n",
    "    num_workers=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "from typing import Literal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "ReturnType = Literal['numpy', 'torch', 'lightning', 'pandas', 'all']\n",
    "\n",
    "def process_sklearn_dataset_dict(dataset_dict:dict, return_type:ReturnType):\n",
    "    X, y, categories = sklearn_to_X_y_categories(dataset_dict)\n",
    "    X_train, X_val, X_test, y_train, y_val, y_test = make_train_val_test(X, y)\n",
    "    train_set = get_torch_dataset(X_train, y_train)\n",
    "    val_set = get_torch_dataset(X_val, y_val)\n",
    "    test_set = get_torch_dataset(X_test, y_test)\n",
    "    data_module = L.LightningDataModule.from_datasets(\n",
    "        train_dataset=train_set, \n",
    "            val_dataset=val_set, \n",
    "            test_dataset=test_set, \n",
    "            predict_dataset=test_set, \n",
    "            batch_size=128,  \n",
    "            num_workers=4\n",
    "        )\n",
    "    if return_type == 'numpy':\n",
    "        return X_train, X_val, X_test, y_train, y_val, y_test\n",
    "    elif return_type == 'torch':\n",
    "        return train_set, val_set, test_set\n",
    "    elif return_type == 'lightning':\n",
    "        return data_module  \n",
    "    elif return_type == 'pandas':\n",
    "        raise NotImplementedError(\"Pandas not implemented yet\") # 这里可以用 dataset_dict 的 frame, 但是 train test split 还有预处理。\n",
    "    elif return_type == 'all':\n",
    "        return X_train, X_val, X_test, y_train, y_val, y_test, train_set, val_set, test_set, data_module, categories\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid return_type: {return_type}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 理论回顾\n",
    "### SVM有哪些优化形式？我们选择哪种来实现代码？\n",
    "\n",
    "\n",
    "参考[论文](https://www.semanticscholar.org/paper/Deep-Learning-using-Support-Vector-Machines-Tang/96c9f11fd9901f2edeaab8cf6bbff2590cea93c4)和[论文](https://www.researchsquare.com/article/rs-1200362/v2)，以及sklearn代码库中的分法，支持向量机（SVM）的优化方法最主要地分为两大流派：\n",
    "1. 使用hinge loss（或者周志华西瓜书SVM章节提到的其他surrogate loss）对线性支持向量机软间隔原始问题进行随机梯度下降SGD（或者西瓜书提到坐标下降也可以）。\n",
    "2. 使用SMO算法（或者通用的凸二次规划算法）来求解Kernel SVM的对偶问题（也可以求解Linear Kernel）。\n",
    "\n",
    "这是两个不同形式的问题。\n",
    "1. 参数保存上，（我个人认为）后者是非参数化方法，需要保存训练集中的支持向量，可能会有很多个，而前者训练成功后就是w和b，可以认为是参数化方法。\n",
    "2. 前者与现代的深度学习方法非常兼容，可以端到端地优化，作为深度学习方法的一个特殊loss。而后者虽然也有很多文章将CNN预训练的feature加上SVC去做分类器，但是本身的优化问题是不一样的优化问题。前者的优化难度和神经网络类似（不是简单，是很难），后者却对数据量非常敏感，数据量大的时候会很慢（不少论文提到对大数据集使用SVM仍然是研究热点，西瓜书指出其时间复杂度理论上不可能低于O(m^2)）。\n",
    "3. 后者支持核方法，而前者有论文和博客指出，SGD不可能优化Kernel Method的SVM。\n",
    "4. 对于前者，[论文](https://openreview.net/forum?id=gLwzzmh79K)说使用GD来优化的情况下，对线性可分数据集而言，hard margin SVM和Logistic Regression是等价的\n",
    "\n",
    "\n",
    "\n",
    "### SVM如何实现多分类？\n",
    "\n",
    "李航书上介绍的SVM是用于二分类问题的，然而本次项目我们需要做MNIST手写数字分类，这需要多分类，因而我们必须了解SVM如何实现多分类。\n",
    "\n",
    "周志华西瓜书告诉我们认为普通的二分类分类器，可以通过ovr或者ovo策略来实现多分类。那么SVM本身有没有更加独特的技巧去实现多分类呢？\n",
    "\n",
    "我们参考这个课件 https://www.mit.edu/~rakhlin/6.883/lectures/lecture05.pdf \n",
    "\n",
    "目前，构造SVM多类分类器的方法主要有两类：\n",
    "\n",
    "1. 直接法（也就是我说的独特的方法）：直接在目标函数上进行修改，将多个分类面的参数求解合并到一个最优化问题中，通过求解该最优化问题“一次性”实现多类分类。\n",
    "2. 间接法（所有二分类器都可以这么操作得到多分类器）：主要是通过组合多个二分类器来实现多分类器的构造，常见的方法有one-against-one（OvO）和one-against-all（OvR）两种。具体来说，\n",
    "    - 一对多法（One-Versus-Rest, OvR）：训练时依次把某个类别的样本归为一类，其他剩余的样本归为另一类，这样k个类别的样本就构造出了k个SVM。分类时将未知样本分类为具有最大分类函数值的那类。\n",
    "    - 一对一法（One-Versus-One, OvO）：将n个类别两两配对，产生n(n-1)/2个二分类任务，获得n(n-1)/2个分类器，新样本交给这些分类器，得到n(n-1)/2个结果，最终结果投票产生。\n",
    "\n",
    "### SVM如何实现概率输出？\n",
    "\n",
    "尽管李航书第一章把SVM是归类为非概率模型，实际上SVM经过一定的操作，是可以得到概率输出的。\n",
    "经典机器学习sklearn库的学习器有一个重要的API，`predict_proba()`，而sklearn中的SVC同样支持该API的调用。\n",
    "\n",
    "那么SVM概率输出有哪些方法呢？是否要用到点和分离超平面的几何间隔呢？经过资料查询，我发现SVM的概率输出几种方法：\n",
    "1. Sigmoid函数转换：可以将实轴上的数值投射到[0,1]上，即将一个输出实值抓化为一个概率值。比如一个分类器的分界线为0，大于0标为+1，小于0标为-1；如果使用sigmoid函数套一下输出值，我们就可以说，输出为0时标为+1的概率为0.5；输出为2时标为+1的概率为0.8等。\n",
    "2. Platt Scaling：这是libsvm中使用的一种方法，核心思想是把分类的结果作为新的训练集，用logistics回归再训练一个关系，得到具体的概率值。\n",
    "3. CalibratedClassifierCV：在scikit-learn中，可以使用专门的函数CalibratedClassifierCV对训练好的分类器模型进行校准，校准过程用到了cross-validation。\n",
    "在使用predict_proba()函数时，返回的是一个n行k列的数组，第i行第j列上的数值是模型预测第i个预测样本为某个标签的概率，并且每一行的概率和为1。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 相关工作\n",
    "\n",
    "类似于我们这次Project的，实现SVM的代码仓库有哪些？\n",
    "\n",
    "教育学习目的的库：\n",
    "- https://github.com/Kaslanarian/libsvm-sc-reading?tab=readme-ov-file\n",
    "    - 这篇文章写得非常用心，阅读libsvm源码解读地非常详细。\n",
    "- https://github.com/lzx1019056432/Be-Friendly-To-New-People/tree/master/SVM%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA\n",
    "    - 作者历时一个月，终于实现了SMO版本的SVM。但是我们做这个Project只有一周，我们要站在巨人的肩膀上实现地更好。\n",
    "- https://github.com/Learner0x5a/SVM-SMO\n",
    "    - 作者推导良久，终于搞懂了SVM以及SMO的公式，实现了符合sklearn接口的SVM。\n",
    "- https://github.com/shicaiwei123/svm-smo?tab=readme-ov-file\n",
    "    - 作者使用numpy实现了 SMO，并且使用ovo策略实现了多分类。\n",
    "- https://github.com/kazuto1011/svm-pytorch\n",
    "    - 作者使用早期版本的PyTorch实现了SVM。\n",
    "- https://bytepawn.com/svm-with-pytorch.html\n",
    "    - 这篇文章实现了二分类情况下，更早期PyTorch版本下实现的SVM。并且开源到github jupyter notebook。\n",
    "\n",
    "\n",
    "实际部署在工业界的Python库：\n",
    "- sklearn 本质上调用了 libsvm\n",
    "    - 注意sklearn很长一段时间不会利用GPU优化SVM！ https://stackoverflow.com/questions/35292741/what-svm-python-modules-use-gpu\n",
    "- https://pypi.org/project/svmlight/\n",
    "\n",
    "非Python语言的库\n",
    "- C++ libsvm https://github.com/cjlin1/libsvm\n",
    "    - GPU升级版，基于CUDA/C++实现 https://mklab.iti.gr/results/gpu-accelerated-libsvm/\n",
    "- Rust https://athemathmo.github.io/rusty-machine/doc/rusty_machine/learning/svm/index.html\n",
    "- Rust https://docs.rs/linfa-svm/latest/linfa_svm/\n",
    "\n",
    "\n",
    "可以看出这次Project的难度不小，我们需要对SVM的原理有深刻的理解，而且需要有较强的工程能力，才能手动实验一个SVM。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实验内容"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 分类的评价指标\n",
    "\n",
    "开尔文爵士曾说，如果你连measure都做不到，谈不上improve。所以我们先measure。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "from namable_classify.metrics import compute_classification_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m\n",
      "\u001b[0mcompute_classification_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0my_true\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0my_pred_logits\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mlogits_to_prob\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0my_pred\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mlabels\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0msupress_warnings\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0my_pred_metrics_only\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m <no docstring>\n",
      "\u001b[0;31mFile:\u001b[0m      ~/repos/novelties/cv/cls/NamableClassify/namable_classify/metrics.py\n",
      "\u001b[0;31mType:\u001b[0m      function"
     ]
    }
   ],
   "source": [
    "compute_classification_metrics?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里涵盖了 `from sklearn.metrics import roc_auc_score, top_k_accuracy_score, matthews_corrcoef, f1_score, precision_score, recall_score, log_loss, balanced_accuracy_score, cohen_kappa_score, hinge_loss, accuracy_score`\n",
    "当结果有意义的时候，这些指标都会被计算，用于评价模型的精度。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 调库实现SVM\n",
    "\n",
    "为了给我们后面的实验一个参照，我们调用现有代码库的SVM，关注其精度与速度的情况。\n",
    "当然如果我们Project在此收尾，只能酌情被扣除分数。\n",
    "在本节之后，我们将使用 PyTorch 和 numpy 这样的基础科学计算库，来在GPU和CPU上实现SVM及其优化。\n",
    "\n",
    "::: {.callout-important}\n",
    "本次Project首先展示了几个常用的SVM库的精度与速度，并且对其进行调参；随后本次Project基于基础科学计算库手写实现了SVM及其优化，和前面的库的精度与速度进行了对比。\n",
    ":::\n",
    "\n",
    "接下来的内容请见文件 [01sv_use_lib.ipynb](./01sv_use_lib.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 实现 Hinge Loss+SGD 版本的 Soft Margin Linear SVM\n",
    "\n",
    "我们现在来实现与`from sklearn.linear_model import SGDClassifier`等价的 SVM，但是我们基于PyTorch实现，在GPU上面运行，期望能在大型数据集上比sklearn的实现快。\n",
    "\n",
    "接下来的内容请见文件 [02svm_handy_crafted_linear.ipynb](./02svm_handy_crafted_linear.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 对手动实现的SVM进行调参\n",
    "\n",
    "这部分内容请见文件 [02svm_handy_crafted_linear.ipynb](./02svm_handy_crafted_linear.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 附加题: 对比不同 kernel 方法下的 SVM 分类器 （对完整SVM进行调参）\n",
    "\n",
    "这一题本质上是让我们以 kernel 的选择（也包括选择线性Kernel）作为目标元参数，其他参数作为冗余或固定元参数，进行调参实验，发现不同 kernel 方法下的 SVM 分类器的分类效果数值上的区别及其显著性，并且从可视化分析上也作出进一步解释。\n",
    "\n",
    "这部分内容请见 [03svm_kernel_hpo.ipynb](./03svm_kernel_hpo.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 附加题: 构建使用 kernel 方法的 SVM 分类器 （手动实现SMO）\n",
    "\n",
    "这也就是让我们手动实现 SMO 优化算法以及Kernel Method。\n",
    "\n",
    "这部分内容请见 [04svm_handy_crafted_kernel.ipynb](./04svm_handy_crafted_kernel.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yuequ",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
