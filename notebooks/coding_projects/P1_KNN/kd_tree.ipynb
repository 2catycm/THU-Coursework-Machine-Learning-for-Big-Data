{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../../thu_sigs_logo.png\" alt=\"清华深研院-横\" style=\"zoom:50%;\" />"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: \"深入探索KD树数据结构实现的KNN算法及其在手写数字识别中的应用\"\n",
    "subtitle: \"大数据机器学习课程第一次实验项目\"\n",
    "author: \"叶璨铭 (2024214500) \\n ycm24@mails.tsinghua.edu.cn\"\n",
    "date: \"2024-10-21\"\n",
    "toc: true\n",
    "number-sections: true\n",
    "highlight-style: pygments\n",
    "date-format: full\n",
    "lang: zh\n",
    "bibliography: [../../references.bib]\n",
    "format: \n",
    "  html:\n",
    "    code-fold: true\n",
    "  gfm: default\n",
    "jupyter: python3\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实验目的\n",
    "> 老师给我们的要求是\n",
    "> 1. 完成 KD 树算法，并利⽤实现的算法完成数字识别任务\n",
    "> 2. 对所建模型进行分析评判。\n",
    "\n",
    "我们"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实验数据\n",
    "> MNIST 数据库是由 Yann et. al. 提供的⼿写数字数据库⽂件, 官网地址为。\n",
    "> 主要包含了 60000 张的训练图像和 10000 张的测试图像\n",
    "> ```python\n",
    "> from sklearn.datasets import fetch_openml\n",
    "> from sklearn.model_selection import train_test_split\n",
    "> from sklearn.neighbors import KNeighborsClassifier\n",
    "> from sklearn.metrics import accuracy_score\n",
    "> import numpy as np\n",
    "> # 获取MNIST数据集,并抽样一部分数据以便后续的计算\n",
    "> idx = np.random.choice(70000,5000,replace=False)\n",
    "> mnist = fetch_openml(\"mnist_784\")\n",
    "> X, y = mnist.data.to_numpy(), mnist.target.to_numpy().astype('int')\n",
    "> X = X[idx]\n",
    "> y = y[idx]\n",
    "> # 划分数据集为训练集和测试集\n",
    "> X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "> \n",
    "> ```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "以上代码有几个小问题，我们需要改进一下\n",
    "1. 由于网络环境问题，fetch_openml(\"mnist_784\") 是无法跑通的，会卡死。\n",
    "事实上，给sklearn贡献过代码的同学可能知道，sklearn还有一个load_digits数据集，这个数据集是sklearn CI（持续集成）测试用例的一部分。这个回归测试通过测试贡献者的新做的改进是否导致性能不如以前的版本，来决定是否接受更改。\n",
    "因此，我们使用load_digits数据集代替mnist_784数据集来完成这个项目。\n",
    "\n",
    "https://scikit-learn.org/1.5/modules/generated/sklearn.datasets.load_digits.html\n",
    "\n",
    "2. 划分数据集时，train_test_split应当使用stratify参数，以确保每一类样本的比例相同。\n",
    "3. import过多，应该只导入需要的模块。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'target', 'frame', 'feature_names', 'target_names', 'images', 'DESCR'])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| export\n",
    "from sklearn.datasets import load_digits\n",
    "dataset_dict = load_digits()\n",
    "dataset_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1797, 64), dtype('float64'), (1797,), dtype('int32'))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| export\n",
    "import numpy as np\n",
    "X:np.array = dataset_dict['data']\n",
    "y:np.array = dataset_dict['target']\n",
    "X.shape, X.dtype, y.shape, y.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "划分数据集为训练集和测试集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1437, 360)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| export\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, \n",
    "                                                    stratify=y)\n",
    "len(X_train), len(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实验内容\n",
    "### KNN和KD树的关系是什么？什么叫基于KD树的KNN算法？\n",
    "\n",
    "在我们开始实验内容之前，有必要澄清这一理论上的概念。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 基于sklearn的KNN算法实现手写数字识别"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN Accuracy: 98.61%\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "# 创建KNeighborsClassifier模型，使用kd树作为搜索算法\n",
    "knn = KNeighborsClassifier(n_neighbors=3, algorithm='kd_tree')\n",
    "\n",
    "# 在训练集上训练模型\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# 在测试集上进行预测\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "# 评估模型性能\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"KNN Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 自己实现KD树"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义KD树节点类\n",
    "class Node:\n",
    "    def __init__(self, data, left=None, right=None):\n",
    "        self.data = data #节点本身的数据\n",
    "        self.left = left #节点的左子树\n",
    "        self.right = right #节点右子树\n",
    "\n",
    "# 递归方法构建KD树\n",
    "\n",
    "def build_kd_tree(X, depth=0):\n",
    "    if len(X) == 0:\n",
    "        return None\n",
    "    k = X.shape[1]\n",
    "    axis = depth % k #根据当前深度，选择划分的维度\n",
    "    X = X[X[:, axis].argsort()]\n",
    "    median = X.shape[0] // 2 #将当前结点数据一分为二\n",
    "    return Node(data=X[median], left=build_kd_tree(X[:median], depth + 1), right=build_kd_tree(X[median + 1:], depth + 1))\n",
    "\n",
    "# 计算点之间的距离，这里使用欧几里得距离\n",
    "def euclidean_distance(x1, x2):\n",
    "    return np.sqrt(np.sum((x1 - x2) ** 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from queue import PriorityQueue\n",
    "# from fastcore.utils import patch\n",
    "# def __len__(self:PriorityQueue):\n",
    "#     return self.qsize()\n",
    "# q = PriorityQueue()\n",
    "# q.put((3, 'c'))\n",
    "# q.put((1, 'd'))\n",
    "# q.put((2, 'b'))\n",
    "# print(q.get())\n",
    "\n",
    "# 注意一个坑点，same priority 下 会崩溃； PriorityQueue文档没写，heapq写了\n",
    "# https://docs.python.org/3/library/heapq.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(-13.30413469565007, 1.2)\n",
      "(0, <__main__.TestNode object at 0x000001B6C730DC90>)\n"
     ]
    }
   ],
   "source": [
    "from queue import PriorityQueue\n",
    "\n",
    "\n",
    "class TestNode:\n",
    "    def __init__(self, point):\n",
    "        self.point = point\n",
    "\n",
    "\n",
    "test1 = [\n",
    "    (-13.30413469565007, 1.2),\n",
    "    (-9.327379053088816, 0.0),\n",
    "    (-13.30413469565007, 1.4),\n",
    "]\n",
    "test2 = [\n",
    "    (-13.30413469565007, TestNode(1.2)),\n",
    "    (-9.327379053088816, TestNode(0.0)),\n",
    "    (-13.30413469565007, TestNode(1.4)),\n",
    "]\n",
    "\n",
    "test3 = [\n",
    "    (0, TestNode(1.2)),\n",
    "    (1, TestNode(0.0)),\n",
    "    (2, TestNode(1.4)),\n",
    "]\n",
    "\n",
    "test_pq = PriorityQueue()\n",
    "for t in test1:\n",
    "    test_pq.put(t)\n",
    "print(test_pq.get())\n",
    "test_pq = PriorityQueue()\n",
    "for t in test3:\n",
    "    test_pq.put(t)\n",
    "print(test_pq.get())\n",
    "# 注意这种情况下报错\n",
    "# for t in test2:\n",
    "#     test_pq.put(t)\n",
    "# print(test_pq.get())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 搜索KD树\n",
    "def search_kd_tree(tree, target, k=3):\n",
    "    if tree is None:\n",
    "        return []\n",
    "    # k_nearest = [] #list用于储存target当前遍历到的k个k近邻\n",
    "    # 我们使用优先队列来储存k_nearest，从而提高效率，优先队列中的元素为(-距离, 节点)的元组，距离远的先取出来\n",
    "    # k_nearest_pq = PriorityQueue(maxsize=k)\n",
    "    k_nearest_pq = PriorityQueue()\n",
    "    entry_count = 0\n",
    "    stack = [(tree, 0)] #用于储存待遍历节点的stack\n",
    "    while stack:\n",
    "        node, depth = stack.pop() # 节点出栈\n",
    "        if node is None:\n",
    "            continue\n",
    "        # print(\" \"*4*depth + f\"node: {node.data}, depth: {depth}\")\n",
    "        distance = euclidean_distance(target, node.data) #计算需要分类的目标点与节点的距离\n",
    "        \n",
    "        # 调换到前面\n",
    "        axis = depth % target.shape[0] #计算当前深度对应的划分维度\n",
    "        axis_diff = target[axis] - node.data[axis] #计算该维度下目标点与当前节点的差\n",
    "        \n",
    "        #如果k_nearest未装满或k_nearest中相距目标点最远的点与目标点的距离大于axis_diff的绝对值时，则另一边的子树也入栈\n",
    "        can_omit_another_side = True\n",
    "        # if len(k_nearest) < k: # 当k_nearest未装满时，直接将节点放入\n",
    "        # if k_nearest.qsize() < k: # 当k_nearest未装满时，直接将节点放入\n",
    "        if k_nearest_pq.qsize() < k: # 当k_nearest未装满时，直接将节点放入\n",
    "        # if not k_nearest_pq.full(): # 当k_nearest未装满时，直接将节点放入\n",
    "            # print(f\"not full, put {(-distance, node)}\")\n",
    "            pass #BLANK_1\n",
    "            # k_nearest.append((node, distance))\n",
    "            k_nearest_pq.put((-distance, entry_count, node))\n",
    "            entry_count+=1\n",
    "            can_omit_another_side = False\n",
    "        else: #当k_nearest装满时，对比该节点与k_nearest中与目标点距离最远的节点的距离，如果小于则替换，如果大于则不替换\n",
    "            pass #BLANK_2\n",
    "            farthest = k_nearest_pq.get()\n",
    "            farthest_distance = -farthest[0]\n",
    "            # print(f\"full, farthest: {farthest}\")\n",
    "            if distance < farthest_distance:\n",
    "                # print(f\"closer, put {(-distance, node)}\")\n",
    "                # assert isinstance(distance, float)\n",
    "                # print(k_nearest_pq.queue)\n",
    "                k_nearest_pq.put((-distance, entry_count, node))\n",
    "                entry_count+=1\n",
    "            else:\n",
    "                k_nearest_pq.put(farthest)\n",
    "            \n",
    "\n",
    "            if farthest_distance > abs(axis_diff): \n",
    "                can_omit_another_side = False\n",
    "        \n",
    "        if axis_diff <= 0: #当差小于0时则，该节点的左子树入栈 #如果k_nearest未装满或k_nearest中相距目标点最远的点与目标点的距离大于axis_diff的绝对值时，则右子树也入栈\n",
    "            pass #BLANK_3\n",
    "            stack.append((node.left, depth+1))\n",
    "            if not can_omit_another_side:\n",
    "                stack.append((node.right, depth+1))\n",
    "        else:#当差大于0时则，该节点的右子树入栈，#如果k_nearest未装满或k_nearest中相距目标点最远的点与目标点的距离大于axis_diff的绝对值时，则左子树也入栈\n",
    "            pass #BLANK_4\n",
    "            stack.append((node.right, depth+1))\n",
    "            if not can_omit_another_side:\n",
    "                stack.append((node.left, depth+1))\n",
    "    # return [data for data, _ in k_nearest] #返回遍历完的kd树后的k_nearest\n",
    "    # return [data for _, data in k_nearest] #返回遍历完的kd树后的k_nearest\n",
    "    return [k_nearest_pq.get()[-1].data for i in range(k_nearest_pq.qsize())] #返回遍历完的kd树后的k_nearest\n",
    "\n",
    "# 使用KNN算法分类\n",
    "def knn_classifier(X_train, y_train, X_test, k=3):\n",
    "    y_pred = []\n",
    "    for i, test_point in enumerate(X_test):\n",
    "        k_nearest = search_kd_tree(kd_tree, test_point, k)\n",
    "        # print(k_nearest)\n",
    "        # print(i)\n",
    "        labels = [y_train[np.where((X_train == point).all(axis=1))[0][0]] for point in k_nearest]\n",
    "        counts = np.bincount(labels)#计算k_nearest中样本最多的标签，预测目标样本为该标签\n",
    "        y_pred.append(np.argmax(counts))\n",
    "    return y_pred\n",
    "\n",
    "# 构建KD树\n",
    "kd_tree = build_kd_tree(X_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k_nearest = search_kd_tree(kd_tree, X_test[7], 3)\n",
    "# k_nearest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN Accuracy: 98.61%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 使用KNN算法进行分类\n",
    "k_neighbors = 3\n",
    "y_pred = knn_classifier(X_train, y_train, X_test, k_neighbors)\n",
    "\n",
    "# 评估分类性能\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"KNN Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 超参数调优"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "实验题目要求我们对 knn 进⾏超参数的搜索。那么什么是超参数搜索呢？为此我们需要理解两个概念——超参数和搜索。\n",
    "\n",
    "#### 什么是参数？什么是超参数？什么是元参数？\n",
    "\n",
    "在大数据分析中，我们往往不知道数据的总体，只能获得数据的一个采样。然而我们对数据的总体是什么分布非常感兴趣，这些未知的分布，我们假设可能是由一些参数来决定的，我们需要根据采样出来的数据对总体的参数进行参数估计（Parameter Estimation）。比如说总体是高斯分布，那么高斯分布的均值和方差就是参数。\n",
    "\n",
    "刚才我们说了参数是什么，那么什么是超参数呢？参数估计我们通常会用极大似然估计方法，但是相比于贝叶斯参数估计来说有一定的局限性。在贝叶斯机器学习中，我们认为参数本身也是有一个概率分布的，而不是确定的值，而描述参数分布的参数，我们称之为超参数。当然，我们可以认为超参数也有其分布，那么就有对应的超超参数，这种多层嵌套的结构称为贝叶斯网络。\n",
    "\n",
    "对应到机器学习和深度学习中，参数是指参数化模型的权重。但是我们没有在使用贝叶斯估计，并没有说这些模型权重具有一定的分布，那么超参数是怎么一回事呢？事实上，根据谷歌团队提出的《深度学习调优指南》，深度学习社区错误地把学习率、批处理大小、正则化系数等参数叫做超参数，这是错误的。他们确实决定了模型的假设空间的不同，决定了最终的性能，而且在模型训练过程中不发生变化，而是决定了训练过程，但是他们本身并不是先验分布的参数，严格来说不应该叫做超参数，应该叫做元参数。\n",
    "\n",
    "#### 什么是搜索？\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KNN和KD树有哪些元参数？\n",
    "上文我们辨析了KNN和KD树的关系，即是否选用KD树作为KNN的近邻搜索算法，本身是KNN的一个元参数，KNN也可以选择Ball Tree、Brute Force等其他近邻搜索算法。\n",
    "\n",
    "KD树本身也有一些元参数，比如分割方式、节点的选择方式等，这些元参数会影响KD树的构建和搜索的系统性能（时间复杂度、空间复杂度），但是不会影响到机器学习的性能（分类准确率、ROC-AUC等指标）。因为不影响机器学习的性能，在本节我们不讨论KD树的元参数如何调优。我们会在下一节，附加题中，讨论不同的KD树构建方式对搜索速度的影响。\n",
    "\n",
    "那么KNN作为一个机器学习算法，有哪些元参数需要调优呢？参考sklearn的KNeighborsClassifier类的参数说明，我们可以看到以下参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31mInit signature:\u001b[0m\n",
      "\u001b[0mKNeighborsClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mn_neighbors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[1;33m*\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mweights\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'uniform'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0malgorithm\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'auto'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mleaf_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mmetric\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'minkowski'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mmetric_params\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mDocstring:\u001b[0m     \n",
      "Classifier implementing the k-nearest neighbors vote.\n",
      "\n",
      "Read more in the :ref:`User Guide <classification>`.\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "n_neighbors : int, default=5\n",
      "    Number of neighbors to use by default for :meth:`kneighbors` queries.\n",
      "\n",
      "weights : {'uniform', 'distance'}, callable or None, default='uniform'\n",
      "    Weight function used in prediction.  Possible values:\n",
      "\n",
      "    - 'uniform' : uniform weights.  All points in each neighborhood\n",
      "      are weighted equally.\n",
      "    - 'distance' : weight points by the inverse of their distance.\n",
      "      in this case, closer neighbors of a query point will have a\n",
      "      greater influence than neighbors which are further away.\n",
      "    - [callable] : a user-defined function which accepts an\n",
      "      array of distances, and returns an array of the same shape\n",
      "      containing the weights.\n",
      "\n",
      "    Refer to the example entitled\n",
      "    :ref:`sphx_glr_auto_examples_neighbors_plot_classification.py`\n",
      "    showing the impact of the `weights` parameter on the decision\n",
      "    boundary.\n",
      "\n",
      "algorithm : {'auto', 'ball_tree', 'kd_tree', 'brute'}, default='auto'\n",
      "    Algorithm used to compute the nearest neighbors:\n",
      "\n",
      "    - 'ball_tree' will use :class:`BallTree`\n",
      "    - 'kd_tree' will use :class:`KDTree`\n",
      "    - 'brute' will use a brute-force search.\n",
      "    - 'auto' will attempt to decide the most appropriate algorithm\n",
      "      based on the values passed to :meth:`fit` method.\n",
      "\n",
      "    Note: fitting on sparse input will override the setting of\n",
      "    this parameter, using brute force.\n",
      "\n",
      "leaf_size : int, default=30\n",
      "    Leaf size passed to BallTree or KDTree.  This can affect the\n",
      "    speed of the construction and query, as well as the memory\n",
      "    required to store the tree.  The optimal value depends on the\n",
      "    nature of the problem.\n",
      "\n",
      "p : float, default=2\n",
      "    Power parameter for the Minkowski metric. When p = 1, this is\n",
      "    equivalent to using manhattan_distance (l1), and euclidean_distance\n",
      "    (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.\n",
      "\n",
      "metric : str or callable, default='minkowski'\n",
      "    Metric to use for distance computation. Default is \"minkowski\", which\n",
      "    results in the standard Euclidean distance when p = 2. See the\n",
      "    documentation of `scipy.spatial.distance\n",
      "    <https://docs.scipy.org/doc/scipy/reference/spatial.distance.html>`_ and\n",
      "    the metrics listed in\n",
      "    :class:`~sklearn.metrics.pairwise.distance_metrics` for valid metric\n",
      "    values.\n",
      "\n",
      "    If metric is \"precomputed\", X is assumed to be a distance matrix and\n",
      "    must be square during fit. X may be a :term:`sparse graph`, in which\n",
      "    case only \"nonzero\" elements may be considered neighbors.\n",
      "\n",
      "    If metric is a callable function, it takes two arrays representing 1D\n",
      "    vectors as inputs and must return one value indicating the distance\n",
      "    between those vectors. This works for Scipy's metrics, but is less\n",
      "    efficient than passing the metric name as a string.\n",
      "\n",
      "metric_params : dict, default=None\n",
      "    Additional keyword arguments for the metric function.\n",
      "\n",
      "n_jobs : int, default=None\n",
      "    The number of parallel jobs to run for neighbors search.\n",
      "    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
      "    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
      "    for more details.\n",
      "    Doesn't affect :meth:`fit` method.\n",
      "\n",
      "Attributes\n",
      "----------\n",
      "classes_ : array of shape (n_classes,)\n",
      "    Class labels known to the classifier\n",
      "\n",
      "effective_metric_ : str or callble\n",
      "    The distance metric used. It will be same as the `metric` parameter\n",
      "    or a synonym of it, e.g. 'euclidean' if the `metric` parameter set to\n",
      "    'minkowski' and `p` parameter set to 2.\n",
      "\n",
      "effective_metric_params_ : dict\n",
      "    Additional keyword arguments for the metric function. For most metrics\n",
      "    will be same with `metric_params` parameter, but may also contain the\n",
      "    `p` parameter value if the `effective_metric_` attribute is set to\n",
      "    'minkowski'.\n",
      "\n",
      "n_features_in_ : int\n",
      "    Number of features seen during :term:`fit`.\n",
      "\n",
      "    .. versionadded:: 0.24\n",
      "\n",
      "feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      "    Names of features seen during :term:`fit`. Defined only when `X`\n",
      "    has feature names that are all strings.\n",
      "\n",
      "    .. versionadded:: 1.0\n",
      "\n",
      "n_samples_fit_ : int\n",
      "    Number of samples in the fitted data.\n",
      "\n",
      "outputs_2d_ : bool\n",
      "    False when `y`'s shape is (n_samples, ) or (n_samples, 1) during fit\n",
      "    otherwise True.\n",
      "\n",
      "See Also\n",
      "--------\n",
      "RadiusNeighborsClassifier: Classifier based on neighbors within a fixed radius.\n",
      "KNeighborsRegressor: Regression based on k-nearest neighbors.\n",
      "RadiusNeighborsRegressor: Regression based on neighbors within a fixed radius.\n",
      "NearestNeighbors: Unsupervised learner for implementing neighbor searches.\n",
      "\n",
      "Notes\n",
      "-----\n",
      "See :ref:`Nearest Neighbors <neighbors>` in the online documentation\n",
      "for a discussion of the choice of ``algorithm`` and ``leaf_size``.\n",
      "\n",
      ".. warning::\n",
      "\n",
      "   Regarding the Nearest Neighbors algorithms, if it is found that two\n",
      "   neighbors, neighbor `k+1` and `k`, have identical distances\n",
      "   but different labels, the results will depend on the ordering of the\n",
      "   training data.\n",
      "\n",
      "https://en.wikipedia.org/wiki/K-nearest_neighbor_algorithm\n",
      "\n",
      "Examples\n",
      "--------\n",
      ">>> X = [[0], [1], [2], [3]]\n",
      ">>> y = [0, 0, 1, 1]\n",
      ">>> from sklearn.neighbors import KNeighborsClassifier\n",
      ">>> neigh = KNeighborsClassifier(n_neighbors=3)\n",
      ">>> neigh.fit(X, y)\n",
      "KNeighborsClassifier(...)\n",
      ">>> print(neigh.predict([[1.1]]))\n",
      "[0]\n",
      ">>> print(neigh.predict_proba([[0.9]]))\n",
      "[[0.666... 0.333...]]\n",
      "\u001b[1;31mFile:\u001b[0m           f:\\transfer\\anaconda3\\envs\\agn\\lib\\site-packages\\sklearn\\neighbors\\_classification.py\n",
      "\u001b[1;31mType:\u001b[0m           ABCMeta\n",
      "\u001b[1;31mSubclasses:\u001b[0m     "
     ]
    }
   ],
   "source": [
    "# help(KNeighborsClassifier)\n",
    "# 使用 ipython的 ? 可以更好地看到 函数和类的docstring信息。\n",
    "KNeighborsClassifier?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其中这几个参数是和分类准确率有关的\n",
    "- `n_neighbors`, 也就是k\n",
    "- `weights`，检索出来的k个点用来决策，这些点一样重要吗？\n",
    "  - 我们李航书学的基础版本是uniform，而distance方法不一样在于\n",
    "  - 每一个点的投票权是距离的-1次方。（哈哈为什么不是像万有引力那样是-2次方）\n",
    "- `p`和`metric`和`metric_params`, 要怎么计算距离？\n",
    "\n",
    "而 `algorithm` `leaf_size` `n_jobs` 三个参数暂时和我们无关。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['cityblock', 'cosine', 'euclidean', 'haversine', 'l2', 'l1', 'manhattan', 'precomputed', 'nan_euclidean'])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import distance_metrics\n",
    "distance_metrics().keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 具体要怎么调参呢？\n",
    "\n",
    "如果我们就把调参问题当做搜索问题，那么它就是一个无梯度黑盒最优化问题。对于这类问题，最平凡（trivial）的搜索方法是全盘遍历（grid search），然而当搜索空间太大的时候，这就不是很高效了。一些基础的改进是贪心算法和随机化搜索方法，比如爬山法、随机采样法、模拟退火法等。而要想得到最先进（SOTA）的性能，演化计算和贝叶斯优化是两个最好的方法，也是目前人工智能仍然活跃的科研方向。\n",
    "\n",
    "然而调参问题并不完全是搜索问题。Google的《深度学习调优指南》指出，调参是一个“探索与利用”（exploration and exploitation）的过程。我的理解是，在我们做深度学习研究的时候，我们其实更想知道，我们的方法对于那些超参数敏感，在其他方法也调到最优超参的情况下我的方法是否仍然显著优于其他方法，而不只是说我的方法在单单一个超参数上优于其他方法（选择“我的方法”还是“其他近期SOTA方法”就是一个离散型目标元参数）。因此，我们需要在调参的过程中理解不同的参数对于结果的影响。这其实也是作为科学家和研究者我们做科学实验的过程。调参的实质不是乱试，而是“控制变量”，参数就是自变量和无关变量，评价指标就是因变量。不过，与我们高中生物课学习的“控制变量法”稍有不同，无关变量不一定是控制相等，在计算资源充足时，无关变量应该控制到最优，所以这里有优化问题。\n",
    "\n",
    "对于具体的调参算法和代码而言，我们当然可以用sklearn默认提供的GridSearchCV、RandomizedSearchCV等方法，我猜做这个作业的大部分同学用的是这两个。但是刚才我们也说了，GridSearch代价太高，而RandomizedSearchCV以及贝叶斯优化、演化计算忙于“利用”，而没有进行单一变量原则，无法通过科学实验“探索”出我们想获得的insight。根据Google的建议，在探索阶段最适合的算法其实是准随机搜索算法（quasi random search）。\n",
    "\n",
    "因此，我们遵循google指南，\n",
    "\n",
    "\n",
    "此外，我还实现了一个“学生实验算法”，这个算法从优化上来说是一种交替优化（alternating optimization）或者叫做多阶段优化（multi-stage optimization）的方法，即先固定一个超参数，然后在这个超参数下进行优化，再固定另一个超参数，再进行优化，以此类推，直到所有超参数都优化完毕。这个算法的好处是遵循了单一变量原则和无关变量控制相等原则，可以探索出很多结论。\n",
    "\n",
    "\n",
    "在这里我们也做一个科学实验，实验假设是在其他参数最优时，使用\"distance\"的KNN比普通的\"uniform\"KNN的效果好。\n",
    "这样我们有一个研究的目标，相当于我们扮演那个提出\"distance\"方法的科学家，要和其他人的方法做比较才能发论文。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 代码实现调优\n",
    "\n",
    "首先我们需要定义KNN元参数的分布空间"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import distance_metrics\n",
    "from ray import train, tune\n",
    "# https://docs.ray.io/en/latest/tune/tutorials/tune-search-spaces.html\n",
    "search_space = dict(\n",
    "    random_state = tune.grid_search([0, 1, 2]) # 做三次实验，因为KNN还是有随机性的（主要是验证集的划分；距离一样的时候选了谁、投票平票的时候决定是哪个类别，这两个sklearn中是确定的）\n",
    "    ,weights = tune.grid_search([\"uniform\", \"weights\"]) # 目标元参数， 我们的零假设是这两个weights不优于uniform，备择假设是weights更好。\n",
    "    ,n_neighbors = tune.randint(1, 20)  # 随机取整数。 TODO 我们还可以用左偏正态分布来建模这个参数的先验分布。\n",
    "    # ray tune也能处理条件分布，但是太复杂了，我们避免`p`参数依赖于`metric`参数生效的问题，我们换成choice来处理。\n",
    "    ,distance_metric = tune.choice(list(distance_metrics().keys())) # grid_search 是要求必须遍历的，而choice是随机选择。\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后我们定义评价函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "    # np.random.seed(random_state)\n",
    "    # indices = np.arange(X_train.shape[0])\n",
    "    # np.random.shuffle(indices)\n",
    "    # scores = cross_val_score(knn, X_train, y_train, cv=5, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9826001742160277"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from sklearn.model_selection import cross_val_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "def evaluate_knn(random_state:int, weights:str, n_neighbors:int, distance_metric:str):\n",
    "    knn = KNeighborsClassifier(n_neighbors=n_neighbors, weights=weights, metric=distance_metric)\n",
    "\n",
    "    # 初始化KFold\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=random_state)\n",
    "    \n",
    "    # 初始化存储每次交叉验证的分数\n",
    "    scores = []\n",
    "    \n",
    "    # 进行5折交叉验证\n",
    "    for train_index, test_index in kf.split(X_train):\n",
    "        # 分割训练集和测试集\n",
    "        X_train_fold, X_test_fold = X_train[train_index], X_train[test_index]\n",
    "        y_train_fold, y_test_fold = y_train[train_index], y_train[test_index]\n",
    "        \n",
    "        # 创建KNN分类器实例\n",
    "        knn = KNeighborsClassifier(n_neighbors=n_neighbors, weights=weights, metric=distance_metric)\n",
    "        \n",
    "        # 训练模型\n",
    "        knn.fit(X_train_fold, y_train_fold)\n",
    "        \n",
    "        # 预测测试集\n",
    "        y_pred = knn.predict(X_test_fold)\n",
    "        \n",
    "        # 计算准确率\n",
    "        score = accuracy_score(y_test_fold, y_pred)\n",
    "        scores.append(score)\n",
    "    return sum(scores)/len(scores)\n",
    "# 测试下函数能不能跑\n",
    "evaluate_knn(random_state=43, weights='uniform', n_neighbors=5, distance_metric='euclidean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 符合optuna接口\n",
    "def objective(meta_parameters):\n",
    "    return dict(accuracy=evaluate_knn(**meta_parameters))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来我们要定义使用的搜索算法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'colorlog'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mray\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtune\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msearch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ConcurrencyLimiter\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mray\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtune\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msearch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptuna\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OptunaSearch\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01moptuna\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msamplers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m QMCSampler\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# quasi random search\u001b[39;00m\n\u001b[0;32m      5\u001b[0m sampler \u001b[38;5;241m=\u001b[39m QMCSampler()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\optuna\\__init__.py:4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01moptuna\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m exceptions\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01moptuna\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m integration\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01moptuna\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m logging\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01moptuna\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m multi_objective\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01moptuna\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pruners\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\optuna\\logging.py:14\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mthreading\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Optional\n\u001b[1;32m---> 14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcolorlog\u001b[39;00m\n\u001b[0;32m     17\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCRITICAL\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDEBUG\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWARNING\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     25\u001b[0m ]\n\u001b[0;32m     27\u001b[0m _lock: threading\u001b[38;5;241m.\u001b[39mLock \u001b[38;5;241m=\u001b[39m threading\u001b[38;5;241m.\u001b[39mLock()\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'colorlog'"
     ]
    }
   ],
   "source": [
    "from ray.tune.search import ConcurrencyLimiter\n",
    "from ray.tune.search.optuna import OptunaSearch\n",
    "from optuna.samplers import QMCSampler\n",
    "# quasi random search\n",
    "sampler = QMCSampler()\n",
    "algo = OptunaSearch(sampler=sampler)\n",
    "algo = ConcurrencyLimiter(algo, max_concurrent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
