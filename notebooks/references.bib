@article{KD-means, abstractNote={K-means clustering is a popular unsupervised classification algorithm employed in several domains, e.g., imaging, segmentation, or compression. Nevertheless, the number of clusters k, fixed apriori, affects mainly the clustering quality. Current State-of-the-art k-means implementations could automatically set of the number of clusters. However, they result in unreasonable processing time while classifying large volumes of data. In this paper, we propose a novel solution based on kd-tree to determine the number of cluster k in the context of massive data for preprocessing data science projects or in near-real-time applications. We demonstrate how our solution outperforms current solutions in terms of clustering quality, and processing time on massive data.}, language={en} }
@book{Russell_Norvig_2016, address={Boston Columbus Indianapolis New York San Francisco Upper Saddle River Amsterdam Cape Town Dubai London Madrid Milan Munich Paris Montreal Toronto Delhi Mexico City Sao Paulo Sydney Hong Kong Seoul Singapore Taipei Tokyo}, edition={Third edition, Global edition}, series={Prentice Hall series in artificial intelligence}, title={Artificial intelligence: a modern approach}, ISBN={978-0-13-604259-4}, publisher={Pearson}, author={Russell, Stuart J. and Norvig, Peter}, year={2016}, collection={Prentice Hall series in artificial intelligence}, language={en} }
@article{Demšar_2006, title={Statistical Comparisons of Classifiers over Multiple Data Sets}, volume={7}, ISSN={1532-4435}, abstractNote={While methods for comparing two learning algorithms on a single data set have been scrutinized for quite some time already, the issue of statistical tests for comparisons of more algorithms on multiple data sets, which is even more essential to typical machine learning studies, has been all but ignored. This article reviews the current practice and then theoretically and empirically examines several suitable tests. Based on that, we recommend a set of simple, yet safe and robust non-parametric tests for statistical comparisons of classifiers: the Wilcoxon signed ranks test for comparison of two classifiers and the Friedman test with the corresponding post-hoc tests for comparison of more classifiers over multiple data sets. Results of the latter can also be neatly presented with the newly introduced CD (critical difference) diagrams.}, journal={The Journal of Machine Learning Research}, author={Demšar, Janez}, year={2006}, month=dec, pages={1–30}, language={en-US} }
@misc{tuningplaybookgithub,
  author = {Varun Godbole and George E. Dahl and Justin Gilmer and Christopher J. Shallue and Zachary Nado},
  title = {Deep Learning Tuning Playbook},
  url = {http://github.com/google-research/tuning_playbook},
  year = {2023},
  note = {Version 1.0}
}
@article{Yao_Liu_1997, title={A new evolutionary system for evolving artificial neural networks}, volume={8}, ISSN={1045-9227, 1941-0093}, DOI={10.1109/72.572107}, abstractNote={This paper presents a new evolutionary system, i.e., EPNet, for evolving artiﬁcial neural networks (ANN’s). The evolutionary algorithm used in EPNet is based on Fogel’s evolutionary programming (EP). Unlike most previous studies on evolving ANN’s, this paper puts its emphasis on evolving ANN’s behaviors. This is one of the primary reasons why EP is adopted. Five mutation operators proposed in EPNet reﬂect such an emphasis on evolving behaviors. Close behavioral links between parents and their offspring are maintained by various mutations, such as partial training and node splitting. EPNet evolves ANN’s architectures and connection weights (including biases) simultaneously in order to reduce the noise in ﬁtness evaluation. The parsimony of evolved ANN’s is encouraged by preferring node/connection deletion to addition. EPNet has been tested on a number of benchmark problems in machine learning and ANN’s, such as the parity problem, the medical diagnosis problems (breast cancer, diabetes, heart disease, and thyroid), the Australian credit card assessment problem, and the Mackey–Glass time series prediction problem. The experimental results show that EPNet can produce very compact ANN’s with good generalization ability in comparison with other algorithms.}, number={3}, journal={IEEE Transactions on Neural Networks}, author={Yao, X. and Liu, Y.}, year={1997}, month=may, pages={694–713}, language={en} }

@misc{人民邮电出版社_2020, type={知乎专栏文章}, title={神经网络简史（上）——从“极高的期待”到“极度的怀疑”}, url={https://zhuanlan.zhihu.com/p/137004590}, abstractNote={神经网络简史（上）——从“极高的期待”到“极度的怀疑” - 来自知乎专栏「漫谈人工智能」，作者: 人民邮电出版社 https://zhuanlan.zhihu.com/p/137004590}, note={赞数:74;}, journal={漫谈人工智能}, author={人民邮电出版社}, year={2020}, month=apr, language={zh} }

@book{LiHang_2019, title={统计学习方法 (第2版)}, ISBN={978-7-302-51727-6}, url={https://book.douban.com/subject/33437381/}, publisher={<a href="https://book.douban.com/press/2562">清华大学出版社</a>    <br>                                                      <span class="pl">出版年:</span> 2019-5}, author={李航}, year={2019}, month=may, language={zh} }

@article{marvin1969perceptrons,
  title={Perceptrons},
  author={Marvin, Minsky and Seymour, A Papert},
  journal={Cambridge, MA: MIT Press},
  volume={6},
  number={318-362},
  pages={7},
  year={1969}
}

@online{1313ShiZhan,
  title = {13.13. 实战 {{Kaggle}} 比赛：图像分类 ({{CIFAR-10}})},
  url = {https://developer.huaweicloud.com/develop/aigallery/notebook/detail?id=e0121485-bb4f-4011-82aa-85d006192132},
  urldate = {2022-11-14},
  file = {D:\ProgramFiles\Zotero\storage\5M9YRYWY\detail.html}
}

@online{17FengSiXinTiaoXiaoXi,
  title = {(17 封私信 / 1 条消息) 深度学习调参有哪些技巧？ - 知乎},
  url = {https://www.zhihu.com/question/25097993/answer/2717281021},
  urldate = {2022-12-02}
}

@online{42FengSiXin24,
  title = {(42 封私信 / 24 条消息) 如何看待 {{NLP}} 领域的 Prompt，能否借鉴到 {{CV}} 领域？ - 知乎},
  url = {https://www.zhihu.com/question/487096135/answer/2515263283},
  urldate = {2023-07-15},
  langid = {english},
  file = {D:\ProgramFiles\Zotero\storage\9ESEXN6X\2515263283.html}
}

@online{88TiaoXiaoXiCeShiHanShuAckely,
  title = {(88条消息) 测试函数： {{Ackely}}，{{Rastrigin}}，{{Griewangk}}，{{SumSquartes}}，{{Sphere}}，{{Quartic}}，{{Schwefel}}' {{Problem}} 12等\_橘子甜不甜的博客-{{CSDN博客}}},
  url = {https://blog.csdn.net/luolang_103/article/details/80886865},
  urldate = {2023-03-10},
  file = {D:\ProgramFiles\Zotero\storage\EST54C86\80886865.html}
}

@online{95TiaoXiaoXiXiLiDuTuXiangFenLeiFGVC,
  title = {(95条消息) 细粒度图像分类（{{FGVC}}）---综述\_xys430381\_1的博客-{{CSDN博客}}},
  url = {https://blog.csdn.net/xys430381_1/article/details/89640699},
  urldate = {2023-07-15},
  langid = {english}
}

@online{99FengSiXin22,
  title = {(99+ 封私信 / 22 条消息) {{Prefix-Tuning}} 和 {{P-tuning-v2}} 有哪些区别？ - 知乎},
  url = {https://www.zhihu.com/question/610271015?write=&utm_id=0},
  urldate = {2024-05-29},
  langid = {english},
  keywords = {⛔ No INSPIRE recid found},
  file = {D:\ProgramFiles\Zotero\storage\8QHADLTE\610271015.html}
}

@inproceedings{aghajanyanIntrinsicDimensionalityExplains2021,
  title = {Intrinsic {{Dimensionality Explains}} the {{Effectiveness}} of {{Language Model Fine-Tuning}}},
  booktitle = {Proceedings of the 59th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} and the 11th {{International Joint Conference}} on {{Natural Language Processing}} ({{Volume}} 1: {{Long Papers}})},
  author = {Aghajanyan, Armen and Gupta, Sonal and Zettlemoyer, Luke},
  editor = {Zong, Chengqing and Xia, Fei and Li, Wenjie and Navigli, Roberto},
  date = {2021-08},
  pages = {7319--7328},
  publisher = {Association for Computational Linguistics},
  location = {Online},
  doi = {10.18653/v1/2021.acl-long.568},
  url = {https://aclanthology.org/2021.acl-long.568},
  urldate = {2024-04-09},
  abstract = {Although pretrained language models can be fine-tuned to produce state-of-the-art results for a very wide range of language understanding tasks, the dynamics of this process are not well understood, especially in the low data regime. Why can we use relatively vanilla gradient descent algorithms (e.g., without strong regularization) to tune a model with hundreds of millions of parameters on datasets with only hundreds or thousands of labeled examples? In this paper, we argue that analyzing fine-tuning through the lens of intrinsic dimension provides us with empirical and theoretical intuitions to explain this remarkable phenomenon. We empirically show that common pre-trained models have a very low intrinsic dimension; in other words, there exists a low dimension reparameterization that is as effective for fine-tuning as the full parameter space. For example, by optimizing only 200 trainable parameters randomly projected back into the full space, we can tune a RoBERTa model to achieve 90\% of the full parameter performance levels on MRPC. Furthermore, we empirically show that pre-training implicitly minimizes intrinsic dimension and, perhaps surprisingly, larger models tend to have lower intrinsic dimension after a fixed number of pre-training updates, at least in part explaining their extreme effectiveness. Lastly, we connect intrinsic dimensionality with low dimensional task representations and compression based generalization bounds to provide intrinsic-dimension-based generalization bounds that are independent of the full parameter count.},
  eventtitle = {{{ACL-IJCNLP}} 2021},
  langid = {english},
  keywords = {⛔ No INSPIRE recid found},
  annotation = {36 citations (Crossref) [2024-04-10]},
  file = {D:\Zotero文献库\00人工智能\00机器学习\学习范式\迁移学习\PEFT\lora类\Aghajanyan et al_2021_Intrinsic Dimensionality Explains the Effectiveness of Language Model.pdf}
}

@online{ainsworthGitReBasinMerging2023,
  title = {Git {{Re-Basin}}: {{Merging Models}} modulo {{Permutation Symmetries}}},
  shorttitle = {Git {{Re-Basin}}},
  author = {Ainsworth, Samuel K. and Hayase, Jonathan and Srinivasa, Siddhartha},
  date = {2023-03-01},
  eprint = {2209.04836},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2209.04836},
  url = {http://arxiv.org/abs/2209.04836},
  urldate = {2023-08-02},
  abstract = {The success of deep learning is due in large part to our ability to solve certain massive non-convex optimization problems with relative ease. Though non-convex optimization is NP-hard, simple algorithms -- often variants of stochastic gradient descent -- exhibit surprising effectiveness in fitting large neural networks in practice. We argue that neural network loss landscapes often contain (nearly) a single basin after accounting for all possible permutation symmetries of hidden units a la Entezari et al. 2021. We introduce three algorithms to permute the units of one model to bring them into alignment with a reference model in order to merge the two models in weight space. This transformation produces a functionally equivalent set of weights that lie in an approximately convex basin near the reference model. Experimentally, we demonstrate the single basin phenomenon across a variety of model architectures and datasets, including the first (to our knowledge) demonstration of zero-barrier linear mode connectivity between independently trained ResNet models on CIFAR-10. Additionally, we identify intriguing phenomena relating model width and training time to mode connectivity. Finally, we discuss shortcomings of the linear mode connectivity hypothesis, including a counterexample to the single basin theory.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {C\:\\Users\\YeCanming\\Documents\\WPS Cloud Files\\200995647\\Zotero文献库\\人工智能\\创新实践\\prompt\\Ainsworth et al_2023_Git Re-Basin.pdf;D\:\\ProgramFiles\\Zotero\\storage\\BV6J6LBI\\2209.html}
}

@online{aisuanfayutuxiangchuli01PyTorchPrerequisitesSyllabus,
  title = {01PyTorch Prerequisites - Syllabus for Neural Network Programming Course-v5cngxo\_哔哩哔哩\_bilibili},
  author = {AI算法与图像处理},
  url = {https://www.bilibili.com/video/BV1uJ41117t6/},
  urldate = {2022-09-15},
  abstract = {01PyTorch Prerequisites - Syllabus for Neural Network Programming Course-v5cngxo是《高效入门pytorch视频教程》（完整版）的第1集视频，该合集共计39集，视频收藏或关注UP主，及时了解更多相关视频内容。},
  langid = {chinese-simplified}
}

@online{AmazonReviewData,
  title = {Amazon Review Data},
  url = {http://jmcauley.ucsd.edu/data/amazon/},
  urldate = {2022-09-10},
  file = {D:\ProgramFiles\Zotero\storage\HPP8T5B6\amazon.html}
}

@online{associationIndexAnacondaArchive,
  title = {Index of /Anaconda/Archive/ | 清华大学开源软件镜像站 | {{Tsinghua Open Source Mirror}}},
  author = {Association, Tsinghua University TUNA},
  url = {https://mirrors.tuna.tsinghua.edu.cn/fancy-index/before.html},
  urldate = {2022-11-15},
  abstract = {Index of /anaconda/archive/ | 清华大学开源软件镜像站，致力于为国内和校内用户提供高质量的开源软件镜像、Linux 镜像源服务，帮助用户更方便地获取开源软件。本镜像站由清华大学 TUNA 协会负责运行维护。}
}

@article{asunthaDeepLearningLung2020,
  title = {Deep Learning for Lung {{Cancer}} Detection and Classification},
  author = {Asuntha, A. and Srinivasan, Andy},
  date = {2020-03},
  journaltitle = {Multimedia Tools and Applications},
  shortjournal = {Multimed Tools Appl},
  volume = {79},
  number = {11-12},
  pages = {7731--7762},
  issn = {1380-7501, 1573-7721},
  doi = {10.1007/s11042-019-08394-3},
  url = {http://link.springer.com/10.1007/s11042-019-08394-3},
  urldate = {2023-11-11},
  abstract = {Lung cancer is one of the main reasons for death in the world among both men and women, with an impressive rate of about five million deadly cases per year. Computed Tomography (CT) scan can provide valuable information in the diagnosis of lung diseases. The main objective of this work is to detect the cancerous lung nodules from the given input lung image and to classify the lung cancer and its severity. To detect the location of the cancerous lung nodules, this work uses novel Deep learning methods. This work uses best feature extraction techniques such as Histogram of oriented Gradients (HoG), wavelet transform-based features, Local Binary Pattern (LBP), Scale Invariant Feature Transform (SIFT) and Zernike Moment. After extracting texture, geometric, volumetric and intensity features, Fuzzy Particle Swarm Optimization (FPSO) algorithm is applied for selecting the best feature. Finally, these features are classified using Deep learning. A novel FPSOCNN reduces computational complexity of CNN. An additional valuation is performed on another dataset coming from Arthi Scan Hospital which is a real-time data set. From the experimental results, it is shown that novel FPSOCNN performs better than other techniques.},
  langid = {english},
  file = {D:\ProgramFiles\Zotero\storage\36YAPQGV\Asuntha and Srinivasan - 2020 - Deep learning for lung Cancer detection and classi.pdf}
}

@article{baliCognizantMultitaskingMultiobjective2021,
  title = {Cognizant {{Multitasking}} in {{Multiobjective Multifactorial Evolution}}: {{MO-MFEA-II}}},
  shorttitle = {Cognizant {{Multitasking}} in {{Multiobjective Multifactorial Evolution}}},
  author = {Bali, Kavitesh Kumar and Gupta, Abhishek and Ong, Yew-Soon and Tan, Puay Siew},
  date = {2021-04},
  journaltitle = {IEEE Transactions on Cybernetics},
  volume = {51},
  number = {4},
  pages = {1784--1796},
  issn = {2168-2275},
  doi = {10.1109/TCYB.2020.2981733},
  abstract = {Humans have the ability to identify recurring patterns in diverse situations encountered over a lifetime, constantly understanding relationships between tasks and efficiently solving them through knowledge reuse. The capacity of artificial intelligence systems to mimic such cognitive behaviors for effective problem solving is deemed invaluable, particularly when tackling real-world problems where speed and accuracy are critical. Recently, the notion of evolutionary multitasking has been explored as a means of solving multiple optimization tasks simultaneously using a single population of evolving individuals. In the presence of similarities (or even partial overlaps) between high-quality solutions of related optimization problems, the resulting scope for intertask genetic transfer often leads to significant performance speedup-as the cost of re-exploring overlapping regions of the search space is reduced. While multitasking solvers have led to recent success stories, a known shortcoming of existing methods is their inability to adapt the extent of transfer in a principled manner. Thus, in the absence of any prior knowledge about the relationships between optimization functions, a threat of predominantly negative (harmful) transfer prevails. With this in mind, this article presents a realization of a cognizant evolutionary multitasking engine within the domain of multiobjective optimization. Our proposed algorithm learns intertask relationships based on overlaps in the probabilistic search distributions derived from data generated during the course of multitasking-and accordingly adapts the extent of genetic transfers online. The efficacy of the method is substantiated on multiobjective benchmark problems as well as a practical case study of knowledge transfers from low-fidelity optimization tasks to substantially reduce the cost of high-fidelity optimization.},
  eventtitle = {{{IEEE Transactions}} on {{Cybernetics}}},
  langid = {english},
  keywords = {Evolutionary computation,Evolutionary multitasking,Genetics,Knowledge transfer,multifactorial optimization,multiobjective optimization,Multitasking,online similarity learning,Optimization,Probabilistic logic,probabilistic modeling,Task analysis},
  file = {C\:\\Users\\YeCanming\\Documents\\WPS Cloud Files\\200995647\\Zotero文献库\\人工智能\\创新实践\\多目标优化\\Bali et al_2021_Cognizant Multitasking in Multiobjective Multifactorial Evolution.pdf;D\:\\ProgramFiles\\Zotero\\storage\\RK72SKYB\\stamp.html}
}

@inproceedings{barVisualPromptingImage2022,
  title = {Visual {{Prompting}} via {{Image Inpainting}}},
  author = {Bar, Amir and Gandelsman, Yossi and Darrell, Trevor and Globerson, Amir and Efros, Alexei A.},
  date = {2022-10-31},
  url = {https://openreview.net/forum?id=o4uFFg9_TpV},
  urldate = {2024-09-27},
  abstract = {How does one adapt a pre-trained visual model to novel downstream tasks without task-specific finetuning or any model modification? Inspired by prompting in NLP, this paper investigates visual prompting: given input-output image example(s) of a new task at test time and a new input image, the goal is to automatically produce the output image, consistent with the given examples. We show that posing this problem as simple image inpainting -- literally just filling in a hole in a concatenated visual prompt image -- turns out to be surprisingly effective, provided that the inpainting algorithm has been trained on the right data. We train masked auto-encoders on a new dataset that we curated -- 88k unlabeled figures from academic papers sources on Arxiv. We apply visual prompting to these pretrained models and demonstrate results on various downstream image-to-image tasks, including foreground segmentation, single object detection, colorization, edge detection, etc. Project page: https://yossigandelsman.github.io/visual\_prompt},
  eventtitle = {Advances in {{Neural Information Processing Systems}}},
  langid = {english},
  keywords = {⛔ No INSPIRE recid found},
  file = {D:\Zotero文献库\00人工智能\00机器学习\学习范式\迁移学习\PEFT\基于特征\prompt\Bar et al_2022_Visual Prompting via Image Inpainting.pdf}
}

@article{baxterModelInductiveBias2000,
  title = {A {{Model}} of {{Inductive Bias Learning}}},
  author = {Baxter, J.},
  date = {2000-03-01},
  journaltitle = {Journal of Artificial Intelligence Research},
  shortjournal = {jair},
  volume = {12},
  eprint = {1106.0245},
  eprinttype = {arXiv},
  eprintclass = {cs},
  pages = {149--198},
  issn = {1076-9757},
  doi = {10.1613/jair.731},
  url = {http://arxiv.org/abs/1106.0245},
  urldate = {2022-11-30},
  abstract = {A major problem in machine learning is that of inductive bias: how to choose a learner's hypothesis space so that it is large enough to contain a solution to the problem being learnt, yet small enough to ensure reliable generalization from reasonably-sized training sets. Typically such bias is supplied by hand through the skill and insights of experts. In this paper a model for automatically learning bias is investigated. The central assumption of the model is that the learner is embedded within an environment of related learning tasks. Within such an environment the learner can sample from multiple tasks, and hence it can search for a hypothesis space that contains good solutions to many of the problems in the environment. Under certain restrictions on the set of all hypothesis spaces available to the learner, we show that a hypothesis space that performs well on a sufficiently large number of training tasks will also perform well when learning novel tasks in the same environment. Explicit bounds are also derived demonstrating that learning multiple tasks within an environment of related tasks can potentially give much better generalization than learning a single task.},
  keywords = {Computer Science - Artificial Intelligence},
  file = {C\:\\Users\\YeCanming\\Documents\\WPS Cloud Files\\200995647\\Zotero文献库\\人工智能\\创新实践\\多任务学习\\Baxter_2000_A Model of Inductive Bias Learning.pdf;D\:\\ProgramFiles\\Zotero\\storage\\XU5IVI77\\1106.html}
}

@inproceedings{bertinettoLearningFeedforwardOneshot2016,
  title = {Learning Feed-Forward One-Shot Learners},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Bertinetto, Luca and Henriques, João F. and Valmadre, Jack and Torr, Philip and Vedaldi, Andrea},
  date = {2016},
  volume = {29},
  publisher = {Curran Associates, Inc.},
  url = {https://papers.nips.cc/paper/2016/hash/839ab46820b524afda05122893c2fe8e-Abstract.html},
  urldate = {2024-09-10},
  abstract = {One-shot learning is usually tackled by using generative models or discriminative embeddings. Discriminative methods based on deep learning, which are very effective in other learning scenarios, are ill-suited for one-shot learning as they need large amounts of training data. In this paper, we propose a method to learn the parameters of a deep model in one shot. We construct the learner as a second deep network, called a learnet, which predicts the parameters of a pupil network from a single exemplar. In this manner we obtain an efficient feed-forward one-shot learner, trained end-to-end by minimizing a one-shot classification objective in a learning to learn formulation. In order to make the construction feasible, we propose a number of factorizations of the parameters of the pupil network. We demonstrate encouraging results by learning characters from single exemplars in Omniglot, and by tracking visual objects from a single initial exemplar in the Visual Object Tracking benchmark.},
  langid = {english},
  keywords = {⛔ No INSPIRE recid found},
  file = {D\:\\ProgramFiles\\Zotero\\storage\\C6DVZPTL\\Bertinetto 等 - 2016 - Learning feed-forward one-shot learners.pdf;D\:\\Zotero文献库\\00人工智能\\00计算机视觉课程\\经典视觉任务的对应设计\\视觉地点定位\\创新点\\PEFT\\adapter类\\Bertinetto et al_2016_Learning feed-forward one-shot learners.pdf}
}

@online{BianMaXiaoDiAUCZhiBiaoFangKengBiJi2019,
  type = {知乎专栏文章},
  title = {AUC指标——防坑笔记},
  author = {编码小弟},
  date = {2019-07-25},
  url = {https://zhuanlan.zhihu.com/p/75131938},
  urldate = {2022-10-26},
  abstract = {AUC指标——防坑笔记 - 来自知乎专栏「常用机器学习/深度学习基础」，作者: 编码小弟 https://zhuanlan.zhihu.com/p/75131938},
  langid = {chinese},
  organization = {常用机器学习/深度学习基础},
  keywords = {推荐系统,机器学习,深度学习（Deep Learning）},
  annotation = {赞数:33;},
  file = {D:\ProgramFiles\Zotero\storage\KIF9QJCJ\75131938.html}
}

@article{biBayesOptimalHierarchicalMultilabel2015,
  title = {Bayes-{{Optimal Hierarchical Multilabel Classification}}},
  author = {Bi, Wei and Kwok, Jame T.},
  date = {2015-11},
  journaltitle = {IEEE Transactions on Knowledge and Data Engineering},
  volume = {27},
  number = {11},
  pages = {2907--2918},
  issn = {1558-2191},
  doi = {10.1109/TKDE.2015.2441707},
  abstract = {Hierarchical multilabel classification allows a sample to belong to multiple class labels residing on a hierarchy, which can be a tree or directed acyclic graph (DAG). However, popular hierarchical loss functions, such as the H-loss, can only be defined on tree hierarchies (but not on DAGs), and may also under- or over-penalize misclassifications near the bottom of the hierarchy. Besides, it has been relatively unexplored on how to make use of the loss functions in hierarchical multilabel classification. To overcome these deficiencies, we first propose hierarchical extensions of the Hamming loss and ranking loss which take the mistake at every node of the label hierarchy into consideration. Then, we first train a general learning model, which is independent of the loss function. Next, using Bayesian decision theory, we develop Bayes-optimal predictions that minimize the corresponding risks with the trained model. Computationally, instead of requiring an exhaustive summation and search for the optimal multilabel, the resultant optimization problem can be efficiently solved by a greedy algorithm. Experimental results on a number of real-world data sets show that the proposed Bayes-optimal classifier outperforms state-of-the-art methods.},
  eventtitle = {{{IEEE Transactions}} on {{Knowledge}} and {{Data Engineering}}},
  langid = {english},
  keywords = {Bayes methods,Bayesian decision theory,Decision theory,Greedy algorithms,hierarchical classification,Hierarchical classification,loss function,multilabel classification,Optimization,Prediction algorithms,Training},
  file = {C:\Users\YeCanming\Documents\WPS Cloud Files\200995647\Zotero文献库\人工智能\创新实践\prompt\Bi_Kwok_2015_Bayes-Optimal Hierarchical Multilabel Classification.pdf}
}

@article{biEfficientMultilabelClassification,
  title = {Efficient {{Multi-label Classification}} with {{Many Labels}}},
  author = {Bi, Wei and Kwok, James T},
  abstract = {In multi-label classification, each sample can be associated with a set of class labels. When the number of labels grows to the hundreds or even thousands, existing multi-label classification methods often become computationally inefficient. In recent years, a number of remedies have been proposed. However, they are based either on simple dimension reduction techniques or involve expensive optimization problems. In this paper, we address this problem by selecting a small subset of class labels that can approximately span the original label space. This is performed by an efficient randomized sampling procedure where the sampling probability of each class label reflects its importance among all the labels. Experiments on a number of realworld multi-label data sets with many labels demonstrate the appealing performance and efficiency of the proposed algorithm.},
  langid = {english},
  file = {D:\ProgramFiles\Zotero\storage\PTZA2N7X\icml13a.pdf}
}

@article{biMultiLabelClassificationTree,
  title = {Multi-{{Label Classification}} on {{Tree-}} and {{DAG-Structured Hierarchies}}},
  author = {Bi, Wei and Kwok, James T},
  abstract = {Many real-world applications involve multilabel classification, in which the labels are organized in the form of a tree or directed acyclic graph (DAG). However, current research efforts typically ignore the label dependencies or can only exploit the dependencies in tree-structured hierarchies. In this paper, we present a novel hierarchical multilabel classification algorithm which can be used on both tree- and DAG-structured hierarchies. The key idea is to formulate the search for the optimal consistent multi-label as the finding of the best subgraph in a tree/DAG. Using a simple greedy strategy, the proposed algorithm is computationally efficient, easy to implement, does not suffer from the problem of insufficient/skewed training data in classifier training, and can be readily used on large hierarchies. Theoretical results guarantee the optimality of the obtained solution. Experiments are performed on a large number of functional genomics data sets. The proposed method consistently outperforms the state-of-the-art method on both tree- and DAG-structured hierarchies.},
  langid = {english},
  file = {D:\ProgramFiles\Zotero\storage\KMW76CMP\Bi 和 Kwok - Multi-Label Classification on Tree- and DAG-Struct.pdf}
}

@book{bishopPatternRecognitionMachine2006,
  title = {Pattern Recognition and Machine Learning},
  author = {Bishop, Christopher M.},
  date = {2006},
  series = {Information Science and Statistics},
  publisher = {Springer},
  location = {New York},
  isbn = {978-0-387-31073-2},
  langid = {english},
  pagetotal = {738},
  keywords = {Machine learning,Pattern perception},
  file = {C\:\\Users\\YeCanming\\Documents\\WPS Cloud Files\\200995647\\Zotero文献库\\人工智能\\机器学习课程\\Bishop_2006_Pattern recognition and machine learning.pdf;C\:\\Users\\YeCanming\\Documents\\WPS Cloud Files\\200995647\\Zotero文献库\\人工智能\\机器学习课程\\Bishop_2006_Pattern recognition and machine learning2.pdf;C\:\\Users\\YeCanming\\Documents\\WPS Cloud Files\\200995647\\Zotero文献库\\人工智能\\机器学习课程\\Bishop_2006_Pattern recognition and machine learning3.pdf}
}

@online{bot14593JiYuMaShiJuChiDeYiChangZhiJianYan2018,
  type = {知乎专栏文章},
  title = {基于马氏距离的异常值检验},
  author = {Bot14593},
  date = {2018-12-16T03:29:42},
  url = {https://zhuanlan.zhihu.com/p/52499462},
  urldate = {2023-11-09},
  abstract = {基于马氏距离的异常值检验 - 来自知乎专栏「R/Python 数据科学实战」，作者: Bot14593 https://zhuanlan.zhihu.com/p/52499462},
  langid = {chinese},
  organization = {R/Python 数据科学实战},
  keywords = {R（编程语言）,数据分析,统计学},
  annotation = {赞数:25;}
}

@online{bullettechShiYongTensorFlowDecisionForestsGouJianShuMoXing2022,
  type = {知乎专栏文章},
  title = {{{使用TensorFlow Decision Forests构建树模型}}},
  author = {BulletTech},
  date = {2022-04-24T11:10:56},
  url = {https://zhuanlan.zhihu.com/p/504279377},
  urldate = {2023-11-10},
  abstract = {使用TensorFlow Decision Forests构建树模型 - 来自知乎专栏「人工智能」，作者: BulletTech https://zhuanlan.zhihu.com/p/504279377},
  langid = {english},
  organization = {人工智能},
  keywords = {TensorFlow 学习,植物,深度学习（Deep Learning）},
  annotation = {赞数:3;}
}

@article{caiFeatureSelectionMachine2018,
  title = {Feature Selection in Machine Learning: {{A}} New Perspective},
  shorttitle = {Feature Selection in Machine Learning},
  author = {Cai, Jie and Luo, Jiawei and Wang, Shulin and Yang, Sheng},
  date = {2018-07},
  journaltitle = {Neurocomputing},
  shortjournal = {Neurocomputing},
  volume = {300},
  pages = {70--79},
  issn = {09252312},
  doi = {10.1016/j.neucom.2017.11.077},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0925231218302911},
  urldate = {2023-10-19},
  abstract = {High-dimensional data analysis is a challenge for researchers and engineers in the fields of machine learning and data mining. Feature selection provides an effective way to solve this problem by removing irrelevant and redundant data, which can reduce computation time, improve learning accuracy, and facilitate a better understanding for the learning model or data. In this study, we discuss several frequently-used evaluation measures for feature selection, and then survey supervised, unsupervised, and semi-supervised feature selection methods, which are widely applied in machine learning problems, such as classification and clustering. Lastly, future challenges about feature selection are discussed.},
  langid = {english},
  file = {C:\Users\YeCanming\Documents\WPS Cloud Files\200995647\Zotero文献库\人工智能\医学人工智能\AI知识查缺补漏\特征工程\Cai et al_2018_Feature selection in machine learning.pdf}
}

@article{caruanaMultitaskLearning1997,
  title = {Multitask {{Learning}}},
  author = {Caruana, Rich},
  date = {1997-07-01},
  journaltitle = {Machine Learning},
  shortjournal = {Machine Learning},
  volume = {28},
  number = {1},
  pages = {41--75},
  issn = {1573-0565},
  doi = {10.1023/A:1007379606734},
  url = {https://doi.org/10.1023/A:1007379606734},
  urldate = {2023-05-03},
  abstract = {Multitask Learning is an approach to inductive transfer that improves generalization by using the domain information contained in the training signals of related tasks as an inductive bias. It does this by learning tasks in parallel while using a shared representation; what is learned for each task can help other tasks be learned better. This paper reviews prior work on MTL, presents new evidence that MTL in backprop nets discovers task relatedness without the need of supervisory signals, and presents new results for MTL with k-nearest neighbor and kernel regression. In this paper we demonstrate multitask learning in three domains. We explain how multitask learning works, and show that there are many opportunities for multitask learning in real domains. We present an algorithm and results for multitask learning with case-based methods like k-nearest neighbor and kernel regression, and sketch an algorithm for multitask learning in decision trees. Because multitask learning works, can be applied to many different kinds of domains, and can be used with different learning algorithms, we conjecture there will be many opportunities for its use on real-world problems.},
  langid = {english},
  keywords = {backpropagation,generalization,inductive transfer,k-nearest neighbor,kernel regression,multitask learning,parallel transfer,supervised learning},
  file = {C:\Users\YeCanming\Documents\WPS Cloud Files\200995647\Zotero文献库\人工智能\创新实践\多任务学习\多任务学习\Caruana_1997_Multitask Learning.pdf}
}

@incollection{caruanaMultitaskLearningKnowledgeBased1993,
  title = {Multitask {{Learning}}: {{A Knowledge-Based Source}} of {{Inductive Bias}}},
  shorttitle = {Multitask {{Learning}}},
  booktitle = {Machine {{Learning Proceedings}} 1993},
  author = {Caruana, Richard A.},
  date = {1993},
  pages = {41--48},
  publisher = {Elsevier},
  doi = {10.1016/B978-1-55860-307-3.50012-5},
  url = {https://linkinghub.elsevier.com/retrieve/pii/B9781558603073500125},
  urldate = {2023-05-03},
  abstract = {This paper suggests that it may be easier to learn several hard tasks at one time than to learn these same tasks separately. In effect, the information provided by the training signal for each task serves as a domain-specific inductive bias for the other tasks. Frequently the world gives us clusters of related tasks to learn. When it does not, it is often straightforward to create additional tasks. For many domains, acquiring inductive bias by collecting additional teaching signal may be more practical than the traditional approach of codifying domain-specific biases acquired from human expertise. We call this approach Multitask Learning (MTL). Since much of the power of an inductive learner follows directly from its inductive bias, multitask learning may yield more powerful learning. An empirical example of multitask connectionist learning is presented where learning improves by training one network on several related tasks at the same time. Multitask decision tree induction is also outlined.},
  isbn = {978-1-55860-307-3},
  langid = {english},
  file = {C:\Users\YeCanming\Documents\WPS Cloud Files\200995647\Zotero文献库\人工智能\创新实践\多任务学习\多任务学习\Caruana_1993_Multitask Learning.pdf}
}

@online{casperOpenProblemsFundamental2023,
  title = {Open {{Problems}} and {{Fundamental Limitations}} of {{Reinforcement Learning}} from {{Human Feedback}}},
  author = {Casper, Stephen and Davies, Xander and Shi, Claudia and Gilbert, Thomas Krendl and Scheurer, Jérémy and Rando, Javier and Freedman, Rachel and Korbak, Tomasz and Lindner, David and Freire, Pedro and Wang, Tony and Marks, Samuel and Segerie, Charbel-Raphaël and Carroll, Micah and Peng, Andi and Christoffersen, Phillip and Damani, Mehul and Slocum, Stewart and Anwar, Usman and Siththaranjan, Anand and Nadeau, Max and Michaud, Eric J. and Pfau, Jacob and Krasheninnikov, Dmitrii and Chen, Xin and Langosco, Lauro and Hase, Peter and Bıyık, Erdem and Dragan, Anca and Krueger, David and Sadigh, Dorsa and Hadfield-Menell, Dylan},
  date = {2023-07-27},
  eprint = {2307.15217},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2307.15217},
  urldate = {2023-08-04},
  abstract = {Reinforcement learning from human feedback (RLHF) is a technique for training AI systems to align with human goals. RLHF has emerged as the central method used to finetune state-of-the-art large language models (LLMs). Despite this popularity, there has been relatively little public work systematizing its flaws. In this paper, we (1) survey open problems and fundamental limitations of RLHF and related methods; (2) overview techniques to understand, improve, and complement RLHF in practice; and (3) propose auditing and disclosure standards to improve societal oversight of RLHF systems. Our work emphasizes the limitations of RLHF and highlights the importance of a multi-faceted approach to the development of safer AI systems.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {D:\ProgramFiles\Zotero\storage\3Y68NYWA\Casper 等 - 2023 - Open Problems and Fundamental Limitations of Reinf.pdf}
}

@video{cbcslteachingLectureMachineLearning2018,
  entrysubtype = {video},
  title = {Lecture 7 | {{Machine Learning}}},
  editor = {{CBCSL teaching}},
  editortype = {director},
  date = {2018-09-20},
  url = {https://www.youtube.com/watch?v=z_k4aiBnoVA},
  urldate = {2022-10-04}
}

@online{ccShuoChiQunZhiDeJianCeHeChuLiFangFa2022,
  type = {知乎专栏文章},
  title = {离群值的检测和处理方法},
  author = {CC说},
  date = {2022-11-18T03:35:14},
  url = {https://zhuanlan.zhihu.com/p/584534130},
  urldate = {2023-11-09},
  abstract = {离群值的检测和处理方法 - 来自知乎专栏「数据科学」，作者: CC说 https://zhuanlan.zhihu.com/p/584534130},
  langid = {chinese},
  organization = {数据科学},
  keywords = {数据分析,数据挖掘,数据科学},
  annotation = {赞数:20;},
  file = {D:\ProgramFiles\Zotero\storage\BJD9QREG\584534130.html}
}

@inproceedings{chauhanEfficientDataMining2016,
  title = {An Efficient Data Mining Classification Approach for Detecting Lung Cancer Disease},
  booktitle = {2016 {{International Conference}} on {{Communication}} and {{Electronics Systems}} ({{ICCES}})},
  author = {Chauhan, Divya and Jaiswal, Varun},
  date = {2016-10},
  pages = {1--8},
  publisher = {IEEE},
  location = {Coimbatore, India},
  doi = {10.1109/CESYS.2016.7889872},
  url = {http://ieeexplore.ieee.org/document/7889872/},
  urldate = {2023-07-30},
  abstract = {Background: Automated disease classification using machine learning often relies on features derived from segmenting individual objects, which can be difficult to automate. Proposed model is a classification based an efficient approach in which machine learning concepts are used for the detection of Lung cancer diseases. The algorithm obtained encouraging results but requires considerable computational expertise to execute. Furthermore, some benchmark sets have been shown to compare the proposed work model working.},
  eventtitle = {2016 {{International Conference}} on {{Communication}} and {{Electronics Systems}} ({{ICCES}})},
  isbn = {978-1-5090-1065-3 978-1-5090-1066-0},
  langid = {english},
  file = {D:\ProgramFiles\Zotero\storage\CBIINPP5\Chauhan 和 Jaiswal - 2016 - An efficient data mining classification approach f.pdf}
}

@online{chavanOneforAllGeneralizedLoRA2023,
  title = {One-for-{{All}}: {{Generalized LoRA}} for {{Parameter-Efficient Fine-tuning}}},
  shorttitle = {One-for-{{All}}},
  author = {Chavan, Arnav and Liu, Zhuang and Gupta, Deepak and Xing, Eric and Shen, Zhiqiang},
  date = {2023-10-16},
  eprint = {2306.07967},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2306.07967},
  url = {http://arxiv.org/abs/2306.07967},
  urldate = {2024-03-21},
  abstract = {We present Generalized LoRA (GLoRA), an advanced approach for universal parameter-efficient fine-tuning tasks. Enhancing Low-Rank Adaptation (LoRA), GLoRA employs a generalized prompt module to optimize pre-trained model weights and adjust intermediate activations, providing more flexibility and capability across diverse tasks and datasets. Moreover, GLoRA facilitates efficient parameter adaptation by employing a scalable, modular, layer-wise structure search that learns individual adapter of each layer. Originating from a unified mathematical formulation, GLoRA exhibits strong transfer learning, few-shot learning and domain generalization abilities, as it adapts to new tasks through not only weights but also additional dimensions like activations. Comprehensive experiments demonstrate that GLoRA outperforms all previous methods in natural, specialized, and structured vision benchmarks, achieving superior accuracy with fewer parameters and computations. The proposed method on LLaMA-1 and LLaMA-2 also show considerable enhancements compared to the original LoRA in the language domain. Furthermore, our structural re-parameterization design ensures that GLoRA incurs no extra inference cost, rendering it a practical solution for resource-limited applications. Code and models are available at: https://github.com/Arnav0400/ViT-Slim/tree/master/GLoRA.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {⛔ No INSPIRE recid found,Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {D\:\\Zotero文献库\\00人工智能\\00计算机视觉课程\\经典视觉任务的对应设计\\视觉地点定位\\创新点\\PEFT\\Chavan et al_2023_One-for-All.pdf;D\:\\ProgramFiles\\Zotero\\storage\\6VMKCY8E\\2306.html}
}

@online{chenAdaptFormerAdaptingVision2022,
  title = {{{AdaptFormer}}: {{Adapting Vision Transformers}} for {{Scalable Visual Recognition}}},
  shorttitle = {{{AdaptFormer}}},
  author = {Chen, Shoufa and Ge, Chongjian and Tong, Zhan and Wang, Jiangliu and Song, Yibing and Wang, Jue and Luo, Ping},
  date = {2022-10-14},
  eprint = {2205.13535},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2205.13535},
  url = {http://arxiv.org/abs/2205.13535},
  urldate = {2024-04-23},
  abstract = {Pretraining Vision Transformers (ViTs) has achieved great success in visual recognition. A following scenario is to adapt a ViT to various image and video recognition tasks. The adaptation is challenging because of heavy computation and memory storage. Each model needs an independent and complete finetuning process to adapt to different tasks, which limits its transferability to different visual domains. To address this challenge, we propose an effective adaptation approach for Transformer, namely AdaptFormer, which can adapt the pre-trained ViTs into many different image and video tasks efficiently. It possesses several benefits more appealing than prior arts. Firstly, AdaptFormer introduces lightweight modules that only add less than 2\% extra parameters to a ViT, while it is able to increase the ViT's transferability without updating its original pre-trained parameters, significantly outperforming the existing 100\textbackslash\% fully fine-tuned models on action recognition benchmarks. Secondly, it can be plug-and-play in different Transformers and scalable to many visual tasks. Thirdly, extensive experiments on five image and video datasets show that AdaptFormer largely improves ViTs in the target domains. For example, when updating just 1.5\% extra parameters, it achieves about 10\% and 19\% relative improvement compared to the fully fine-tuned models on Something-Something\textasciitilde v2 and HMDB51, respectively. Code is available at https://github.com/ShoufaChen/AdaptFormer.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {⛔ No INSPIRE recid found,Computer Science - Computer Vision and Pattern Recognition},
  file = {D\:\\Zotero文献库\\00人工智能\\00机器学习\\学习范式\\迁移学习\\PEFT\\adapter类\\Chen et al_2022_AdaptFormer.pdf;D\:\\ProgramFiles\\Zotero\\storage\\HZDY9A9F\\2205.html}
}

@online{chenConvAdapterExploringParameter2024,
  title = {Conv-{{Adapter}}: {{Exploring Parameter Efficient Transfer Learning}} for {{ConvNets}}},
  shorttitle = {Conv-{{Adapter}}},
  author = {Chen, Hao and Tao, Ran and Zhang, Han and Wang, Yidong and Li, Xiang and Ye, Wei and Wang, Jindong and Hu, Guosheng and Savvides, Marios},
  date = {2024-04-12},
  eprint = {2208.07463},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2208.07463},
  urldate = {2024-04-30},
  abstract = {While parameter efficient tuning (PET) methods have shown great potential with transformer architecture on Natural Language Processing (NLP) tasks, their effectiveness with large-scale ConvNets is still under-studied on Computer Vision (CV) tasks. This paper proposes Conv-Adapter, a PET module designed for ConvNets. Conv-Adapter is light-weight, domain-transferable, and architecture-agnostic with generalized performance on different tasks. When transferring on downstream tasks, Conv-Adapter learns tasks-specific feature modulation to the intermediate representations of backbones while keeping the pre-trained parameters frozen. By introducing only a tiny amount of learnable parameters, e.g., only 3.5\% full fine-tuning parameters of ResNet50. It can also be applied for transformer-based backbones. Conv-Adapter outperforms previous PET baseline methods and achieves comparable or surpasses the performance of full fine-tuning on 23 classification tasks of various domains. It also presents superior performance on the few-shot classification with an average margin of 3.39\%. Beyond classification, Conv-Adapter can generalize to detection and segmentation tasks with more than 50\% reduction of parameters but comparable performance to the traditional full fine-tuning.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {⛔ No INSPIRE recid found,Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition},
  file = {D:\ProgramFiles\Zotero\storage\LD7S4WGT\Chen 等 - 2024 - Conv-Adapter Exploring Parameter Efficient Transf.pdf}
}

@online{chenEfficientAdaptersGiant2023,
  title = {Efficient {{Adapters}} for {{Giant Speech Models}}},
  author = {Chen, Nanxin and Shafran, Izhak and Zhang, Yu and Chiu, Chung-Cheng and Soltau, Hagen and Qin, James and Wu, Yonghui},
  date = {2023-06-13},
  eprint = {2306.08131},
  eprinttype = {arXiv},
  eprintclass = {cs, eess},
  doi = {10.48550/arXiv.2306.08131},
  url = {http://arxiv.org/abs/2306.08131},
  urldate = {2024-04-14},
  abstract = {Large pre-trained speech models are widely used as the de-facto paradigm, especially in scenarios when there is a limited amount of labeled data available. However, finetuning all parameters from the self-supervised learned model can be computationally expensive, and becomes infeasiable as the size of the model and the number of downstream tasks scales. In this paper, we propose a novel approach called Two Parallel Adapter (TPA) that is inserted into the conformer-based model pre-trained model instead. TPA is based on systematic studies of the residual adapter, a popular approach for finetuning a subset of parameters. We evaluate TPA on various public benchmarks and experiment results demonstrates its superior performance, which is close to the full finetuning on different datasets and speech tasks. These results show that TPA is an effective and efficient approach for serving large pre-trained speech models. Ablation studies show that TPA can also be pruned, especially for lower blocks.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {⛔ No INSPIRE recid found,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  file = {D\:\\Zotero文献库\\00人工智能\\00机器学习\\学习范式\\迁移学习\\PEFT\\adapter类\\Chen et al_2023_Efficient Adapters for Giant Speech Models.pdf;D\:\\ProgramFiles\\Zotero\\storage\\QYRDEDX3\\2306.html}
}

@article{chenKnowledgeguideHierarchicalLearning2021,
  title = {A Knowledge-Guide Hierarchical Learning Method for Long-Tailed Image Classification},
  author = {Chen, Qiong and Liu, Qingfa and Lin, Enlu},
  date = {2021-10-12},
  journaltitle = {Neurocomputing},
  shortjournal = {Neurocomputing},
  volume = {459},
  pages = {408--418},
  issn = {0925-2312},
  doi = {10.1016/j.neucom.2021.07.008},
  url = {https://www.sciencedirect.com/science/article/pii/S0925231221010420},
  urldate = {2024-04-05},
  abstract = {Deep visual recognition methods have achieved excellent performance on artificially constructed image datasets where the data distribution is balanced. However, in real-world scenarios, data distribution is usually extremely imbalanced and exhibit a long-tailed distribution where data in each head class is more than the class in the tail. Many efficient deep learning methods fail to work normally, i.e., they perform well in the head class while poor in the tail class. In this paper, we propose a two-layer Hierarchical-Learning Long-Tailed Recognition (HL-LTR) algorithm which transforms the long-tailed problem into a hierarchical classification problem by constructing a hierarchical superclass tree in which each layer corresponds to a recognition task. In the first layer of the tree, the degree of data imbalance is largely decreased. The recognition task of the second layer is the original long-tailed recognition problem. The training of HL-LTR is top-down. The knowledge learned by the first layer transfers to classes of the second layer and guides the feature learning of the second layer by using attention mechanism module and knowledge distillation method. Compared with directly solving the most difficult long-tailed recognition task, HL-LTR achieves better performance due to its progressive learning method from easy to difficult and effective knowledge transfer strategy.},
  langid = {english},
  keywords = {⛔ No INSPIRE recid found,Image classification,Imbalanced data,Long-tailed distribution},
  annotation = {14 citations (Crossref) [2024-04-06]}
}

@article{chiccoAdvantagesMatthewsCorrelation2020,
  title = {The Advantages of the {{Matthews}} Correlation Coefficient ({{MCC}}) over {{F1}} Score and Accuracy in Binary Classification Evaluation},
  author = {Chicco, Davide and Jurman, Giuseppe},
  date = {2020-01-02},
  journaltitle = {BMC Genomics},
  shortjournal = {BMC Genomics},
  volume = {21},
  number = {1},
  pages = {6},
  issn = {1471-2164},
  doi = {10.1186/s12864-019-6413-7},
  url = {https://doi.org/10.1186/s12864-019-6413-7},
  urldate = {2023-08-25},
  abstract = {To evaluate binary classifications and their confusion matrices, scientific researchers can employ several statistical rates, accordingly to the goal of the experiment they are investigating. Despite being a crucial issue in machine learning, no widespread consensus has been reached on a unified elective chosen measure yet. Accuracy and F1 score computed on confusion matrices have been (and still are) among the most popular adopted metrics in binary classification tasks. However, these statistical measures can dangerously show overoptimistic inflated results, especially on imbalanced datasets.},
  langid = {english},
  keywords = {Accuracy,Binary classification,Biostatistics,Confusion matrices,Dataset imbalance,F1 score,Genomics,Machine learning,Matthews correlation coefficient},
  file = {C\:\\Users\\YeCanming\\Documents\\WPS Cloud Files\\200995647\\Zotero文献库\\人工智能\\医学人工智能\\指标选择\\Chicco_Jurman_2020_The advantages of the Matthews correlation coefficient (MCC) over F1 score and.pdf;D\:\\ProgramFiles\\Zotero\\storage\\MK2AAMY6\\s12864-019-6413-7.html}
}

@article{chiccoMatthewsCorrelationCoefficient2021,
  title = {The {{Matthews}} Correlation Coefficient ({{MCC}}) Is More Reliable than Balanced Accuracy, Bookmaker Informedness, and Markedness in Two-Class Confusion Matrix Evaluation},
  author = {Chicco, Davide and Tötsch, Niklas and Jurman, Giuseppe},
  date = {2021-02-04},
  journaltitle = {BioData Mining},
  shortjournal = {BioData Mining},
  volume = {14},
  number = {1},
  pages = {13},
  issn = {1756-0381},
  doi = {10.1186/s13040-021-00244-z},
  url = {https://doi.org/10.1186/s13040-021-00244-z},
  urldate = {2023-08-25},
  abstract = {Evaluating binary classifications is a pivotal task in statistics and machine learning, because it can influence decisions in multiple areas, including for example prognosis or therapies of patients in critical conditions. The scientific community has not agreed on a general-purpose statistical indicator for evaluating two-class confusion matrices (having true positives, true negatives, false positives, and false negatives) yet, even if advantages of the Matthews correlation coefficient (MCC) over accuracy and F1 score have already been shown.In this manuscript, we reaffirm that MCC is a robust metric that summarizes the classifier performance in a single value, if positive and negative cases are of equal importance. We compare MCC to other metrics which value positive and negative cases equally: balanced accuracy (BA), bookmaker informedness (BM), and markedness (MK). We explain the mathematical relationships between MCC and these indicators, then show some use cases and a bioinformatics scenario where these metrics disagree and where MCC generates a more informative response.Additionally, we describe three exceptions where BM can be more appropriate: analyzing classifications where dataset prevalence is unrepresentative, comparing classifiers on different datasets, and assessing the random guessing level of a classifier. Except in these cases, we believe that MCC is the most informative among the single metrics discussed, and suggest it as standard measure for scientists of all fields. A Matthews correlation coefficient close to +1, in fact, means having high values for all the other confusion matrix metrics. The same cannot be said for balanced accuracy, markedness, bookmaker informedness, accuracy and F1 score.},
  langid = {english},
  keywords = {Balanced accuracy,Binary classification,Bookmaker informedness,Confusion matrix,Machine learning,Markedness,Matthews correlation coefficient},
  file = {C\:\\Users\\YeCanming\\Documents\\WPS Cloud Files\\200995647\\Zotero文献库\\人工智能\\医学人工智能\\指标选择\\Chicco et al_2021_The Matthews correlation coefficient (MCC) is more reliable than balanced.pdf;D\:\\ProgramFiles\\Zotero\\storage\\4YDA59YF\\s13040-021-00244-z.html}
}

@article{chiccoMatthewsCorrelationCoefficient2023,
  title = {The {{Matthews}} Correlation Coefficient~({{MCC}}) Should Replace the {{ROC~AUC}} as the Standard Metric for Assessing Binary Classification},
  author = {Chicco, Davide and Jurman, Giuseppe},
  date = {2023-02-17},
  journaltitle = {BioData Mining},
  shortjournal = {BioData Mining},
  volume = {16},
  number = {1},
  pages = {4},
  issn = {1756-0381},
  doi = {10.1186/s13040-023-00322-4},
  url = {https://doi.org/10.1186/s13040-023-00322-4},
  urldate = {2023-08-25},
  abstract = {Binary classification is a common task for which machine learning and computational statistics are used, and the area under the receiver operating characteristic curve~(ROC~AUC) has become the common standard metric to evaluate binary classifications in most scientific fields. The ROC curve has true positive rate~(also called sensitivity or recall) on the y axis and false positive rate on the x axis, and the ROC~AUC can range from 0 (worst result) to 1~(perfect result). The ROC~AUC, however, has several flaws and drawbacks. This score is generated including predictions that obtained insufficient sensitivity and specificity, and moreover it does not say anything about positive predictive value~(also known as precision) nor negative predictive value~(NPV) obtained by the classifier, therefore potentially generating inflated overoptimistic results. Since it is common to include ROC~AUC alone without precision and negative predictive value, a researcher might erroneously conclude that their classification was successful. Furthermore, a given point in the ROC space does not identify a single confusion matrix nor a group of matrices sharing the same MCC value. Indeed, a given (sensitivity, specificity) pair can cover a broad MCC range, which casts doubts on the reliability of ROC~AUC as a performance measure. In contrast, the Matthews correlation coefficient~(MCC) generates a high score in its \$\$[-1; +1]\$\$interval only if the classifier scored a high value for all the four basic rates of the confusion matrix: sensitivity, specificity, precision, and negative predictive value. A high MCC (for example, MCC \$\$=\$\$0.9), moreover, always corresponds to a high ROC~AUC, and not vice versa. In this short study, we explain why the Matthews correlation coefficient should replace the ROC~AUC as standard statistic in all the scientific studies involving a binary classification, in all scientific fields.},
  langid = {english},
  keywords = {Area under the curve,AUC,Binary classification,Confusion matrix,Data mining,Data science,Matthews correlation coefficient,Receiver operating characteristic curve,ROC,ROC AUC,Supervised machine learning},
  file = {C\:\\Users\\YeCanming\\Documents\\WPS Cloud Files\\200995647\\Zotero文献库\\人工智能\\医学人工智能\\指标选择\\Chicco_Jurman_2023_The Matthews correlation coefficient (MCC) should replace the ROC AUC as the.pdf;D\:\\ProgramFiles\\Zotero\\storage\\KZQA8HWJ\\s13040-023-00322-4.html}
}

@article{collellSimplePluginBagging2018,
  title = {A Simple Plug-in Bagging Ensemble Based on Threshold-Moving for Classifying Binary and Multiclass Imbalanced Data},
  author = {Collell, Guillem and Prelec, Drazen and Patil, Kaustubh R.},
  date = {2018-01},
  journaltitle = {Neurocomputing},
  shortjournal = {Neurocomputing},
  volume = {275},
  pages = {330--340},
  issn = {09252312},
  doi = {10.1016/j.neucom.2017.08.035},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S092523121731456X},
  urldate = {2023-07-30},
  langid = {english},
  file = {C:\Users\YeCanming\Documents\WPS Cloud Files\200995647\Zotero文献库\人工智能\医学人工智能\Collell et al_2018_A simple plug-in bagging ensemble based on threshold-moving for classifying.pdf}
}

@article{costaGaussianMixtureModel2012,
  title = {Gaussian Mixture Model of Heart Rate Variability},
  author = {Costa, Tommaso and Boccignone, Giuseppe and Ferraro, Mario},
  date = {2012},
  journaltitle = {PloS One},
  shortjournal = {PLoS One},
  volume = {7},
  number = {5},
  eprint = {22666386},
  eprinttype = {pmid},
  pages = {e37731},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0037731},
  abstract = {Heart rate variability (HRV) is an important measure of sympathetic and parasympathetic functions of the autonomic nervous system and a key indicator of cardiovascular condition. This paper proposes a novel method to investigate HRV, namely by modelling it as a linear combination of Gaussians. Results show that three Gaussians are enough to describe the stationary statistics of heart variability and to provide a straightforward interpretation of the HRV power spectrum. Comparisons have been made also with synthetic data generated from different physiologically based models showing the plausibility of the Gaussian mixture parameters.},
  langid = {english},
  pmcid = {PMC3364278},
  keywords = {Adolescent,Adult,Autonomic Nervous System,Electrocardiography,Female,Heart Rate,Humans,Male,Models Neurological,Nonlinear Dynamics,Normal Distribution,Signal Processing Computer-Assisted,Young Adult},
  file = {D:\Zotero文献库\00人工智能\00机器学习课程\医学人工智能\类似文章\数据来源\类似体检数据\Costa et al_2012_Gaussian mixture model of heart rate variability.pdf}
}

@online{crawshawMultiTaskLearningDeep2020,
  title = {Multi-{{Task Learning}} with {{Deep Neural Networks}}: {{A Survey}}},
  shorttitle = {Multi-{{Task Learning}} with {{Deep Neural Networks}}},
  author = {Crawshaw, Michael},
  date = {2020-09-10},
  eprint = {2009.09796},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.2009.09796},
  url = {http://arxiv.org/abs/2009.09796},
  urldate = {2023-05-03},
  abstract = {Multi-task learning (MTL) is a subfield of machine learning in which multiple tasks are simultaneously learned by a shared model. Such approaches offer advantages like improved data efficiency, reduced overfitting through shared representations, and fast learning by leveraging auxiliary information. However, the simultaneous learning of multiple tasks presents new design and optimization challenges, and choosing which tasks should be learned jointly is in itself a non-trivial problem. In this survey, we give an overview of multi-task learning methods for deep neural networks, with the aim of summarizing both the well-established and most recent directions within the field. Our discussion is structured according to a partition of the existing deep MTL techniques into three groups: architectures, optimization methods, and task relationship learning. We also provide a summary of common multi-task benchmarks.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\YeCanming\\Documents\\WPS Cloud Files\\200995647\\Zotero文献库\\人工智能\\创新实践\\多任务学习\\多任务学习\\Crawshaw_2020_Multi-Task Learning with Deep Neural Networks.pdf;D\:\\ProgramFiles\\Zotero\\storage\\YJNNTTUQ\\2009.html}
}

@online{CS405Fall2019,
  title = {{{CS405-Fall2019}}},
  url = {http://hqlab.isus.tech/teaching/CS405/},
  urldate = {2022-10-31},
  file = {D:\ProgramFiles\Zotero\storage\DMGQMWTB\CS405.html}
}

@online{CS405OJProblem,
  title = {{{CS405 OJ}} | {{Problem List}}},
  url = {http://oj.isus.tech/problem},
  urldate = {2022-09-30},
  file = {D:\ProgramFiles\Zotero\storage\EYLAA524\problem.html}
}

@online{DaMoXingWeiDiaoFinetuneFangFaZongJieLoRA,
  title = {大模型微调（finetune）方法总结-{{LoRA}},{{Adapter}},{{Prefix-tuning}}，{{P-tuning}}，{{Prompt-tuning}}},
  url = {https://zhuanlan.zhihu.com/p/636481171},
  urldate = {2023-07-15},
  abstract = {1. LoRA paper：LoRA: Low-Rank Adaptation of Large Language Models（https://arxiv.org/pdf/2106.09685.pdf） code:GitHub - microsoft/LoRA: Code for loralib, an implementation of \&amp;quot;LoRA: Low-Ran…},
  langid = {english},
  organization = {知乎专栏},
  file = {D:\ProgramFiles\Zotero\storage\PA3NIPWP\636481171.html}
}

@online{DaMoXingXunLianPEFTYuLORAJieShaoChangHongYuDeBoKeCSDNBoKe,
  title = {大模型训练——PEFT与LORA介绍\_常鸿宇的博客-CSDN博客},
  url = {https://blog.csdn.net/weixin_44826203/article/details/129733930?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522168748627416800184115404%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&request_id=168748627416800184115404&biz_id=0&spm=1018.2226.3001.4187},
  urldate = {2023-06-23},
  langid = {chinese},
  file = {D:\ProgramFiles\Zotero\storage\33936CY6\129733930.html}
}

@online{DangWoMenZaiTanLunMultiTaskLearningDuoRenWu,
  title = {{{当我们在谈论Multi-Task Learning}}(多任务/多目标学习) - 知乎},
  url = {https://zhuanlan.zhihu.com/p/352428655},
  urldate = {2023-03-21},
  file = {D:\ProgramFiles\Zotero\storage\QZBH6ZJN\352428655.html}
}

@article{danjumaPerformanceEvaluationMachine2015,
  title = {Performance {{Evaluation}} of {{Machine Learning Algorithms}} in {{Post-operative Life Expectancy}} in the {{Lung Cancer Patients}}},
  author = {Danjuma, K.},
  date = {2015-04-17},
  journaltitle = {ArXiv},
  url = {https://www.semanticscholar.org/paper/Performance-Evaluation-of-Machine-Learning-in-Life-Danjuma/6925b3d1ef066063b5eaabea083f75d8e4eb02e4},
  urldate = {2023-07-29},
  abstract = {The nature of clinical data makes it difficult to quickly select, tune and apply machine learning algorithms to clinical prognosis. As a result, a lot of time is spent searching for the most appropriate machine learning algorithms applicable in clinical prognosis that contains either binary-valued or multi-valued attributes. The study set out to identify and evaluate the performance of machine learning classification schemes applied in clinical prognosis of post-operative life expectancy in the lung cancer patients. Multilayer Perceptron, J48, and the Naive Bayes algorithms were used to train and test models on Thoracic Surgery datasets obtained from the University of California Irvine machine learning repository. Stratified 10-fold cross-validation was used to evaluate baseline performance accuracy of the classifiers. The comparative analysis shows that multilayer perceptron performed best with classification accuracy of 82.3\%, J48 came out second with classification accuracy of 81.8\%, and Naive Bayes came out the worst with classification accuracy of 74.4\%. The quality and outcome of the chosen machine learning algorithms depends on the ingenuity of the clinical miner.},
  langid = {english},
  file = {C:\Users\YeCanming\Documents\WPS Cloud Files\200995647\Zotero文献库\人工智能\医学人工智能\UCI数据集论文\Danjuma_2015_Performance Evaluation of Machine Learning Algorithms in Post-operative Life.pdf}
}

@inproceedings{dengResearchFeatureOptimization2021,
  title = {Research on {{Feature Optimization Scheme Based}} on {{Data Feature Enhancement}}},
  booktitle = {2021 {{IEEE}} 21st {{International Conference}} on {{Software Quality}}, {{Reliability}} and {{Security Companion}} ({{QRS-C}})},
  author = {Deng, Zhi and Shi, Zhao and Wang, Zhenxin and Liu, Tao},
  date = {2021-12},
  pages = {270--278},
  issn = {2693-9371},
  doi = {10.1109/QRS-C55045.2021.00048},
  url = {https://ieeexplore.ieee.org/document/9742211},
  urldate = {2023-10-19},
  abstract = {Based on the common knowledge in the field of feature engineering that “data and features determine the upper limit of the model, and models and algorithms just approach the upper limit”, this article believes that the knowledge of feature engineering can be used to extract features from the original data to the maximum. Used by algorithms and models to improve the accuracy of algorithms and models. Based on the above considerations, this paper proposes a feature optimization scheme based on data feature enhancement (DFET) to improve the performance of deep learning models in natural language processing. The DFET feature optimization scheme uses machine learning classification algorithms to weaken noise features and adopts a sliding window Data enhancement is performed in the form of a window, the ignored features are selected through the form of windows, and low-dimensional data with significant features are generated. This part of the data is added to the original model training to fine-tune the model training, and then optimize the model to meet the requirements of improving accuracy. The paper examines the security bug report prediction problem in the field of natural language processing to verify the impact of data feature optimization on the performance optimization of deep learning models. Experiments show that compared with the basic deep learning model, the model after the DFET feature optimization scheme can show more excellent feature learning ability in text classification and prediction.},
  eventtitle = {2021 {{IEEE}} 21st {{International Conference}} on {{Software Quality}}, {{Reliability}} and {{Security Companion}} ({{QRS-C}})},
  langid = {english},
  file = {D:\ProgramFiles\Zotero\storage\I6LU5AJV\9742211.html}
}

@article{desideriMultiplegradientDescentAlgorithm2012,
  title = {Multiple-Gradient Descent Algorithm ({{MGDA}}) for Multiobjective Optimization},
  author = {Désidéri, Jean-Antoine},
  date = {2012-03},
  journaltitle = {Comptes Rendus Mathematique},
  shortjournal = {Comptes Rendus Mathematique},
  volume = {350},
  number = {5-6},
  pages = {313--318},
  issn = {1631073X},
  doi = {10.1016/j.crma.2012.03.014},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S1631073X12000738},
  urldate = {2023-04-27},
  abstract = {Semantic Scholar extracted view of "Multiple-gradient descent algorithm (MGDA) for multiobjective optimization" by J. Désidéri},
  langid = {english},
  file = {C\:\\Users\\YeCanming\\Documents\\WPS Cloud Files\\200995647\\Zotero文献库\\人工智能\\创新实践\\多任务学习\\Désidéri_2012_Multiple-gradient descent algorithm (MGDA) for multiobjective optimization.pdf;D\:\\ProgramFiles\\Zotero\\storage\\9P39UVLK\\Désidéri - 2012 - Multiple-gradient descent algorithm (MGDA) for mul.pdf}
}

@online{DiaoYanSoftPrompt,
  title = {【调研】{{Soft Prompt Tuning}} 模型发展调研：{{P-tuning}},{{Prefix-tuning}},{{Prompt-tuning}},{{P-tuning}} v2,{{PPT-CSDN博客}}},
  url = {https://blog.csdn.net/qq_39328436/article/details/122951888},
  urldate = {2024-05-29},
  langid = {english},
  keywords = {⛔ No INSPIRE recid found},
  file = {D:\ProgramFiles\Zotero\storage\5VDDG2FP\122951888.html}
}

@online{dingDeltaTuningComprehensive2022,
  title = {Delta {{Tuning}}: {{A Comprehensive Study}} of {{Parameter Efficient Methods}} for {{Pre-trained Language Models}}},
  shorttitle = {Delta {{Tuning}}},
  author = {Ding, Ning and Qin, Yujia and Yang, Guang and Wei, Fuchao and Yang, Zonghan and Su, Yusheng and Hu, Shengding and Chen, Yulin and Chan, Chi-Min and Chen, Weize and Yi, Jing and Zhao, Weilin and Wang, Xiaozhi and Liu, Zhiyuan and Zheng, Hai-Tao and Chen, Jianfei and Liu, Yang and Tang, Jie and Li, Juanzi and Sun, Maosong},
  date = {2022-03-14},
  eprint = {2203.06904},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2203.06904},
  url = {http://arxiv.org/abs/2203.06904},
  urldate = {2023-08-11},
  abstract = {Despite the success, the process of fine-tuning large-scale PLMs brings prohibitive adaptation costs. In fact, fine-tuning all the parameters of a colossal model and retaining separate instances for different tasks are practically infeasible. This necessitates a new branch of research focusing on the parameter-efficient adaptation of PLMs, dubbed as delta tuning in this paper. In contrast with the standard fine-tuning, delta tuning only fine-tunes a small portion of the model parameters while keeping the rest untouched, largely reducing both the computation and storage costs. Recent studies have demonstrated that a series of delta tuning methods with distinct tuned parameter selection could achieve performance on a par with full-parameter fine-tuning, suggesting a new promising way of stimulating large-scale PLMs. In this paper, we first formally describe the problem of delta tuning and then comprehensively review recent delta tuning approaches. We also propose a unified categorization criterion that divide existing delta tuning methods into three groups: addition-based, specification-based, and reparameterization-based methods. Though initially proposed as an efficient method to steer large models, we believe that some of the fascinating evidence discovered along with delta tuning could help further reveal the mechanisms of PLMs and even deep neural networks. To this end, we discuss the theoretical principles underlying the effectiveness of delta tuning and propose frameworks to interpret delta tuning from the perspective of optimization and optimal control, respectively. Furthermore, we provide a holistic empirical study of representative methods, where results on over 100 NLP tasks demonstrate a comprehensive performance comparison of different approaches. The experimental results also cover the analysis of combinatorial, scaling and transferable properties of delta tuning.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {⛔ No INSPIRE recid found,Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {C\:\\Users\\YeCanming\\Documents\\WPS Cloud Files\\200995647\\Zotero文献库\\人工智能\\创新实践\\prompt\\Ding et al_2022_Delta Tuning.pdf;D\:\\ProgramFiles\\Zotero\\storage\\N3XWTMKA\\2203.html}
}

@online{DoesCifar100Dataset2017,
  title = {Does the Cifar100 Dataset of Pytorch Provide Coarse Labels?},
  date = {2017-12-06T16:55:25+00:00},
  url = {https://discuss.pytorch.org/t/does-the-cifar100-dataset-of-pytorch-provide-coarse-labels/10858},
  urldate = {2023-07-16},
  abstract = {A quick question here about cifar100  I am wondering if the cifar100 dataset of pytorch provide a way to get the coarse labels or if there is 3rd implemented codes to realize this.  I have refered to the manual of torchvision.datasets.CIFAR100, there seems to be no description about coarse labels.  Thanks in advance  ----------------update-------------------  No such option in torchvision.datasets.CIFAR100  Solved by adding an option parameter to the code of https://github.com/pytorch/vision/blo...},
  langid = {english},
  organization = {PyTorch Forums},
  file = {D:\ProgramFiles\Zotero\storage\VAKR8DLK\10858.html}
}

@online{dongEfficientAdaptationLarge2024,
  title = {Efficient {{Adaptation}} of {{Large Vision Transformer}} via {{Adapter Re-Composing}}},
  author = {Dong, Wei and Yan, Dawei and Lin, Zhijun and Wang, Peng},
  date = {2024-01-16},
  eprint = {2310.06234},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2310.06234},
  url = {http://arxiv.org/abs/2310.06234},
  urldate = {2024-04-15},
  abstract = {The advent of high-capacity pre-trained models has revolutionized problem-solving in computer vision, shifting the focus from training task-specific models to adapting pre-trained models. Consequently, effectively adapting large pre-trained models to downstream tasks in an efficient manner has become a prominent research area. Existing solutions primarily concentrate on designing lightweight adapters and their interaction with pre-trained models, with the goal of minimizing the number of parameters requiring updates. In this study, we propose a novel Adapter Re-Composing (ARC) strategy that addresses efficient pre-trained model adaptation from a fresh perspective. Our approach considers the reusability of adaptation parameters and introduces a parameter-sharing scheme. Specifically, we leverage symmetric down-/up-projections to construct bottleneck operations, which are shared across layers. By learning low-dimensional re-scaling coefficients, we can effectively re-compose layer-adaptive adapters. This parameter-sharing strategy in adapter design allows us to significantly reduce the number of new parameters while maintaining satisfactory performance, thereby offering a promising approach to compress the adaptation cost. We conduct experiments on 24 downstream image classification tasks using various Vision Transformer variants to evaluate our method. The results demonstrate that our approach achieves compelling transfer learning performance with a reduced parameter count. Our code is available at \textbackslash href\{https://github.com/DavidYanAnDe/ARC\}\{https://github.com/DavidYanAnDe/ARC\}.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {⛔ No INSPIRE recid found,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {D\:\\Zotero文献库\\00人工智能\\00机器学习\\学习范式\\迁移学习\\PEFT\\residual\\Dong et al_2024_Efficient Adaptation of Large Vision Transformer via Adapter Re-Composing.pdf;D\:\\ProgramFiles\\Zotero\\storage\\WUKU2ILY\\2310.html}
}

@online{dongLPTLongtailedPrompt2023,
  title = {{{LPT}}: {{Long-tailed Prompt Tuning}} for {{Image Classification}}},
  shorttitle = {{{LPT}}},
  author = {Dong, Bowen and Zhou, Pan and Yan, Shuicheng and Zuo, Wangmeng},
  date = {2023-03-28},
  eprint = {2210.01033},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2210.01033},
  url = {http://arxiv.org/abs/2210.01033},
  urldate = {2023-04-27},
  abstract = {For long-tailed classification, most works often pretrain a big model on a large-scale dataset, and then fine-tune the whole model for adapting to long-tailed data. Though promising, fine-tuning the whole pretrained model tends to suffer from high cost in computation and deployment of different models for different tasks, as well as weakened generalization ability for overfitting to certain features of long-tailed data. To alleviate these issues, we propose an effective Long-tailed Prompt Tuning method for long-tailed classification. LPT introduces several trainable prompts into a frozen pretrained model to adapt it to long-tailed data. For better effectiveness, we divide prompts into two groups: 1) a shared prompt for the whole long-tailed dataset to learn general features and to adapt a pretrained model into target domain; and 2) group-specific prompts to gather group-specific features for the samples which have similar features and also to empower the pretrained model with discrimination ability. Then we design a two-phase training paradigm to learn these prompts. In phase 1, we train the shared prompt via supervised prompt tuning to adapt a pretrained model to the desired long-tailed domain. In phase 2, we use the learnt shared prompt as query to select a small best matched set for a group of similar samples from the group-specific prompt set to dig the common features of these similar samples, then optimize these prompts with dual sampling strategy and asymmetric GCL loss. By only fine-tuning a few prompts while fixing the pretrained model, LPT can reduce training and deployment cost by storing a few prompts, and enjoys a strong generalization ability of the pretrained model. Experiments show that on various long-tailed benchmarks, with only \textasciitilde 1.1\% extra parameters, LPT achieves comparable performance than previous whole model fine-tuning methods, and is more robust to domain-shift.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\YeCanming\\Documents\\WPS Cloud Files\\200995647\\Zotero文献库\\人工智能\\计算机视觉课程\\大模型提示微调用于长尾学习\\Dong et al_2023_LPT.pdf;D\:\\ProgramFiles\\Zotero\\storage\\GZXM53II\\2210.html}
}

@article{dreiseitlPhysiciansValueDecision2005,
  title = {Do Physicians Value Decision Support? {{A}} Look at the Effect of Decision Support Systems on Physician Opinion},
  shorttitle = {Do Physicians Value Decision Support?},
  author = {Dreiseitl, Stephan and Binder, Michael},
  date = {2005-01},
  journaltitle = {Artificial Intelligence in Medicine},
  shortjournal = {Artificial Intelligence in Medicine},
  volume = {33},
  number = {1},
  pages = {25--30},
  issn = {09333657},
  doi = {10.1016/j.artmed.2004.07.007},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0933365704001071},
  urldate = {2023-07-30},
  abstract = {Objective: Clinical decision support systems are on the verge of becoming routine software tools in clinical settings. We investigate the question of how physicians react when faced with decision support suggestions that contradict their own diagnoses. Methodology: We used a study design involving 52 volunteer dermatologists who each rated the malignancy of 25 lesion images on an ordinal scale and gave a dichotomous excise/no excise recommendation for each lesion image. After seeing the system’s rating and excise suggestions, the physicians could revise their initial recommendations. Results: We observed that in 24\% of the cases in which the physicians’ diagnoses did not match those of the decision support system, the physicians changed their diagnoses. There was a slight but significant negative correlation between susceptibility to change and experience level of the physicians. Physicians were significantly less likely to follow the decision system’s recommendations when they were confident of their initial diagnoses. No differences between the physicians’ inclinations to following excise versus no excise recommendations could be observed. Conclusion: These results indicate that physicians are quite susceptible to accepting the recommendations of decision support systems, and that quality assurance and validation of such systems is therefore of paramount importance.},
  langid = {english},
  file = {D:\ProgramFiles\Zotero\storage\26GDIZ7S\Dreiseitl 和 Binder - 2005 - Do physicians value decision support A look at th.pdf}
}

@online{ebanScalableLearningNonDecomposable2017,
  title = {Scalable {{Learning}} of {{Non-Decomposable Objectives}}},
  author = {Eban, Elad ET and Schain, Mariano and Mackey, Alan and Gordon, Ariel and Saurous, Rif A. and Elidan, Gal},
  date = {2017-03-01},
  eprint = {1608.04802},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.1608.04802},
  url = {http://arxiv.org/abs/1608.04802},
  urldate = {2022-12-15},
  abstract = {Modern retrieval systems are often driven by an underlying machine learning model. The goal of such systems is to identify and possibly rank the few most relevant items for a given query or context. Thus, such systems are typically evaluated using a ranking-based performance metric such as the area under the precision-recall curve, the \$F\_\textbackslash beta\$ score, precision at fixed recall, etc. Obviously, it is desirable to train such systems to optimize the metric of interest. In practice, due to the scalability limitations of existing approaches for optimizing such objectives, large-scale retrieval systems are instead trained to maximize classification accuracy, in the hope that performance as measured via the true objective will also be favorable. In this work we present a unified framework that, using straightforward building block bounds, allows for highly scalable optimization of a wide range of ranking-based objectives. We demonstrate the advantage of our approach on several real-life retrieval problems that are significantly larger than those considered in the literature, while achieving substantial improvement in performance over the accuracy-objective baseline.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\YeCanming\\Documents\\WPS Cloud Files\\200995647\\Zotero文献库\\人工智能\\创新实践\\损失函数\\Eban et al_2017_Scalable Learning of Non-Decomposable Objectives.pdf;D\:\\ProgramFiles\\Zotero\\storage\\QT36CIQ9\\1608.html}
}

@online{EdumunozsalaVit_base224in21kftcifar100Hugging2023,
  title = {Edumunozsala/Vit\_base-224-In21k-Ft-Cifar100 · {{Hugging Face}}},
  date = {2023-01-25},
  url = {https://huggingface.co/edumunozsala/vit_base-224-in21k-ft-cifar100},
  urldate = {2023-08-07},
  abstract = {We’re on a journey to advance and democratize artificial intelligence through open source and open science.},
  langid = {english},
  file = {D:\ProgramFiles\Zotero\storage\JUHVNPNI\vit_base-224-in21k-ft-cifar100.html}
}

@online{ericksonAutoGluonTabularRobustAccurate2020,
  title = {{{AutoGluon-Tabular}}: {{Robust}} and {{Accurate AutoML}} for {{Structured Data}}},
  shorttitle = {{{AutoGluon-Tabular}}},
  author = {Erickson, Nick and Mueller, Jonas and Shirkov, Alexander and Zhang, Hang and Larroy, Pedro and Li, Mu and Smola, Alexander},
  date = {2020-03-13},
  eprint = {2003.06505},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/2003.06505},
  urldate = {2023-07-30},
  abstract = {We introduce AutoGluon-Tabular, an opensource2 AutoML framework that requires only a single line of Python to train highly accurate machine learning models on an unprocessed tabular dataset such as a CSV file. Unlike existing AutoML frameworks that primarily focus on model/hyperparameter selection, AutoGluonTabular succeeds by ensembling multiple models and stacking them in multiple layers. Experiments reveal that our multi-layer combination of many models offers better use of allocated training time than seeking out the best.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C:\Users\YeCanming\Documents\WPS Cloud Files\200995647\Zotero文献库\人工智能\医学人工智能\Erickson et al_2020_AutoGluon-Tabular.pdf}
}

@online{ERROROptimizerGot2017,
  title = {{{ERROR}}:Optimizer Got an Empty Parameter List},
  shorttitle = {{{ERROR}}},
  date = {2017-03-31T03:50:24+00:00},
  url = {https://discuss.pytorch.org/t/error-optimizer-got-an-empty-parameter-list/1501},
  urldate = {2022-10-31},
  abstract = {I am trying to build network with a new structure,The model is built with two separated class.When I run it,the error says that :optimizer got an empty parameter list.I am new to PyTorch and I don’t know what causes the error.Can you give me some suggestions?Thank you!},
  langid = {english},
  organization = {PyTorch Forums},
  file = {D:\ProgramFiles\Zotero\storage\EE44V4CC\2.html}
}

@inproceedings{fakoorFastAccurateSimple2020,
  title = {Fast, {{Accurate}}, and {{Simple Models}} for {{Tabular Data}} via {{Augmented Distillation}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Fakoor, Rasool and Mueller, Jonas W and Erickson, Nick and Chaudhari, Pratik and Smola, Alexander J},
  date = {2020},
  volume = {33},
  pages = {8671--8681},
  publisher = {Curran Associates, Inc.},
  url = {https://proceedings.neurips.cc/paper/2020/hash/62d75fb2e3075506e8837d8f55021ab1-Abstract.html},
  urldate = {2024-09-15},
  abstract = {Automated machine learning (AutoML) can produce complex model ensembles by stacking, bagging, and boosting many individual models like trees, deep networks, and nearest neighbor estimators. While highly accurate, the resulting predictors are large, slow, and opaque as compared to their constituents. To improve the deployment of AutoML on tabular data, we propose FAST-DAD to distill arbitrarily-complex ensemble predictors into individual models like boosted trees, random forests, and deep networks. At the heart of our approach is a data augmentation strategy based on Gibbs sampling from a self-attention pseudolikelihood estimator. Across 30 datasets spanning regression and binary/multiclass classification tasks, FAST-DAD distillation produces significantly better individual models than one obtains through standard training on the original data. Our individual distilled models are over 10x faster and more accurate than ensemble predictors produced by AutoML tools like H2O/AutoSklearn.},
  langid = {english},
  keywords = {⛔ No INSPIRE recid found},
  file = {D:\Zotero文献库\00人工智能\00机器学习\医学人工智能\AI知识查缺补漏\automl\Fakoor et al_2020_Fast, Accurate, and Simple Models for Tabular Data via Augmented Distillation.pdf}
}

@article{fayekProgressiveLearningDeep2020,
  title = {Progressive Learning: {{A}} Deep Learning Framework for Continual Learning},
  shorttitle = {Progressive Learning},
  author = {Fayek, Haytham M. and Cavedon, Lawrence and Wu, Hong Ren},
  date = {2020-08-01},
  journaltitle = {Neural Networks},
  shortjournal = {Neural Networks},
  volume = {128},
  pages = {345--357},
  issn = {0893-6080},
  doi = {10.1016/j.neunet.2020.05.011},
  url = {https://www.sciencedirect.com/science/article/pii/S0893608020301817},
  urldate = {2023-06-28},
  abstract = {Continual learning is the ability of a learning system to solve new tasks by utilizing previously acquired knowledge from learning and performing prior tasks without having significant adverse effects on the acquired prior knowledge. Continual learning is key to advancing machine learning and artificial intelligence. Progressive learning is a deep learning framework for continual learning that comprises three procedures: curriculum, progression, and pruning. The curriculum procedure is used to actively select a task to learn from a set of candidate tasks. The progression procedure is used to grow the capacity of the model by adding new parameters that leverage parameters learned in prior tasks, while learning from data available for the new task at hand, without being susceptible to catastrophic forgetting. The pruning procedure is used to counteract the growth in the number of parameters as further tasks are learned, as well as to mitigate negative forward transfer, in which prior knowledge unrelated to the task at hand may interfere and worsen performance. Progressive learning is evaluated on a number of supervised classification tasks in the image recognition and speech recognition domains to demonstrate its advantages compared with baseline methods. It is shown that, when tasks are related, progressive learning leads to faster learning that converges to better generalization performance using a smaller number of dedicated parameters.},
  langid = {english},
  keywords = {Computer vision,Continual learning,Deep learning,Machine learning,Neural networks,Speech recognition},
  file = {C\:\\Users\\YeCanming\\Documents\\WPS Cloud Files\\200995647\\Zotero文献库\\人工智能\\创新实践\\prompt\\Fayek et al_2020_Progressive learning.pdf;D\:\\ProgramFiles\\Zotero\\storage\\32WY5TKR\\S0893608020301817.html}
}

@online{Feirootyonghuanzhuangcudayucudnn,
  title = {非root用户安装cuda与cudnn},
  url = {https://zhuanlan.zhihu.com/p/198161777},
  urldate = {2022-11-15},
  abstract = {很多小伙伴都和我抱怨（其实我自己也是）服务器上管理员已安装好显卡驱动或已安装的CUDA版本无法满足自己要求（要么太高要么太低），与自己需要的TensorFlow或者Pytorch版本不兼容，急的头皮发麻！！！今天熬夜写…},
  langid = {chinese},
  organization = {知乎专栏}
}

@article{fernandoNecrotizingSoftTissue2019,
  title = {Necrotizing {{Soft Tissue Infection}}: {{Diagnostic Accuracy}} of {{Physical Examination}}, {{Imaging}}, and {{LRINEC Score}}: {{A Systematic Review}} and {{Meta-Analysis}}},
  shorttitle = {Necrotizing {{Soft Tissue Infection}}},
  author = {Fernando, Shannon M. and Tran, Alexandre and Cheng, Wei and Rochwerg, Bram and Kyeremanteng, Kwadwo and Seely, Andrew J. E. and Inaba, Kenji and Perry, Jeffrey J.},
  date = {2019-01},
  journaltitle = {Annals of Surgery},
  volume = {269},
  number = {1},
  pages = {58--65},
  issn = {0003-4932, 1528-1140},
  doi = {10.1097/SLA.0000000000002774},
  url = {https://journals.lww.com/00000658-201901000-00015},
  urldate = {2023-08-16},
  abstract = {Conclusions: Absence of any 1 physical examination feature (eg, fever or hypotension) is not sufficient to rule-out NSTI. CT is superior to plain radiography. LRINEC had poor sensitivity, and should not be used to ruleout NSTI. Given the poor sensitivity of these tests, a high clinical suspicion warrants early surgical consultation for definitive diagnosis and management.},
  langid = {english},
  file = {D:\ProgramFiles\Zotero\storage\D56TH7H6\Fernando 等 - 2019 - Necrotizing Soft Tissue Infection Diagnostic Accu.pdf}
}

@inproceedings{ferreiraComparisonAutoMLTools2021,
  title = {A {{Comparison}} of {{AutoML Tools}} for {{Machine Learning}}, {{Deep Learning}} and {{XGBoost}}},
  booktitle = {2021 {{International Joint Conference}} on {{Neural Networks}} ({{IJCNN}})},
  author = {Ferreira, Luís and Pilastri, André and Martins, Carlos Manuel and Pires, Pedro Miguel and Cortez, Paulo},
  date = {2021-07},
  pages = {1--8},
  issn = {2161-4407},
  doi = {10.1109/IJCNN52387.2021.9534091},
  abstract = {This paper presents a benchmark of supervised Automated Machine Learning (AutoML) tools. Firstly, we analyze the characteristics of eight recent open-source AutoML tools (Auto-Keras, Auto-PyTorch, Auto-Sklearn, AutoGluon, H2O AutoML, rminer, TPOT and TransmogrifAI) and describe twelve popular OpenML datasets that were used in the benchmark (divided into regression, binary and multi-class classification tasks). Then, we perform a comparison study with hundreds of computational experiments based on three scenarios: General Machine Learning (GML), Deep Learning (DL) and XGBoost (XGB). To select the best tool, we used a lexicographic approach, considering first the average prediction score for each task and then the computational effort. The best predictive results were achieved for GML, which were further compared with the best OpenML public results. Overall, the best GML AutoML tools obtained competitive results, outperforming the best OpenML models in five datasets. These results confirm the potential of the general-purpose AutoML tools to fully automate the Machine Learning (ML) algorithm selection and tuning.},
  eventtitle = {2021 {{International Joint Conference}} on {{Neural Networks}} ({{IJCNN}})},
  langid = {english},
  keywords = {Automated Deep Learning (AutoDL),Automated Machine Learning (AutoML),Benchmark testing,Benchmarking,Classification,Computer architecture,Deep learning,Machine learning algorithms,Neural Architecture Search (NAS),Predictive models,Regression,Software,Supervised Learning,Tools,Water},
  file = {C\:\\Users\\YeCanming\\Documents\\WPS Cloud Files\\200995647\\Zotero文献库\\人工智能\\医学人工智能\\automl\\benchmark\\Ferreira et al_2021_A Comparison of AutoML Tools for Machine Learning, Deep Learning and XGBoost.pdf;D\:\\ProgramFiles\\Zotero\\storage\\Y4EAV7CY\\9534091.html}
}

@online{FetchOpenmlDoes,
  title = {Fetch\_openml Does Not Cache the Data · {{Issue}} \#18783 · Scikit-Learn/Scikit-Learn},
  url = {https://github.com/scikit-learn/scikit-learn/issues/18783},
  urldate = {2022-10-26},
  abstract = {It seems that fetch\_openml is not caching the data. If I run: from sklearn.datasets import fetch\_openml import time time0 = time.time() cifar10 = fetch\_openml(\&\#39;CIFAR\_10\&\#39;, cache=True) print(...},
  langid = {english},
  organization = {GitHub},
  file = {D:\ProgramFiles\Zotero\storage\SZML4EKG\18783.html}
}

@online{gaoDeepHierarchicalClassification2020,
  title = {Deep {{Hierarchical Classification}} for {{Category Prediction}} in {{E-commerce System}}},
  author = {Gao, Dehong and Yang, Wenjing and Zhou, Huiling and Wei, Yi and Hu, Yi and Wang, Hao},
  date = {2020-05-13},
  eprint = {2005.06692},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2005.06692},
  url = {http://arxiv.org/abs/2005.06692},
  urldate = {2024-04-05},
  abstract = {In e-commerce system, category prediction is to automatically predict categories of given texts. Different from traditional classification where there are no relations between classes, category prediction is reckoned as a standard hierarchical classification problem since categories are usually organized as a hierarchical tree. In this paper, we address hierarchical category prediction. We propose a Deep Hierarchical Classification framework, which incorporates the multi-scale hierarchical information in neural networks and introduces a representation sharing strategy according to the category tree. We also define a novel combined loss function to punish hierarchical prediction losses. The evaluation shows that the proposed approach outperforms existing approaches in accuracy.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {⛔ No INSPIRE recid found,Computer Science - Information Retrieval,Computer Science - Machine Learning},
  file = {D:\ProgramFiles\Zotero\storage\EM7PBRHV\2005.html}
}

@online{GaoSiPoSuBeiYeSiGaussianNaive,
  title = {高斯朴素贝叶斯（Gaussian Naive Bayes）原理与实现——垃圾邮件识别实战 - ISGuXing - 博客园},
  url = {https://www.cnblogs.com/ISGuXing/p/13777895.html},
  urldate = {2022-09-29},
  abstract = {朴素贝叶斯（Naive Bayes）： 根据贝叶斯定理和朴素假设提出的朴素贝叶斯模型。 贝叶斯定理： 朴素假设（特征条件独立性假设）： 代入可知朴素贝叶斯模型计算公式： 因为朴素贝叶斯是用来分类任务，},
  langid = {chinese},
  file = {D:\ProgramFiles\Zotero\storage\4P8A3ZY4\13777895.html}
}

@article{garcialeivaNovelHyperparameterFreeApproach2019,
  title = {A {{Novel Hyperparameter-Free Approach}} to {{Decision Tree Construction That Avoids Overfitting}} by {{Design}}},
  author = {Garcia Leiva, Rafael and Fernandez Anta, Antonio and Mancuso, Vincenzo and Casari, Paolo},
  date = {2019},
  journaltitle = {IEEE Access},
  shortjournal = {IEEE Access},
  volume = {7},
  pages = {99978--99987},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2019.2930235},
  url = {https://ieeexplore.ieee.org/document/8767915/},
  urldate = {2023-07-29},
  abstract = {Decision trees are an extremely popular machine learning technique. Unfortunately, overfitting in decision trees still remains an open issue that sometimes prevents achieving good performance. In this paper, we present a novel approach for the construction of decision trees that avoids the overfitting by design, without losing accuracy. A distinctive feature of our algorithm is that it requires neither the optimization of any hyperparameters, nor the use of regularization techniques, thus significantly reducing the decision tree training time. Moreover, our algorithm produces much smaller and shallower trees than traditional algorithms, facilitating the interpretability of the resulting models. For reproducibility, we provide an open source version of the algorithm.},
  langid = {english},
  file = {C:\Users\YeCanming\Documents\WPS Cloud Files\200995647\Zotero文献库\人工智能\医学人工智能\UCI数据集论文\Garcia Leiva et al_2019_A Novel Hyperparameter-Free Approach to Decision Tree Construction That Avoids.pdf}
}

@online{gengRecommendationLanguageProcessing2023,
  title = {Recommendation as {{Language Processing}} ({{RLP}}): {{A Unified Pretrain}}, {{Personalized Prompt}} \& {{Predict Paradigm}} ({{P5}})},
  shorttitle = {Recommendation as {{Language Processing}} ({{RLP}})},
  author = {Geng, Shijie and Liu, Shuchang and Fu, Zuohui and Ge, Yingqiang and Zhang, Yongfeng},
  date = {2023-01-02},
  eprint = {2203.13366},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2203.13366},
  urldate = {2023-06-19},
  abstract = {For a long time, different recommendation tasks typically require designing task-specific architectures and training objectives. As a result, it is hard to transfer the learned knowledge and representations from one task to another, thus restricting the generalization ability of existing recommendation approaches, e.g., a sequential recommendation model can hardly be applied or transferred to a review generation method. To deal with such issues, considering that language can describe almost anything and language grounding is a powerful medium to represent various problems or tasks, we present a flexible and unified text-to-text paradigm called "Pretrain, Personalized Prompt, and Predict Paradigm" (P5) for recommendation, which unifies various recommendation tasks in a shared framework. In P5, all data such as user-item interactions, user descriptions, item metadata, and user reviews are converted to a common format -- natural language sequences. The rich information from natural language assists P5 to capture deeper semantics for personalization and recommendation. Specifically, P5 learns different tasks with the same language modeling objective during pretraining. Thus, it serves as the foundation model for various downstream recommendation tasks, allows easy integration with other modalities, and enables instruction-based recommendation based on prompts. P5 advances recommender systems from shallow model to deep model to big model, and will revolutionize the technical form of recommender systems towards universal recommendation engine. With adaptive personalized prompt for different users, P5 is able to make predictions in a zero-shot or few-shot manner and largely reduces the necessity for extensive fine-tuning. On several recommendation benchmarks, we conduct experiments to show the effectiveness of P5. We release the source code at https://github.com/jeykigung/P5.},
  langid = {english},
  pubstate = {prepublished},
  version = {7},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Information Retrieval,Computer Science - Machine Learning},
  file = {C\:\\Users\\YeCanming\\Documents\\WPS Cloud Files\\200995647\\Zotero文献库\\人工智能\\医学人工智能\\Geng et al_2023_Recommendation as Language Processing (RLP).pdf;D\:\\ProgramFiles\\Zotero\\storage\\S7JGHQCD\\2203.html}
}

@online{gesmundoContinualDevelopmentMethodology2022,
  title = {A {{Continual Development Methodology}} for {{Large-scale Multitask Dynamic ML Systems}}},
  author = {Gesmundo, Andrea},
  date = {2022-11-06},
  eprint = {2209.07326},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2209.07326},
  urldate = {2023-06-28},
  abstract = {The traditional Machine Learning (ML) methodology requires to fragment the development and experimental process into disconnected iterations whose feedback is used to guide design or tuning choices. This methodology has multiple efficiency and scalability disadvantages, such as leading to spend significant resources into the creation of multiple trial models that do not contribute to the final solution.The presented work is based on the intuition that defining ML models as modular and extensible artefacts allows to introduce a novel ML development methodology enabling the integration of multiple design and evaluation iterations into the continuous enrichment of a single unbounded intelligent system. We define a novel method for the generation of dynamic multitask ML models as a sequence of extensions and generalizations. We first analyze the capabilities of the proposed method by using the standard ML empirical evaluation methodology. Finally, we propose a novel continuous development methodology that allows to dynamically extend a pre-existing multitask large-scale ML system while analyzing the properties of the proposed method extensions. This results in the generation of an ML model capable of jointly solving 124 image classification tasks achieving state of the art quality with improved size and compute cost.},
  langid = {english},
  pubstate = {prepublished},
  version = {3},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {C\:\\Users\\YeCanming\\Documents\\WPS Cloud Files\\200995647\\Zotero文献库\\人工智能\\创新实践\\prompt\\Gesmundo_2022_A Continual Development Methodology for Large-scale Multitask Dynamic ML Systems.pdf;D\:\\ProgramFiles\\Zotero\\storage\\C4XNAKWV\\2209.html}
}

@online{GitJieJueFilenameTooLongDeWenTi,
  title = {{{Git解决Filename}} Too long的问题\_{{EdmondYoung的博客-CSDN博客}}},
  url = {https://blog.csdn.net/yyd19921214/article/details/90479804},
  urldate = {2022-11-14}
}

@article{giunchigliaCoherentHierarchicalMultiLabel,
  title = {Coherent {{Hierarchical Multi-Label Classiﬁcation Networks}}},
  author = {Giunchiglia, Eleonora and Lukasiewicz, Thomas},
  abstract = {Hierarchical multi-label classification (HMC) is a challenging classification task extending standard multi-label classification problems by imposing a hierarchy constraint on the classes. In this paper, we propose C-HMCNN(h), a novel approach for HMC problems, which, given a network h for the underlying multi-label classification problem, exploits the hierarchy information in order to produce predictions coherent with the constraint and improve performance. We conduct an extensive experimental analysis showing the superior performance of C-HMCNN(h) when compared to state-of-the-art models.},
  langid = {english},
  keywords = {⛔ No INSPIRE recid found},
  file = {D:\ProgramFiles\Zotero\storage\DD8C5HD7\Giunchiglia 和 Lukasiewicz - Coherent Hierarchical Multi-Label Classiﬁcation Ne.pdf}
}

@online{goyalAccurateLargeMinibatch2018,
  title = {Accurate, {{Large Minibatch SGD}}: {{Training ImageNet}} in 1 {{Hour}}},
  shorttitle = {Accurate, {{Large Minibatch SGD}}},
  author = {Goyal, Priya and Dollár, Piotr and Girshick, Ross and Noordhuis, Pieter and Wesolowski, Lukasz and Kyrola, Aapo and Tulloch, Andrew and Jia, Yangqing and He, Kaiming},
  date = {2018-04-30},
  eprint = {1706.02677},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1706.02677},
  url = {http://arxiv.org/abs/1706.02677},
  urldate = {2024-05-12},
  abstract = {Deep learning thrives with large neural networks and large datasets. However, larger networks and larger datasets result in longer training times that impede research and development progress. Distributed synchronous SGD offers a potential solution to this problem by dividing SGD minibatches over a pool of parallel workers. Yet to make this scheme efficient, the per-worker workload must be large, which implies nontrivial growth in the SGD minibatch size. In this paper, we empirically show that on the ImageNet dataset large minibatches cause optimization difficulties, but when these are addressed the trained networks exhibit good generalization. Specifically, we show no loss of accuracy when training with large minibatch sizes up to 8192 images. To achieve this result, we adopt a hyper-parameter-free linear scaling rule for adjusting learning rates as a function of minibatch size and develop a new warmup scheme that overcomes optimization challenges early in training. With these simple techniques, our Caffe2-based system trains ResNet-50 with a minibatch size of 8192 on 256 GPUs in one hour, while matching small minibatch accuracy. Using commodity hardware, our implementation achieves \textasciitilde 90\% scaling efficiency when moving from 8 to 256 GPUs. Our findings enable training visual recognition models on internet-scale data with high efficiency.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {⛔ No INSPIRE recid found,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Distributed Parallel and Cluster Computing,Computer Science - Machine Learning},
  file = {D\:\\Zotero文献库\\00人工智能\\00计算机视觉\\先进思想的论文\\理解训练过程\\Goyal et al_2018_Accurate, Large Minibatch SGD.pdf;D\:\\ProgramFiles\\Zotero\\storage\\QEYVN436\\1706.html}
}

@inproceedings{gunasingheEarlyPredictionLung2019,
  title = {Early {{Prediction}} of {{Lung Diseases}}},
  booktitle = {2019 {{IEEE}} 5th {{International Conference}} for {{Convergence}} in {{Technology}} ({{I2CT}})},
  author = {Gunasinghe, Anuradha D. and Aponso, Achala C. and Thirimanna, Harsha},
  date = {2019-03},
  pages = {1--4},
  doi = {10.1109/I2CT45611.2019.9033668},
  abstract = {Machine learning is a branch of artificial intelligence that employs a variety of statistical, probabilistic and optimization techniques that allows computers to "learn" from past examples and to detect hard-to-discern patterns from large, noisy or complex data sets. Machine learning offers a principle approach for developing sophisticated, automatic, and objective algorithms for analysis of high-dimensional and multimodal biomedical data. Machine Learning plays an important role in medical systems. Earlier identification of diseases, we can be helped to detect earlier and more accurately, which can save many people as well as reduce the pressure on the system. Lung diseases are the one of the leading cause of death. The early identification and prediction of a lung diseases have become a necessity in the research, as it can facilitate the subsequent clinical management of patients. Machine Learning based decision support system provide the contribution to the doctors in their diagnosis decisions. Project considered about the breathing problems of patients as well as Asthma, Chronic Obstructive Pulmonary Disease (COPD), Tuberculosis, Pneumothorax and Lung cancer. Machine Learning and Deep Learning used to process data as well as create models for diagnosing patients. Combining the processing of patient information with data from chest X-rays, using CNN with the well-known pre-trained model, Caps Net network for data this form are the methods used for this project to identify the lung diseases. Initially studied and analyzed the data set, then apply Machine Learning and Deep Learning to predict that the patient has a lung disease or not. Project is a binary classification with input is patient's data (age, gender, chest X-ray images \& view position) and output is found what the diseases is or not. The aim of the paper is to detect and diagnose the lung diseases as early as possible which will help the doctor to save the patient's life. This paper describes how lung diseases was predicted and controlled, using Machine Learning.},
  eventtitle = {2019 {{IEEE}} 5th {{International Conference}} for {{Convergence}} in {{Technology}} ({{I2CT}})},
  langid = {english},
  keywords = {Computer architecture,Deep Learning,Diseases,Feature extraction,Lung,Lung Diseases,Machine learning,Machine Learning,Training,X-rays},
  file = {C\:\\Users\\YeCanming\\Documents\\WPS Cloud Files\\200995647\\Zotero文献库\\人工智能\\医学人工智能\\类似文章\\Gunasinghe et al_2019_Early Prediction of Lung Diseases.pdf;D\:\\ProgramFiles\\Zotero\\storage\\F2I59ELW\\9033668.html}
}

@article{guoCNNRNNLargescaleHierarchical2018,
  title = {{{CNN-RNN}}: A Large-Scale Hierarchical Image Classification Framework},
  shorttitle = {{{CNN-RNN}}},
  author = {Guo, Yanming and Liu, Yu and Bakker, Erwin M. and Guo, Yuanhao and Lew, Michael S.},
  date = {2018-04-01},
  journaltitle = {Multimedia Tools and Applications},
  shortjournal = {Multimed Tools Appl},
  volume = {77},
  number = {8},
  pages = {10251--10271},
  issn = {1573-7721},
  doi = {10.1007/s11042-017-5443-x},
  url = {https://doi.org/10.1007/s11042-017-5443-x},
  urldate = {2024-04-05},
  abstract = {Objects are often organized in a semantic hierarchy of categories, where fine-level categories are grouped into coarse-level categories according to their semantic relations. While previous works usually only classify objects into the leaf categories, we argue that generating hierarchical labels can actually describe how the leaf categories evolved from higher level coarse-grained categories, thus can provide a better understanding of the objects. In this paper, we propose to utilize the CNN-RNN framework to address the hierarchical image classification task. CNN allows us to obtain discriminative features for the input images, and RNN enables us to jointly optimize the classification of coarse and fine labels. This framework can not only generate hierarchical labels for images, but also improve the traditional leaf-level classification performance due to incorporating the hierarchical information. Moreover, this framework can be built on top of any CNN architecture which is primarily designed for leaf-level classification. Accordingly, we build a high performance network based on the CNN-RNN paradigm which outperforms the original CNN (wider-ResNet) and also the current state-of-the-art. In addition, we investigate how to utilize the CNN-RNN framework to improve the fine category classification when a fraction of the training data is only annotated with coarse labels. Experimental results demonstrate that CNN-RNN can use the coarse-labeled training data to improve the classification of fine categories, and in some cases it even surpasses the performance achieved by fully annotated training data. This reveals that, CNN-RNN can alleviate the challenge of specialized and expensive annotation of fine labels.},
  langid = {english},
  keywords = {⛔ No INSPIRE recid found,Convolutional neural network,Hierarchical image classification,Recurrent neural network,Wider-Resnet},
  annotation = {85 citations (Crossref) [2024-04-06]},
  file = {D:\Zotero文献库\00人工智能\00机器学习\学习范式\多任务学习\多层级多标签分类\深度学习方法\Guo et al_2018_CNN-RNN.pdf}
}

@inproceedings{guoParameterEfficientTransferLearning2021,
  title = {Parameter-{{Efficient Transfer Learning}} with {{Diff Pruning}}},
  booktitle = {Proceedings of the 59th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} and the 11th {{International Joint Conference}} on {{Natural Language Processing}} ({{Volume}} 1: {{Long Papers}})},
  author = {Guo, Demi and Rush, Alexander and Kim, Yoon},
  editor = {Zong, Chengqing and Xia, Fei and Li, Wenjie and Navigli, Roberto},
  date = {2021-08},
  pages = {4884--4896},
  publisher = {Association for Computational Linguistics},
  location = {Online},
  doi = {10.18653/v1/2021.acl-long.378},
  url = {https://aclanthology.org/2021.acl-long.378},
  urldate = {2024-09-06},
  abstract = {The large size of pretrained networks makes them difficult to deploy for multiple tasks in storage-constrained settings. Diff pruning enables parameter-efficient transfer learning that scales well with new tasks. The approach learns a task-specific “diff” vector that extends the original pretrained parameters. This diff vector is adaptively pruned during training with a differentiable approximation to the L0-norm penalty to encourage sparsity. As the number of tasks increases, diff pruning remains parameter-efficient, as it requires storing only a small diff vector for each task. Since it does not require access to all tasks during training, it is attractive in on-device deployment settings where tasks arrive in stream or even from different providers. Diff pruning can match the performance of finetuned baselines on the GLUE benchmark while only modifying 0.5\% of the pretrained model's parameters per task and scales favorably in comparison to popular pruning approaches.},
  eventtitle = {{{ACL-IJCNLP}} 2021},
  langid = {english},
  keywords = {⛔ No INSPIRE recid found},
  annotation = {39 citations (Crossref) [2024-09-06]},
  file = {D\:\\ProgramFiles\\Zotero\\storage\\IC93A2Y5\\Guo 等 - 2021 - Parameter-Efficient Transfer Learning with Diff Pr.pdf;D\:\\Zotero文献库\\00人工智能\\00机器学习\\学习范式\\迁移学习\\PEFT\\selection\\Guo et al_2021_Parameter-Efficient Transfer Learning with Diff Pruning.pdf}
}

@video{GuPaoChengXuYuanWhatMakesHealthcare2023,
  entrysubtype = {tvbroadcast},
  title = {1. {{What Makes Healthcare Unique}}},
  booktitle = {1. {{What Makes Healthcare Unique}}},
  namea = {{咕泡程序员}},
  nameatype = {collaborator},
  date = {2023-10-17},
  number = {1},
  url = {https://www.bilibili.com/video/BV1WQ4y1s7Kw?p=1},
  urldate = {2023-11-11},
  abstract = {MIT 6.S897 Machine Learning for Healthcare, Spring 2019 经典永流传  ➤ 全套资料库：公众号「咕泡AI」回复关键字「156」  获取：课件PDF+笔记+习题集+拓展阅读资料    另附（机器学习基础知识+OpenCV基础及实战+医学领域深度学习基础+大模型以及论文）},
  langid = {english},
  annotation = {636/9747}
}

@inproceedings{halimuEmpiricalComparisonArea2019,
  title = {Empirical {{Comparison}} of {{Area}} under {{ROC}} Curve ({{AUC}}) and {{Mathew Correlation Coefficient}} ({{MCC}}) for {{Evaluating Machine Learning Algorithms}} on {{Imbalanced Datasets}} for {{Binary Classification}}},
  booktitle = {Proceedings of the 3rd {{International Conference}} on {{Machine Learning}} and {{Soft Computing}}},
  author = {Halimu, Chongomweru and Kasem, Asem and Newaz, S. H. Shah},
  date = {2019-01-25},
  series = {{{ICMLSC}} '19},
  pages = {1--6},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3310986.3311023},
  url = {https://dl.acm.org/doi/10.1145/3310986.3311023},
  urldate = {2023-08-24},
  abstract = {A common challenge encountered when trying to perform classifications and comparing classifiers is selecting a suitable performance metric. This is particularly important when the data has class-imbalance problems. Area under the Receiver Operating Characteristic Curve (AUC) has been commonly used by the machine learning community in such situations, and recently researchers are starting to use Matthew Correlation Coefficient (MCC), especially in biomedical research. However, there is no empirical study that has been conducted to compare the suitability of the two metrics. In this paper, the aim of this study is to provide insights about how AUC and MCC are compared to each other when used with classical machine learning algorithms over a range of imbalanced datasets. In our study, we utilize an earlier-proposed criteria for comparing metrics based on the degree of consistency and degree of Discriminancy to compare AUC against MCC. We carry out experiments using four machine learning algorithms on 54 imbalanced datasets, with imbalance ratios ranging from 1\% to 10\%. The results demonstrate that both AUC and MCC are statistically consistent with each other; however AUC is more discriminating than MCC. The same observation is noticed when evaluated on 23 balanced datasets. This suggests AUC to be a better measure than MCC in evaluating and comparing classification algorithms.},
  isbn = {978-1-4503-6612-0},
  langid = {english},
  keywords = {Area Under Curve,Consistency,Decision Tree,Discriminancy,Indifferency,K-Nearest Neighbors,Machine Learning,Matthew Correlation Coefficient,Random Forest,Support Vector Machines},
  file = {C:\Users\YeCanming\Documents\WPS Cloud Files\200995647\Zotero文献库\人工智能\医学人工智能\指标选择\Halimu et al_2019_Empirical Comparison of Area under ROC curve (AUC) and Mathew Correlation.pdf}
}

@article{hanClassificationMalwareSelfdriving2021,
  title = {Classification of Malware for Self-Driving Systems},
  author = {Han, Xiangyu and Jin, Fusheng and Wang, Runan and Wang, Shuliang and Yuan, Ye},
  date = {2021-03-07},
  journaltitle = {Neurocomputing},
  shortjournal = {Neurocomputing},
  volume = {428},
  pages = {352--360},
  issn = {0925-2312},
  doi = {10.1016/j.neucom.2020.02.131},
  url = {https://www.sciencedirect.com/science/article/pii/S0925231220312042},
  urldate = {2023-10-19},
  abstract = {Classification and distinguishing of malware is key to predict the malicious attack, which is essential in self-driving systems. In order to handle large number of malware variants, many machine learning methods have been proposed. However, the accuracy and efficiency of multiple class classification of malware still remained inadequate to meet demand. In this paper, we propose a 4-LFE method to deal with the issues above. We extract multi-features from malicious programs by combining pixel and n-gram features. In the process of feature selection, we apply L1-L2 penalty into the Logistic Regression, then use LDA to reduce dimensions of malware features. Based on the selected features, we study the performance of classification on ten machine learning algorithms. We assess our approach’s precision on a public dataset consisting 10,868 malware samples. Experimental results show our method could classify malware to their family with accuracy of 99.99\%.},
  langid = {english},
  keywords = {Feature dimension reduction,Feature selection,Linear discriminant analysis,Malware classification for self-driving systems},
  file = {D:\ProgramFiles\Zotero\storage\9YIWF5WU\S0925231220312042.html}
}

@online{hanParameterEfficientFineTuningLarge2024,
  title = {Parameter-{{Efficient Fine-Tuning}} for {{Large Models}}: {{A Comprehensive Survey}}},
  shorttitle = {Parameter-{{Efficient Fine-Tuning}} for {{Large Models}}},
  author = {Han, Zeyu and Gao, Chao and Liu, Jinyang and Zhang, Jeff and Zhang, Sai Qian},
  date = {2024-07-12},
  eprint = {2403.14608},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2403.14608},
  urldate = {2024-09-11},
  abstract = {Large models represent a groundbreaking advancement in multiple application fields, enabling remarkable achievements across various tasks. However, their unprecedented scale comes with significant computational costs. These models, often consisting of billions of parameters, require vast amounts of computational resources for execution. Especially, the expansive scale and computational demands pose considerable challenges when customizing them for particular downstream tasks, particularly over the hardware platforms constrained by computational capabilities.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {⛔ No INSPIRE recid found,Computer Science - Machine Learning},
  file = {D:\ProgramFiles\Zotero\storage\JFXPTXFE\Han 等 - 2024 - Parameter-Efficient Fine-Tuning for Large Models .pdf}
}

@article{heAccurateClassificationPulmonary2023,
  title = {Accurate Classification of Pulmonary Nodules by a Combined Model of Clinical, Imaging, and Cell-Free {{DNA}} Methylation Biomarkers: A Model Development and External Validation Study},
  shorttitle = {Accurate Classification of Pulmonary Nodules by a Combined Model of Clinical, Imaging, and Cell-Free {{DNA}} Methylation Biomarkers},
  author = {He, Jianxing and Wang, Bo and Tao, Jinsheng and Liu, Qin and Peng, Minhua and Xiong, Shan and Li, Jianfu and Cheng, Bo and Li, Caichen and Jiang, Shunjun and Qiu, Xiangcheng and Yang, Yang and Ye, Zhujia and Zeng, Fanrui and Zhang, Jian and Liu, Dan and Li, Weimin and Chen, Zhiwei and Zeng, Qingsi and Fan, Jian-Bing and Liang, Wenhua},
  date = {2023-08-09},
  journaltitle = {The Lancet Digital Health},
  shortjournal = {The Lancet Digital Health},
  issn = {2589-7500},
  doi = {10.1016/S2589-7500(23)00125-5},
  url = {https://www.sciencedirect.com/science/article/pii/S2589750023001255},
  urldate = {2023-08-25},
  abstract = {Background There is an unmet clinical need for accurate non-invasive tests to facilitate the early diagnosis of lung cancer. We propose a combined model of clinical, imaging, and cell-free DNA methylation biomarkers that aims to improve the classification of pulmonary nodules. Methods We conducted a prospective specimen collection and retrospective masked evaluation study. We recruited participants with a solitary pulmonary nodule sized 5–30 mm from 24 hospitals across 20 cities in China. Participants who were aged 18 years or older and had been referred with 5–30 mm non-calcified and solitary pulmonary nodules, including solid nodules, part solid nodules, and pure ground-glass nodules, were included. We developed a combined clinical and imaging biomarkers (CIBM) model by machine learning for the classification of malignant and benign pulmonary nodules in a cohort (n=839) and validated it in two cohorts (n=258 in the first cohort and n=283 in the second cohort). We then integrated the CIBM model with our previously established circulating tumour DNA methylation model (PulmoSeek) to create a new combined model, PulmoSeek Plus (n=258), and verified it in an independent cohort (n=283). The clinical utility of the models was evaluated using decision curve analysis. A low cutoff (0·65) for high sensitivity and a high cutoff (0·89) for high specificity were applied simultaneously to stratify pulmonary nodules into low-risk, medium-risk, and high-risk groups. The primary outcome was the diagnostic performance of the CIBM, PulmoSeek, and PulmoSeek Plus models. Participants in this study were drawn from two prospective clinical studies that were registered (NCT03181490 and NCT03651986), the first of which was completed, and the second of which is ongoing because 25\% of participants have not yet finished the required 3-year follow-up. Findings We recruited a total of 1380 participants. 1097 participants were enrolled from July 7, 2017, to Feb 12, 2019; 839 participants were used for the CIBM model training set, and the rest (n=258) for the first CIBM validation set and the PulmoSeek Plus training set. 283 participants were enrolled from Oct 26, 2018, to March 20, 2020, as an independent validation set for the PulmoSeek Plus model and the second validation set for the CIBM model. The CIBM model validation cohorts had area under the curves (AUCs) of 0·85 (95\% CI 0·80–0·89) and 0·85 (0·81–0·89). The PulmoSeek Plus model had better discrimination capacity compared with the CIBM and PulmoSeek models with an increase of 0·05 in AUC (PulmoSeek Plus vs CIBM, 95\% CI 0·022–0·087, p=0·001; and PulmoSeek Plus vs PulmoSeek, 0·018–0·083, p=0·002). The overall sensitivity of the PulmoSeek Plus model was 0·98 (0·97–0·99) at a fixed specificity of 0·50 for ruling out lung cancer. A high sensitivity of 0·98 (0·96–0·99) was maintained in early-stage lung cancer (stages 0 and I) and 0·99 (0·96–1·00) in 5–10 mm nodules. The decision curve showed that if an invasive intervention, such as surgical resection or biopsy, was deemed necessary at more than the risk threshold score of 0·54, the PulmoSeek Plus model would provide a standardised net benefit of 82·38\% (76·06–86·79\%), equivalent to correctly identifying approximately 83 of 100 people with lung cancer. Using the PulmoSeek Plus model to classify pulmonary nodules with two cutoffs (0·65 and 0·89) would have reduced 89\% (105/118) of unnecessary surgeries and 73\% (308/423) of delayed treatments. Interpretation The PulmoSeek Plus Model combining clinical, imaging, and cell-free DNA methylation biomarkers aids the early diagnosis of pulmonary nodules, with potential application in clinical decision making for the management of pulmonary nodules. Funding The China National Science Foundation, the Key Project of Guangzhou Scientific Research Project, the High-Level University Construction Project of Guangzhou Medical University, the National Key Research \& Development Programme, the Guangdong High Level Hospital Construction “Reaching Peak” Plan, the Guangdong Basic and Applied Basic Research Foundation, the National Natural Science Foundation of China, The Leading Projects of Guangzhou Municipal Health Sciences Foundation, the Key Research and Development Plan of Shaanxi Province of China, the Scheme of Guangzhou Economic and Technological Development District for Leading Talents in Innovation and Entrepreneurship, the Scheme of Guangzhou for Leading Talents in Innovation and Entrepreneurship, the Scheme of Guangzhou for Leading Team in Innovation, the Guangzhou Development Zone International Science and Technology Cooperation Project, and the Science and Technology Planning Project of Guangzhou.},
  langid = {english},
  file = {C\:\\Users\\YeCanming\\Documents\\WPS Cloud Files\\200995647\\Zotero文献库\\人工智能\\医学人工智能\\He et al_2023_Accurate classification of pulmonary nodules by a combined model of clinical,.pdf;D\:\\ProgramFiles\\Zotero\\storage\\HRTGPHAB\\S2589750023001255.html}
}

@online{heAutomatedModelDesign2021,
  title = {Automated {{Model Design}} and {{Benchmarking}} of {{3D Deep Learning Models}} for {{COVID-19 Detection}} with {{Chest CT Scans}}},
  author = {He, Xin and Wang, Shihao and Chu, Xiaowen and Shi, Shaohuai and Tang, Jiangping and Liu, Xin and Yan, Chenggang and Zhang, Jiyong and Ding, Guiguang},
  date = {2021-02-12},
  eprint = {2101.05442},
  eprinttype = {arXiv},
  eprintclass = {cs, eess},
  url = {http://arxiv.org/abs/2101.05442},
  urldate = {2023-07-30},
  abstract = {The COVID-19 pandemic has spread globally for several months. Because its transmissibility and high pathogenicity seriously threaten people’s lives, it is crucial to accurately and quickly detect COVID-19 infection. Many recent studies have shown that deep learning (DL) based solutions can help detect COVID-19 based on chest CT scans. However, most existing work focuses on 2D datasets, which may result in low quality models as the real CT scans are 3D images. Besides, the reported results span a broad spectrum on different datasets with a relatively unfair comparison. In this paper, we first use three state-of-the-art 3D models (ResNet3D101, DenseNet3D121, and MC3 18) to establish the baseline performance on the three publicly available chest CT scan datasets. Then we propose a differentiable neural architecture search (DNAS) framework to automatically search for the 3D DL models for 3D chest CT scans classification with the Gumbel Softmax technique to improve the searching efficiency. We further exploit the Class Activation Mapping (CAM) technique on our models to provide the interpretability of the results. The experimental results show that our automatically searched models (CovidNet3D) outperform the baseline human-designed models on the three datasets with tens of times smaller model size and higher accuracy. Furthermore, the results also verify that CAM can be well applied in CovidNet3D for COVID-19 datasets to provide interpretability for medical diagnosis.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Electrical Engineering and Systems Science - Image and Video Processing},
  file = {D:\ProgramFiles\Zotero\storage\2WF9HJ5W\He 等 - 2021 - Automated Model Design and Benchmarking of 3D Deep.pdf}
}

@article{heAutoMLSurveyStateoftheart2021,
  title = {{{AutoML}}: {{A}} Survey of the State-of-the-Art},
  shorttitle = {{{AutoML}}},
  author = {He, Xin and Zhao, Kaiyong and Chu, Xiaowen},
  date = {2021-01},
  journaltitle = {Knowledge-Based Systems},
  shortjournal = {Knowledge-Based Systems},
  volume = {212},
  pages = {106622},
  issn = {09507051},
  doi = {10.1016/j.knosys.2020.106622},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0950705120307516},
  urldate = {2023-08-06},
  abstract = {Deep learning (DL) techniques have obtained remarkable achievements on various tasks, such as image recognition, object detection, and language modeling. However, building a high-quality DL system for a specific task highly relies on human expertise, hindering its wide application. Meanwhile, automated machine learning (AutoML) is a promising solution for building a DL system without human assistance and is being extensively studied. This paper presents a comprehensive and up-to-date review of the state-of-the-art (SOTA) in AutoML. According to the DL pipeline, we introduce AutoML methods –– covering data preparation, feature engineering, hyperparameter optimization, and neural architecture search (NAS) –– with a particular focus on NAS, as it is currently a hot sub-topic of AutoML. We summarize the representative NAS algorithms’ performance on the CIFAR-10 and ImageNet datasets and further discuss the following subjects of NAS methods: one/two-stage NAS, one-shot NAS, joint hyperparameter and architecture optimization, and resource-aware NAS. Finally, we discuss some open problems related to the existing AutoML methods for future research.},
  langid = {english},
  file = {D:\ProgramFiles\Zotero\storage\2H7GLZ5L\He 等 - 2021 - AutoML A survey of the state-of-the-art.pdf}
}

@article{heAutoMLSurveyStateoftheArt2021,
  title = {{{AutoML}}: {{A Survey}} of the {{State-of-the-Art}}},
  shorttitle = {{{AutoML}}},
  author = {He, Xin and Zhao, Kaiyong and Chu, Xiaowen},
  date = {2021-01},
  journaltitle = {Knowledge-Based Systems},
  shortjournal = {Knowledge-Based Systems},
  volume = {212},
  eprint = {1908.00709},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  pages = {106622},
  issn = {09507051},
  doi = {10.1016/j.knosys.2020.106622},
  url = {http://arxiv.org/abs/1908.00709},
  urldate = {2023-10-17},
  abstract = {Deep learning (DL) techniques have penetrated all aspects of our lives and brought us great convenience. However, building a high-quality DL system for a specific task highly relies on human expertise, hindering the applications of DL to more areas. Automated machine learning (AutoML) becomes a promising solution to build a DL system without human assistance, and a growing number of researchers focus on AutoML. In this paper, we provide a comprehensive and up-to-date review of the state-of-the-art (SOTA) in AutoML. First, we introduce AutoML methods according to the pipeline, covering data preparation, feature engineering, hyperparameter optimization, and neural architecture search (NAS). We focus more on NAS, as it is currently very hot sub-topic of AutoML. We summarize the performance of the representative NAS algorithms on the CIFAR-10 and ImageNet datasets and further discuss several worthy studying directions of NAS methods: one/two-stage NAS, one-shot NAS, and joint hyperparameter and architecture optimization. Finally, we discuss some open problems of the existing AutoML methods for future research.},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\YeCanming\\Documents\\WPS Cloud Files\\200995647\\Zotero文献库\\人工智能\\医学人工智能\\AI知识查缺补漏\\automl\\He et al_2021_AutoML.pdf;D\:\\ProgramFiles\\Zotero\\storage\\5ZKM6IPH\\1908.html}
}

@inproceedings{heMetaBalanceImprovingMultiTask2022,
  title = {{{MetaBalance}}: {{Improving Multi-Task Recommendations}} via {{Adapting Gradient Magnitudes}} of {{Auxiliary Tasks}}},
  shorttitle = {{{MetaBalance}}},
  booktitle = {Proceedings of the {{ACM Web Conference}} 2022},
  author = {He, Yun and Feng, Xue and Cheng, Cheng and Ji, Geng and Guo, Yunsong and Caverlee, James},
  date = {2022-04-25},
  eprint = {2203.06801},
  eprinttype = {arXiv},
  eprintclass = {cs},
  pages = {2205--2215},
  doi = {10.1145/3485447.3512093},
  url = {http://arxiv.org/abs/2203.06801},
  urldate = {2022-09-10},
  abstract = {In many personalized recommendation scenarios, the generalization ability of a target task can be improved via learning with additional auxiliary tasks alongside this target task on a multi-task network. However, this method often suffers from a serious optimization imbalance problem. On the one hand, one or more auxiliary tasks might have a larger influence than the target task and even dominate the network weights, resulting in worse recommendation accuracy for the target task. On the other hand, the influence of one or more auxiliary tasks might be too weak to assist the target task. More challenging is that this imbalance dynamically changes throughout the training process and varies across the parts of the same network. We propose a new method: MetaBalance to balance auxiliary losses via directly manipulating their gradients w.r.t the shared parameters in the multi-task network. Specifically, in each training iteration and adaptively for each part of the network, the gradient of an auxiliary loss is carefully reduced or enlarged to have a closer magnitude to the gradient of the target loss, preventing auxiliary tasks from being so strong that dominate the target task or too weak to help the target task. Moreover, the proximity between the gradient magnitudes can be flexibly adjusted to adapt MetaBalance to different scenarios. The experiments show that our proposed method achieves a significant improvement of 8.34\% in terms of NDCG@10 upon the strongest baseline on two real-world datasets. The code of our approach can be found at here: https://github.com/facebookresearch/MetaBalance},
  langid = {english},
  keywords = {Computer Science - Information Retrieval,Computer Science - Machine Learning},
  file = {C:\Users\YeCanming\Documents\WPS Cloud Files\200995647\Zotero文献库\人工智能\创新实践\多任务学习\已推荐系统为例的多任务学习\He et al_2022_MetaBalance.pdf}
}

@inproceedings{hePracticalLessonsPredicting2014,
  title = {Practical {{Lessons}} from {{Predicting Clicks}} on {{Ads}} at {{Facebook}}},
  booktitle = {Proceedings of the {{Eighth International Workshop}} on {{Data Mining}} for {{Online Advertising}}},
  author = {He, Xinran and Pan, Junfeng and Jin, Ou and Xu, Tianbing and Liu, Bo and Xu, Tao and Shi, Yanxin and Atallah, Antoine and Herbrich, Ralf and Bowers, Stuart and Candela, Joaquin Quiñonero},
  date = {2014-08-24},
  pages = {1--9},
  publisher = {ACM},
  location = {New York NY USA},
  doi = {10.1145/2648584.2648589},
  url = {https://dl.acm.org/doi/10.1145/2648584.2648589},
  urldate = {2023-08-18},
  abstract = {Online advertising allows advertisers to only bid and pay for measurable user responses, such as clicks on ads. As a consequence, click prediction systems are central to most online advertising systems. With over 750 million daily active users and over 1 million active advertisers, predicting clicks on Facebook ads is a challenging machine learning task. In this paper we introduce a model which combines decision trees with logistic regression, outperforming either of these methods on its own by over 3\%, an improvement with significant impact to the overall system performance. We then explore how a number of fundamental parameters impact the final prediction performance of our system. Not surprisingly, the most important thing is to have the right features: those capturing historical information about the user or ad dominate other types of features. Once we have the right features and the right model (decisions trees plus logistic regression), other factors play small roles (though even small improvements are important at scale). Picking the optimal handling for data freshness, learning rate schema and data sampling improve the model slightly, though much less than adding a high-value feature, or picking the right model to begin with.},
  eventtitle = {{{KDD}} '14: {{The}} 20th {{ACM SIGKDD International Conference}} on {{Knowledge Discovery}} and {{Data Mining}}},
  isbn = {978-1-4503-2999-6},
  langid = {english},
  file = {D:\ProgramFiles\Zotero\storage\AKNZTJUX\He 等 - 2014 - Practical Lessons from Predicting Clicks on Ads at.pdf}
}

@inproceedings{heUnifiedViewParameterEfficient2022,
  title = {Towards a {{Unified View}} of {{Parameter-Efficient Transfer Learning}}},
  booktitle = {The {{Tenth International Conference}} on {{Learning Representations}}, {{ICLR}} 2022, {{Virtual Event}}, {{April}} 25-29, 2022},
  author = {He, Junxian and Zhou, Chunting and Ma, Xuezhe and Berg-Kirkpatrick, Taylor and Neubig, Graham},
  date = {2022},
  publisher = {OpenReview.net},
  url = {https://openreview.net/forum?id=0RDcd5Axok},
  urldate = {2024-05-22},
  langid = {english},
  keywords = {⛔ No INSPIRE recid found,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {C\:\\Users\\YeCanming\\Documents\\WPS Cloud Files\\200995647\\Zotero文献库\\人工智能\\创新实践\\prompt\\He et al_2022_Towards a Unified View of Parameter-Efficient Transfer Learning.pdf;D\:\\ProgramFiles\\Zotero\\storage\\L7KKYDVF\\2110.html}
}

@article{houlsbyParameterEfficientTransferLearning,
  title = {Parameter-{{Efficient Transfer Learning}} for {{NLP}}},
  author = {Houlsby, Neil and Giurgiu, Andrei and Jastrzebski, Stanisław and Morrone, Bruna},
  abstract = {Fine-tuning large pre-trained models is an effective transfer mechanism in NLP. However, in the presence of many downstream tasks, fine-tuning is parameter inefficient: an entire new model is required for every task. As an alternative, we propose transfer with adapter modules. Adapter modules yield a compact and extensible model; they add only a few trainable parameters per task, and new tasks can be added without revisiting previous ones. The parameters of the original network remain fixed, yielding a high degree of parameter sharing. To demonstrate adapter’s effectiveness, we transfer the recently proposed BERT Transformer model to 26 diverse text classification tasks, including the GLUE benchmark. Adapters attain near state-of-the-art performance, whilst adding only a few parameters per task. On GLUE, we attain within 0.4\% of the performance of full fine-tuning, adding only 3.6\% parameters per task. By contrast, fine-tuning trains 100\% of the parameters per task.},
  langid = {english},
  keywords = {⛔ No INSPIRE recid found},
  file = {D:\ProgramFiles\Zotero\storage\TYBMMXNH\Houlsby 等 - Parameter-Efficient Transfer Learning for NLP.pdf}
}

@online{huangLoraHubEfficientCrossTask2023,
  title = {{{LoraHub}}: {{Efficient Cross-Task Generalization}} via {{Dynamic LoRA Composition}}},
  shorttitle = {{{LoraHub}}},
  author = {Huang, Chengsong and Liu, Qian and Lin, Bill Yuchen and Pang, Tianyu and Du, Chao and Lin, Min},
  date = {2023-07-25},
  eprint = {2307.13269},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2307.13269},
  urldate = {2023-08-04},
  abstract = {Low-rank adaptations (LoRA) are often employed to fine-tune large language models (LLMs) for new tasks. This paper investigates LoRA composability for cross-task generalization and introduces LoraHub, a strategic framework devised for the purposive assembly of LoRA modules trained on diverse given tasks, with the objective of achieving adaptable performance on unseen tasks. With just a few examples from a novel task, LoraHub enables the fluid combination of multiple LoRA modules, eradicating the need for human expertise. Notably, the composition requires neither additional model parameters nor gradients. Our empirical results, derived from the Big-Bench Hard (BBH) benchmark, suggest that LoraHub can effectively mimic the performance of in-context learning in few-shot scenarios, excluding the necessity of in-context examples alongside each inference input. A significant contribution of our research is the fostering of a community for LoRA, where users can share their trained LoRA modules, thereby facilitating their application to new tasks. We anticipate this resource will widen access to and spur advancements in general intelligence as well as LLMs in production. Code will be available at github.com/sail-sg/lorahub.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {D:\ProgramFiles\Zotero\storage\BKIYVP2H\Huang 等 - 2023 - LoraHub Efficient Cross-Task Generalization via D.pdf}
}

@inproceedings{huangMultitaskLearningApproach2022,
  title = {A {{Multi-task Learning Approach}} for {{Predicting Intentions Using Smart Home IoT Services}}},
  booktitle = {Service-{{Oriented Computing}} (Icsoc 2022)},
  author = {Huang, Bing and Zhang, Boyu and Sheng, Quan Z. and Lam, Kwok-Yan},
  editor = {Troya, J. and Medjahed, B. and Piattini, M. and Yao, L. and Fernandez, P. and Ruiz-Cortes, A.},
  date = {2022},
  volume = {13740},
  pages = {413--421},
  publisher = {Springer International Publishing Ag},
  location = {Cham},
  issn = {0302-9743},
  doi = {10.1007/978-3-031-20984-0_29},
  url = {https://www.webofscience.com/wos/alldb/full-record/WOS:000898280300029},
  urldate = {2023-02-16},
  abstract = {We propose a novel approach for predicting a resident's future intentions in terms of what, how, when, and where he will do next. The intention model is learned from his previous interactions with various types of IoT services. In particular, we propose a multi-task learning approach for predicting resident's future intention. The multi-task learning approach jointly learns the tasks of what, how, when, and where to boost the overall performance of the four tasks. We demonstrate the effectiveness and performance of our approach by conducting experiments on real-world datasets.},
  isbn = {978-3-031-20983-3 978-3-031-20984-0},
  langid = {english},
  keywords = {Activity prediction,Intention,IoT service,Multi-task learning,Smart   home},
  annotation = {WOS:000898280300029}
}

@inproceedings{huLoRALowRankAdaptation2021,
  title = {{{LoRA}}: {{Low-Rank Adaptation}} of {{Large Language Models}}},
  shorttitle = {{{LoRA}}},
  author = {Hu, Edward J. and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
  date = {2021-10-06},
  url = {https://openreview.net/forum?id=nZeVKeeFYf9},
  urldate = {2024-05-10},
  abstract = {An important paradigm of natural language processing consists of large-scale pre-training on general domain data and adaptation to particular tasks or domains. As we pre-train larger models, full fine-tuning, which retrains all model parameters, becomes less feasible. Using GPT-3 175B as an example -- deploying independent instances of fine-tuned models, each with 175B parameters, is prohibitively expensive. We propose Low-Rank Adaptation, or LoRA, which freezes the pre-trained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable parameters for downstream tasks. Compared to GPT-3 175B fine-tuned with Adam, LoRA can reduce the number of trainable parameters by a factor of 10,000 and the GPU memory requirement by a factor of 3. LoRA performs on-par or better than fine-tuning in model quality on RoBERTa, DeBERTa, GPT-2, and GPT-3, despite having fewer trainable parameters, a higher training throughput, and, unlike adapters, no additional inference latency. We also provide an empirical investigation into rank-deficiency in language model adaptation, which sheds light on the efficacy of LoRA. We release a package that facilitates the integration of LoRA with PyTorch models and provide our implementations and model checkpoints for RoBERTa, DeBERTa, and GPT-2 at https://github.com/microsoft/LoRA.},
  eventtitle = {International {{Conference}} on {{Learning Representations}}},
  langid = {english},
  keywords = {⛔ No INSPIRE recid found,Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {C\:\\Users\\YeCanming\\Documents\\WPS Cloud Files\\200995647\\Zotero文献库\\人工智能\\医学人工智能\\Hu et al_2021_LoRA.pdf;D\:\\ProgramFiles\\Zotero\\storage\\29HDK9ZB\\2106.html}
}

@report{InadmissibilityUsualEstimator,
  title = {Inadmissibility of the {{Usual Estimator}} for the {{Mean}} of a {{Multivariate Normal Distribution}}},
  url = {https://apps.dtic.mil/sti/citations/AD1028390},
  urldate = {2023-05-03},
  abstract = {If one observes the real random variables Xi, Xn independently normally distributed with unknown means xi...x in and variance 1, it is customary to estimate xi by Xi. If the loss is the sum of squares of the errors, this estimator is admissible for n  or equal to 2, but inadmissible for n more than or equal to 3. Since the usual estimator is best among those which transform correctly under translation, any admissible estimator for n equals more than or equal to 3 involves an arbitrary choice. While the results of this paper are not in a form suitable for immediate practical application, the possible improvement over the usual estimator seems to be large enough to be of practical importance if n is large.},
  langid = {english},
  file = {D:\ProgramFiles\Zotero\storage\HEIGKAMZ\AD1028390.html}
}

@online{IncrementalLearningDeepa,
  title = {Incremental {{Learning Through Deep Adaptation}}},
  url = {https://ieeexplore.ieee.org/document/8554156},
  urldate = {2024-09-10},
  abstract = {Given an existing trained neural network, it is often desirable to learn new capabilities without hindering performance of those already learned. Existing approaches either learn sub-optimal solutions, require joint training, or incur a substantial increment in the number of parameters for each added domain, typically as many as the original network. We propose a method called Deep Adaptation Modules (DAM) that constrains newly learned filters to be linear combinations of existing ones. DAMs precisely preserve performance on the original domain, require a fraction (typically 13 percent, dependent on network architecture) of the number of parameters compared to standard fine-tuning procedures and converge in less cycles of training to a comparable or better level of performance. When coupled with standard network quantization techniques, we further reduce the parameter cost to around 3 percent of the original with negligible or no loss in accuracy. The learned architecture can be controlled to switch between various learned representations, enabling a single network to solve a task from multiple different domains. We conduct extensive experiments showing the effectiveness of our method on a range of image classification tasks and explore different aspects of its behavior.},
  langid = {english},
  keywords = {⛔ No INSPIRE recid found},
  file = {D:\ProgramFiles\Zotero\storage\AFRRVSG6\8554156.html}
}

@software{iridiumblueRocstarObjectiveFunction2022,
  title = {Roc-Star : {{An}} Objective Function for {{ROC-AUC}} That Actually Works.},
  shorttitle = {Roc-Star},
  author = {{iridiumblue}},
  date = {2022-12-13T22:22:30Z},
  origdate = {2020-04-14T05:07:24Z},
  url = {https://github.com/iridiumblue/roc-star},
  urldate = {2022-12-15},
  abstract = {Loss function which directly targets ROC-AUC}
}

@inproceedings{jacotNeuralTangentKernel2018,
  title = {Neural {{Tangent Kernel}}: {{Convergence}} and {{Generalization}} in {{Neural Networks}}},
  shorttitle = {Neural {{Tangent Kernel}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Jacot, Arthur and Gabriel, Franck and Hongler, Clement},
  date = {2018},
  volume = {31},
  publisher = {Curran Associates, Inc.},
  url = {https://proceedings.neurips.cc/paper/2018/hash/5a4be1fa34e62bb8a6ec6b91d2462f5a-Abstract.html},
  urldate = {2024-09-12},
  abstract = {At initialization, artificial neural networks (ANNs) are equivalent to Gaussian processes in the infinite-width limit, thus connecting them to kernel methods. We prove that the evolution of an ANN during training can also be described by a kernel: during gradient descent on the parameters of an ANN, the network function (which maps input vectors to output vectors) follows the so-called kernel gradient associated with a new object, which we call the Neural Tangent Kernel (NTK). This kernel is central to describe the generalization features of ANNs. While the NTK is random at initialization and varies during training, in the infinite-width limit it converges to an explicit limiting kernel and stays constant during training. This makes it possible to study the training of ANNs in function space instead of parameter space. Convergence of the training can then be related to the positive-definiteness of the limiting NTK.},
  langid = {english},
  keywords = {⛔ No INSPIRE recid found},
  file = {D:\Zotero文献库\00人工智能\00机器学习\学习范式\迁移学习\PEFT\理论解释\Jacot et al_2018_Neural Tangent Kernel.pdf}
}

@article{jiaDynamicFilterNetworks2016,
  title = {Dynamic {{Filter Networks}}},
  author = {Jia, Xu and De Brabandere, Bert and Tuytelaars, Tinne and Gool, Luc V.},
  date = {2016},
  journaltitle = {Advances in Neural Information Processing Systems},
  volume = {29},
  url = {https://papers.nips.cc/paper_files/paper/2016/hash/8bf1211fd4b7b94528899de0a43b9fb3-Abstract.html},
  urldate = {2024-09-10},
  langid = {english},
  keywords = {⛔ No INSPIRE recid found},
  file = {D:\Zotero文献库\00人工智能\00机器学习\学习范式\迁移学习\PEFT\理论解释\Jia et al_2016_Dynamic Filter Networks.pdf}
}

@inproceedings{jiangResTuningFlexibleEfficient2023,
  title = {Res-{{Tuning}}: {{A Flexible}} and {{Efficient Tuning Paradigm}} via {{Unbinding Tuner}} from {{Backbone}}},
  shorttitle = {Res-{{Tuning}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 36: {{Annual Conference}} on {{Neural Information Processing Systems}} 2023, {{NeurIPS}} 2023, {{New Orleans}}, {{LA}}, {{USA}}, {{December}} 10 - 16, 2023},
  author = {Jiang, Zeyinzi and Mao, Chaojie and Huang, Ziyuan and Ma, Ao and Lv, Yiliang and Shen, Yujun and Zhao, Deli and Zhou, Jingren},
  editor = {Oh, Alice and Naumann, Tristan and Globerson, Amir and Saenko, Kate and Hardt, Moritz and Levine, Sergey},
  date = {2023},
  url = {http://papers.nips.cc/paper\_files/paper/2023/hash/8514a5203b87cba5e440bd62ab18f2b4-Abstract-Conference.html},
  urldate = {2024-05-22},
  langid = {english},
  keywords = {⛔ No INSPIRE recid found,Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition},
  file = {D\:\\Zotero文献库\\00人工智能\\00机器学习\\学习范式\\迁移学习\\PEFT\\Jiang et al_2023_Res-Tuning.pdf;D\:\\ProgramFiles\\Zotero\\storage\\4EYP69WJ\\2310.html}
}

@inproceedings{jiaVisualPromptTuning2022,
  title = {Visual {{Prompt Tuning}}},
  booktitle = {Computer {{Vision}} - {{ECCV}} 2022 - 17th {{European Conference}}, {{Tel Aviv}}, {{Israel}}, {{October}} 23-27, 2022, {{Proceedings}}, {{Part XXXIII}}},
  author = {Jia, Menglin and Tang, Luming and Chen, Bor-Chun and Cardie, Claire and Belongie, Serge J. and Hariharan, Bharath and Lim, Ser-Nam},
  editor = {Avidan, Shai and Brostow, Gabriel J. and Cissé, Moustapha and Farinella, Giovanni Maria and Hassner, Tal},
  date = {2022},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  volume = {13693},
  pages = {709--727},
  publisher = {Springer},
  doi = {10.1007/978-3-031-19827-4_41},
  url = {https://doi.org/10.1007/978-3-031-19827-4\_41},
  urldate = {2024-05-22},
  langid = {english},
  keywords = {⛔ No INSPIRE recid found,Computer Science - Computer Vision and Pattern Recognition},
  annotation = {190 citations (Crossref) [2024-05-22]},
  file = {C\:\\Users\\YeCanming\\Documents\\WPS Cloud Files\\200995647\\Zotero文献库\\人工智能\\计算机视觉课程\\大模型提示微调用于长尾学习\\Jia et al_2022_Visual Prompt Tuning.pdf;D\:\\ProgramFiles\\Zotero\\storage\\CI8IYLSW\\2203.html;D\:\\ProgramFiles\\Zotero\\storage\\UKRT9L62\\2203.html}
}

@online{jieConvolutionalBypassesAre2022,
  title = {Convolutional {{Bypasses Are Better Vision Transformer Adapters}}},
  author = {Jie, Shibo and Deng, Zhi-Hong},
  date = {2022-08-09},
  eprint = {2207.07039},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2207.07039},
  url = {http://arxiv.org/abs/2207.07039},
  urldate = {2024-03-21},
  abstract = {The pretrain-then-finetune paradigm has been widely adopted in computer vision. But as the size of Vision Transformer (ViT) grows exponentially, the full finetuning becomes prohibitive in view of the heavier storage overhead. Motivated by parameter-efficient transfer learning (PETL) on language transformers, recent studies attempt to insert lightweight adaptation modules (e.g., adapter layers or prompt tokens) to pretrained ViT and only finetune these modules while the pretrained weights are frozen. However, these modules were originally proposed to finetune language models and did not take into account the prior knowledge specifically for visual tasks. In this paper, we propose to construct Convolutional Bypasses (Convpass) in ViT as adaptation modules, introducing only a small amount (less than 0.5\% of model parameters) of trainable parameters to adapt the large ViT. Different from other PETL methods, Convpass benefits from the hard-coded inductive bias of convolutional layers and thus is more suitable for visual tasks, especially in the low-data regime. Experimental results on VTAB-1K benchmark and few-shot learning datasets show that Convpass outperforms current language-oriented adaptation modules, demonstrating the necessity to tailor vision-oriented adaptation modules for adapting vision models.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {⛔ No INSPIRE recid found,Computer Science - Computer Vision and Pattern Recognition},
  file = {D\:\\Zotero文献库\\00人工智能\\00计算机视觉课程\\经典视觉任务的对应设计\\视觉地点定位\\创新点\\PEFT\\Jie_Deng_2022_Convolutional Bypasses Are Better Vision Transformer Adapters.pdf;D\:\\ProgramFiles\\Zotero\\storage\\7QMICNRD\\2207.html}
}

@article{jinAutoLRSAutomaticLearningRate2021,
  title = {{{AutoLRS}}: {{Automatic Learning-Rate Schedule}} by {{Bayesian Optimization}} on the {{Fly}}},
  shorttitle = {{{AutoLRS}}},
  author = {Jin, Yuchen and Zhou, Tianyi and Zhao, Liangyu and Zhu, Yibo and Guo, Chuanxiong and Canini, Marco and Krishnamurthy, A.},
  date = {2021-05-22},
  journaltitle = {ArXiv},
  url = {https://www.semanticscholar.org/paper/AutoLRS%3A-Automatic-Learning-Rate-Schedule-by-on-the-Jin-Zhou/76488b0743c9553b7b1d7ec46afe107ea60a67ca},
  urldate = {2024-09-05},
  abstract = {The learning rate (LR) schedule is one of the most important hyper-parameters needing careful tuning in training DNNs. However, it is also one of the least automated parts of machine learning systems and usually costs significant manual effort and computing. Though there are pre-defined LR schedules and optimizers with adaptive LR, they introduce new hyperparameters that need to be tuned separately for different tasks/datasets. In this paper, we consider the question: Can we automatically tune the LR over the course of training without human involvement? We propose an efficient method, AutoLRS, which automatically optimizes the LR for each training stage by modeling training dynamics. AutoLRS aims to find an LR applied to every \$\textbackslash tau\$ steps that minimizes the resulted validation loss. We solve this black-box optimization on the fly by Bayesian optimization (BO). However, collecting training instances for BO requires a system to evaluate each LR queried by BO's acquisition function for \$\textbackslash tau\$ steps, which is prohibitively expensive in practice. Instead, we apply each candidate LR for only \$\textbackslash tau'\textbackslash ll\textbackslash tau\$ steps and train an exponential model to predict the validation loss after \$\textbackslash tau\$ steps. This mutual-training process between BO and the loss-prediction model allows us to limit the training steps invested in the BO search. We demonstrate the advantages and the generality of AutoLRS through extensive experiments of training DNNs for tasks from diverse domains using different optimizers. The LR schedules auto-generated by AutoLRS lead to a speedup of \$1.22\textbackslash times\$, \$1.43\textbackslash times\$, and \$1.5\textbackslash times\$ when training ResNet-50, Transformer, and BERT, respectively, compared to the LR schedules in their original papers, and an average speedup of \$1.31\textbackslash times\$ over state-of-the-art heavily-tuned LR schedules.},
  langid = {english},
  keywords = {⛔ No INSPIRE recid found},
  file = {D:\Zotero文献库\00人工智能\00机器学习\优化器\超参优化\Jin et al_2021_AutoLRS.pdf}
}

@online{JingXuanJianYiShouCangJiQiXueXiShuJuYuChuLi,
  title = {【精选】【建议收藏】机器学习数据预处理（三）——数据分桶及数据标准化（内附代码）\_数据预处理 分桶\_生鱼同学的博客-CSDN博客},
  url = {https://blog.csdn.net/weixin_44035098/article/details/126513299},
  urldate = {2023-11-09},
  langid = {chinese},
  file = {D:\ProgramFiles\Zotero\storage\T5YDQ8KW\126513299.html}
}

@online{JingXuanZhouZhiHuaTuanDuiTensorFlowKaiYuanJueCeSenLinKuTFDF_AIGuaNiuCheDeBoKeCSDNBoKe,
  title = {【精选】周志华团队 | {{TensorFlow开源决策森林库TF-DF}}\_{{AI蜗牛车的博客-CSDN博客}}},
  url = {https://blog.csdn.net/qq_33431368/article/details/117433098},
  urldate = {2023-11-10},
  langid = {english}
}

@online{jpotworUnderstandingAlphaParameter2023,
  type = {Forum post},
  title = {Understanding Alpha Parameter Tuning in {{LORA}} Paper},
  author = {{jpotwor}},
  date = {2023-08-14},
  url = {https://datascience.stackexchange.com/q/123229},
  urldate = {2024-09-09},
  langid = {english},
  organization = {Data Science Stack Exchange},
  keywords = {⛔ No INSPIRE recid found},
  file = {D:\ProgramFiles\Zotero\storage\AMCGVV4A\understanding-alpha-parameter-tuning-in-lora-paper.html}
}

@online{KaiFaJiHeCeShiJiDeDingYi,
  title = {开发集和测试集的定义},
  url = {https://deeplearning-ai.github.io/machine-learning-yearning-cn//machine-learning-yearning-cn/docs/ch05/},
  urldate = {2023-03-24},
  abstract = {继续分析我们之前提到的猫咪图片的案例：现在你负责运营一个移动端 app，用户会向这个 app 上传许多不同内容的图片。而你希望这个 app 能够从图片中自动地找到含有猫的图片。},
  organization = {机器学习训练秘籍},
  file = {D:\ProgramFiles\Zotero\storage\N7K34IGB\ch05.html}
}

@article{karmakersantuAutoMLDateChallenges2022,
  title = {{{AutoML}} to {{Date}} and {{Beyond}}: {{Challenges}} and {{Opportunities}}},
  shorttitle = {{{AutoML}} to {{Date}} and {{Beyond}}},
  author = {Karmaker (“Santu”), Shubhra Kanti and Hassan, Md. Mahadi and Smith, Micah J. and Xu, Lei and Zhai, Chengxiang and Veeramachaneni, Kalyan},
  date = {2022-11-30},
  journaltitle = {ACM Computing Surveys},
  shortjournal = {ACM Comput. Surv.},
  volume = {54},
  number = {8},
  pages = {1--36},
  issn = {0360-0300, 1557-7341},
  doi = {10.1145/3470918},
  url = {https://dl.acm.org/doi/10.1145/3470918},
  urldate = {2023-08-23},
  abstract = {As big data becomes ubiquitous across domains, and more and more stakeholders aspire to make the most of their data, demand for machine learning tools has spurred researchers to explore the possibilities of automated machine learning (AutoML). AutoML tools aim to make machine learning accessible for non-machine learning experts (domain experts), to improve the efficiency of machine learning, and to accelerate machine learning research. But although automation and efficiency are among AutoML’s main selling points, the process still requires human involvement at a number of vital steps, including understanding the attributes of domain-specific data, defining prediction problems, creating a suitable training dataset, and selecting a promising machine learning technique. These steps often require a prolonged back-and-forth that makes this process inefficient for domain experts and data scientists alike and keeps so-called AutoML systems from being truly automatic. In this review article, we introduce a new classification system for AutoML systems, using a seven-tiered schematic to distinguish these systems based on their level of autonomy. We begin by describing what an end-to-end machine learning pipeline actually looks like, and which subtasks of the machine learning pipeline have been automated so far. We highlight those subtasks that are still done manually—generally by a data scientist—and explain how this limits domain experts’ access to machine learning. Next, we introduce our novel level-based taxonomy for AutoML systems and define each level according to the scope of automation support provided. Finally, we lay out a roadmap for the future, pinpointing the research required to further automate the end-to-end machine learning pipeline and discussing important challenges that stand in the way of this ambitious goal.},
  langid = {english},
  file = {D:\ProgramFiles\Zotero\storage\PI8GGEYD\Karmaker (“Santu”) 等 - 2022 - AutoML to Date and Beyond Challenges and Opportun.pdf}
}

@online{KeJiZhouDiZhenKanTanJiaoNiShenRuLiJieYuXunLian2021,
  type = {知乎专栏文章},
  title = {教你深入理解“预训练”},
  author = {科技州地震勘探},
  date = {2021-05-09},
  url = {https://zhuanlan.zhihu.com/p/370859857},
  urldate = {2022-10-16},
  abstract = {教你深入理解“预训练” - 来自知乎专栏，作者: 科技州地震勘探 https://zhuanlan.zhihu.com/p/370859857},
  langid = {chinese},
  organization = {回答},
  keywords = {机器翻译,深度学习（Deep Learning）,自然语言处理},
  annotation = {赞数:64;},
  file = {D:\ProgramFiles\Zotero\storage\74V8P86Z\370859857.html}
}

@online{kendallMultiTaskLearningUsing2018,
  title = {Multi-{{Task Learning Using Uncertainty}} to {{Weigh Losses}} for {{Scene Geometry}} and {{Semantics}}},
  author = {Kendall, Alex and Gal, Yarin and Cipolla, Roberto},
  date = {2018-04-24},
  eprint = {1705.07115},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1705.07115},
  url = {http://arxiv.org/abs/1705.07115},
  urldate = {2022-11-30},
  abstract = {Numerous deep learning applications benefit from multi-task learning with multiple regression and classification objectives. In this paper we make the observation that the performance of such systems is strongly dependent on the relative weighting between each task's loss. Tuning these weights by hand is a difficult and expensive process, making multi-task learning prohibitive in practice. We propose a principled approach to multi-task deep learning which weighs multiple loss functions by considering the homoscedastic uncertainty of each task. This allows us to simultaneously learn various quantities with different units or scales in both classification and regression settings. We demonstrate our model learning per-pixel depth regression, semantic and instance segmentation from a monocular input image. Perhaps surprisingly, we show our model can learn multi-task weightings and outperform separate models trained individually on each task.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\YeCanming\\Documents\\WPS Cloud Files\\200995647\\Zotero文献库\\人工智能\\创新实践\\多任务学习\\Kendall et al_2018_Multi-Task Learning Using Uncertainty to Weigh Losses for Scene Geometry and.pdf;D\:\\ProgramFiles\\Zotero\\storage\\7JZ5D2A8\\1705.html}
}

@article{kihmClassificationRedBlood2018,
  title = {Classification of Red Blood Cell Shapes in Flow Using Outlier Tolerant Machine Learning},
  author = {Kihm, Alexander and Kaestner, Lars and Wagner, Christian and Quint, Stephan},
  editor = {Pivkin, Igor V.},
  date = {2018-06-15},
  journaltitle = {PLOS Computational Biology},
  shortjournal = {PLoS Comput Biol},
  volume = {14},
  number = {6},
  pages = {e1006278},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1006278},
  url = {https://dx.plos.org/10.1371/journal.pcbi.1006278},
  urldate = {2023-11-09},
  abstract = {The manual evaluation, classification and counting of biological objects demands for an enormous expenditure of time and subjective human input may be a source of error. Investigating the shape of red blood cells (RBCs) in microcapillary Poiseuille flow, we overcome this drawback by introducing a convolutional neural regression network for an automatic, outlier tolerant shape classification. From our experiments we expect two stable geometries: the so-called ‘slipper’ and ‘croissant’ shapes depending on the prevailing flow conditions and the cell-intrinsic parameters. Whereas croissants mostly occur at low shear rates, slippers evolve at higher flow velocities. With our method, we are able to find the transition point between both ‘phases’ of stable shapes which is of high interest to ensuing theoretical studies and numerical simulations. Using statistically based thresholds, from our data, we obtain so-called phase diagrams which are compared to manual evaluations. Prospectively, our concept allows us to perform objective analyses of measurements for a variety of flow conditions and to receive comparable results. Moreover, the proposed procedure enables unbiased studies on the influence of drugs on flow properties of single RBCs and the resulting macroscopic change of the flow behavior of whole blood.},
  langid = {english},
  file = {D:\ProgramFiles\Zotero\storage\UQ7WQYI5\Kihm et al. - 2018 - Classification of red blood cell shapes in flow us.pdf}
}

@inproceedings{kopiczkoVeRAVectorbasedRandom2023,
  title = {{{VeRA}}: {{Vector-based Random Matrix Adaptation}}},
  shorttitle = {{{VeRA}}},
  author = {Kopiczko, Dawid Jan and Blankevoort, Tijmen and Asano, Yuki M.},
  date = {2023-10-13},
  url = {https://openreview.net/forum?id=NjNfLdxr3A},
  urldate = {2024-09-26},
  abstract = {Low-rank adapation (LoRA) is a popular method that reduces the number of trainable parameters when finetuning large language models, but still faces acute storage challenges when scaling to even larger models or deploying numerous per-user or per-task adapted models. In this work, we present Vector-based Random Matrix Adaptation (VeRA), which significantly reduces the number of trainable parameters compared to LoRA, yet maintains the same performance. It achieves this by using a single pair of low-rank matrices shared across all layers and learning small scaling vectors instead. We demonstrate its effectiveness on the GLUE and E2E benchmarks, image classification tasks, and show its application in instruction-tuning of 7B and 13B language models. Website: https://dkopi.github.io/vera},
  eventtitle = {The {{Twelfth International Conference}} on {{Learning Representations}}},
  langid = {english},
  keywords = {⛔ No INSPIRE recid found},
  file = {D\:\\ProgramFiles\\Zotero\\storage\\E8WED2DN\\Kopiczko 等 - 2023 - VeRA Vector-based Random Matrix Adaptation.pdf;D\:\\Zotero文献库\\00人工智能\\00机器学习\\学习范式\\迁移学习\\PEFT\\Kopiczko et al_2023_VeRA.pdf}
}

@article{kosolwattanaSelfinspectedAdaptiveSMOTE2023,
  title = {A Self-Inspected Adaptive {{SMOTE}} Algorithm ({{SASMOTE}}) for Highly Imbalanced Data Classification in Healthcare},
  author = {Kosolwattana, Tanapol and Liu, Chenang and Hu, Renjie and Han, Shizhong and Chen, Hua and Lin, Ying},
  date = {2023-04-25},
  journaltitle = {BioData Mining},
  shortjournal = {BioData Mining},
  volume = {16},
  number = {1},
  pages = {15},
  issn = {1756-0381},
  doi = {10.1186/s13040-023-00330-4},
  url = {https://doi.org/10.1186/s13040-023-00330-4},
  urldate = {2023-07-18},
  abstract = {In many healthcare applications, datasets for classification may be highly imbalanced due to the rare occurrence of target events such as disease onset. The SMOTE (Synthetic Minority Over-sampling Technique) algorithm has been developed as an effective resampling method for imbalanced data classification by oversampling samples from the minority class. However, samples generated by SMOTE may be ambiguous, low-quality and non-separable with the majority class. To enhance the quality of generated samples, we proposed a novel self-inspected adaptive SMOTE (SASMOTE) model that leverages an adaptive nearest neighborhood selection algorithm to identify the “visible” nearest neighbors, which are used to generate samples likely to fall into the minority class. To further enhance the quality of the generated samples, an uncertainty elimination via self-inspection approach is introduced in the proposed SASMOTE model. Its objective is to filter out the generated samples that are highly uncertain and inseparable with the majority class. The effectiveness of the proposed algorithm is compared with existing SMOTE-based algorithms and demonstrated through two real-world case studies in healthcare, including risk gene discovery and fatal congenital heart disease prediction. By generating the higher quality synthetic samples, the proposed algorithm is able to help achieve better prediction performance (in terms of F1 score) on average compared to the other methods, which is promising to enhance the usability of machine learning models on highly imbalanced healthcare data.},
  langid = {english},
  keywords = {Adaptive nearest neighborhood selection,Imbalanced data classification in healthcare,Self-inspection,SMOTE-based resampling},
  file = {C\:\\Users\\YeCanming\\Documents\\WPS Cloud Files\\200995647\\Zotero文献库\\人工智能\\医学人工智能\\类似文章\\Kosolwattana et al_2023_A self-inspected adaptive SMOTE algorithm (SASMOTE) for highly imbalanced data.pdf;D\:\\ProgramFiles\\Zotero\\storage\\JHLXQRDP\\s13040-023-00330-4.html}
}

@online{LadderSideTuningYuXunLianMoXingDe,
  title = {Ladder {{Side-Tuning}}：预训练模型的“过墙梯” - 科学空间|{{Scientific Spaces}}},
  url = {https://kexue.fm/archives/9138},
  urldate = {2023-06-23},
  langid = {english}
}

@article{langoMulticlassFeatureSelection2018,
  title = {Multi-Class and Feature Selection Extensions of {{Roughly Balanced Bagging}} for Imbalanced Data},
  author = {Lango, Mateusz and Stefanowski, Jerzy},
  date = {2018-02},
  journaltitle = {Journal of Intelligent Information Systems},
  volume = {50},
  number = {1},
  pages = {97--127},
  publisher = {Springer Nature B.V.},
  issn = {09259902},
  doi = {10.1007/s10844-017-0446-7},
  url = {https://www.proquest.com/docview/1993457397/abstract/DC56D89F2200498DPQ/1},
  urldate = {2023-06-19},
  abstract = {Roughly Balanced Bagging is one of the most efficient ensembles specialized for class imbalanced data. In this paper, we study its basic properties that may influence its good classification performance. We experimentally analyze them with respect to bootstrap construction, deciding on the number of component classifiers, their diversity, and ability to deal with the most difficult types of the minority examples. Then, we introduce two generalizations of this ensemble for dealing with a higher number of attributes and for adapting it to handle multiple minority classes. Experiments with synthetic and real life data confirm usefulness of both proposals.},
  langid = {english},
  pagetotal = {97-127},
  keywords = {Bagging,Feature extraction,Intelligent systems}
}

@article{leScalingTreebasedAutomated2020,
  title = {Scaling Tree-Based Automated Machine Learning to Biomedical Big Data with a Feature Set Selector},
  author = {Le, Trang T and Fu, Weixuan and Moore, Jason H},
  editor = {Kelso, Janet},
  date = {2020-01-01},
  journaltitle = {Bioinformatics},
  volume = {36},
  number = {1},
  pages = {250--256},
  issn = {1367-4803, 1367-4811},
  doi = {10.1093/bioinformatics/btz470},
  url = {https://academic.oup.com/bioinformatics/article/36/1/250/5511404},
  urldate = {2023-07-30},
  abstract = {Motivation: Automated machine learning (AutoML) systems are helpful data science assistants designed to scan data for novel features, select appropriate supervised learning models and optimize their parameters. For this purpose, Tree-based Pipeline Optimization Tool (TPOT) was developed using strongly typed genetic programming to recommend an optimized analysis pipeline for the data scientist’s prediction problem. However, like other AutoML systems, TPOT may reach computational resource limits when working on big data such as whole-genome expression data.},
  langid = {english},
  file = {D:\ProgramFiles\Zotero\storage\Y45QVRDC\Le 等 - 2020 - Scaling tree-based automated machine learning to b.pdf}
}

@online{lesterPowerScaleParameterEfficient2021,
  title = {The {{Power}} of {{Scale}} for {{Parameter-Efficient Prompt Tuning}}},
  author = {Lester, Brian and Al-Rfou, Rami and Constant, Noah},
  date = {2021-09-02},
  eprint = {2104.08691},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2104.08691},
  url = {http://arxiv.org/abs/2104.08691},
  urldate = {2023-06-19},
  abstract = {In this work, we explore "prompt tuning", a simple yet effective mechanism for learning "soft prompts" to condition frozen language models to perform specific downstream tasks. Unlike the discrete text prompts used by GPT-3, soft prompts are learned through backpropagation and can be tuned to incorporate signal from any number of labeled examples. Our end-to-end learned approach outperforms GPT-3's "few-shot" learning by a large margin. More remarkably, through ablations on model size using T5, we show that prompt tuning becomes more competitive with scale: as models exceed billions of parameters, our method "closes the gap" and matches the strong performance of model tuning (where all model weights are tuned). This finding is especially relevant in that large models are costly to share and serve, and the ability to reuse one frozen model for multiple downstream tasks can ease this burden. Our method can be seen as a simplification of the recently proposed "prefix tuning" of Li and Liang (2021), and we provide a comparison to this and other similar approaches. Finally, we show that conditioning a frozen model with soft prompts confers benefits in robustness to domain transfer, as compared to full model tuning.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {⛔ No INSPIRE recid found,Computer Science - Computation and Language},
  file = {C\:\\Users\\YeCanming\\Documents\\WPS Cloud Files\\200995647\\Zotero文献库\\人工智能\\医学人工智能\\Lester et al_2021_The Power of Scale for Parameter-Efficient Prompt Tuning.pdf;D\:\\ProgramFiles\\Zotero\\storage\\LHTS54SH\\2104.html}
}

@article{liangOA1604Combined2021,
  title = {{{OA16}}.04 {{A Combined Model}} of {{Clinical}}, {{Imaging}} and {{DNA Methylation Biomarkers}} to {{Improve}} the {{Classification}} of {{Pulmonary Nodules}}},
  author = {Liang, W. and Liu, Q. and Wang, B. and Tao, J. and Chen, Z. and Zeng, Q. and Fan, J. and He, J.},
  date = {2021-10-01},
  journaltitle = {Journal of Thoracic Oncology},
  shortjournal = {Journal of Thoracic Oncology},
  series = {Abstracts from the 2021 {{World Conference}} on {{Lung Cancer}}},
  volume = {16},
  pages = {S877},
  issn = {1556-0864},
  doi = {10.1016/j.jtho.2021.08.089},
  url = {https://www.sciencedirect.com/science/article/pii/S1556086421025120},
  urldate = {2023-10-30},
  issue = {10, Supplement},
  langid = {english},
  file = {C\:\\Users\\YeCanming\\Documents\\WPS Cloud Files\\200995647\\Zotero文献库\\人工智能\\医学人工智能\\类似文章\\重点是医学结论\\Liang et al_2021_OA16.pdf;D\:\\ProgramFiles\\Zotero\\storage\\7QEWI7A4\\S1556086421025120.html}
}

@online{lianScalingShiftingYour2023,
  title = {Scaling \& {{Shifting Your Features}}: {{A New Baseline}} for {{Efficient Model Tuning}}},
  shorttitle = {Scaling \& {{Shifting Your Features}}},
  author = {Lian, Dongze and Zhou, Daquan and Feng, Jiashi and Wang, Xinchao},
  date = {2023-01-15},
  eprint = {2210.08823},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2210.08823},
  urldate = {2024-08-29},
  abstract = {Existing fine-tuning methods either tune all parameters of the pre-trained model (full fine-tuning), which is not efficient, or only tune the last linear layer (linear probing), which suffers a significant accuracy drop compared to the full fine-tuning. In this paper, we propose a new parameter-efficient fine-tuning method termed as SSF, representing that researchers only need to Scale and Shift the deep Features extracted by a pre-trained model to catch up with the performance of full finetuning. In this way, SSF also surprisingly outperforms other parameter-efficient fine-tuning approaches even with a smaller number of tunable parameters. Furthermore, different from some existing parameter-efficient fine-tuning methods (e.g., Adapter or VPT) that introduce the extra parameters and computational cost in the training and inference stages, SSF only adds learnable parameters during the training stage, and these additional parameters can be merged into the original pre-trained model weights via re-parameterization in the inference phase. With the proposed SSF, our model obtains 2.46\% (90.72\% vs. 88.54\%) and 11.48\% (73.10\% vs. 65.57\%) performance improvement on FGVC and VTAB-1k in terms of Top-1 accuracy compared to the full fine-tuning but only fine-tuning about 0.3M parameters. We also conduct amounts of experiments in various model families (CNNs, Transformers, and MLPs) and datasets. Results on 26 image classification datasets in total and 3 robustness \& out-of-distribution datasets show the effectiveness of SSF. Code is available at https://github.com/dongzelian/SSF.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {⛔ No INSPIRE recid found,Computer Science - Computer Vision and Pattern Recognition},
  file = {D\:\\Zotero文献库\\00人工智能\\00计算机视觉课程\\经典视觉任务的对应设计\\视觉地点定位\\创新点\\PEFT\\Lian et al_2023_Scaling & Shifting Your Features.pdf;D\:\\ProgramFiles\\Zotero\\storage\\PWXXW65N\\2210.html}
}

@article{liCardiovascularRiskFactors2020,
  title = {Cardiovascular Risk Factors in {{China}}: A Nationwide Population-Based Cohort Study},
  shorttitle = {Cardiovascular Risk Factors in {{China}}},
  author = {Li, Xi and Wu, Chaoqun and Lu, Jiapeng and Chen, Bowang and Li, Yichong and Yang, Yang and Hu, Shengshou and Li, Jing},
  date = {2020-12},
  journaltitle = {The Lancet Public Health},
  shortjournal = {The Lancet Public Health},
  volume = {5},
  number = {12},
  pages = {e672-e681},
  issn = {24682667},
  doi = {10.1016/S2468-2667(20)30191-2},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S2468266720301912},
  urldate = {2023-07-29},
  abstract = {Background It is estimated that 4 million deaths are due to cardiovascular diseases each year in China. Comprehensive understanding about modifiable risk factors and how the risk differs across regions is needed to inform public health policies. We aimed to examine the geographical profile of cardiovascular disease risk across China.},
  langid = {english},
  file = {C:\Users\YeCanming\Documents\WPS Cloud Files\200995647\Zotero文献库\人工智能\医学人工智能\类似文章\Li et al_2020_Cardiovascular risk factors in China.pdf}
}

@inproceedings{liMeasuringIntrinsicDimension2018,
  title = {Measuring the {{Intrinsic Dimension}} of {{Objective Landscapes}}},
  author = {Li, Chunyuan and Farkhoor, Heerad and Liu, Rosanne and Yosinski, Jason},
  date = {2018-02-15},
  url = {https://openreview.net/forum?id=ryup8-WCW},
  urldate = {2024-09-17},
  abstract = {Many recently trained neural networks employ large numbers of parameters to achieve good performance. One may intuitively use the number of parameters required as a rough gauge of the difficulty of a problem. But how accurate are such notions? How many parameters are really needed? In this paper we attempt to answer this question by training networks not in their native parameter space, but instead in a smaller, randomly oriented subspace. We slowly increase the dimension of this subspace, note at which dimension solutions first appear, and define this to be the intrinsic dimension of the objective landscape. The approach is simple to implement, computationally tractable, and produces several suggestive conclusions. Many problems have smaller intrinsic dimensions than one might suspect, and the intrinsic dimension for a given dataset varies little across a family of models with vastly different sizes. This latter result has the profound implication that once a parameter space is large enough to solve a problem, extra parameters serve directly to increase the dimensionality of the solution manifold. Intrinsic dimension allows some quantitative comparison of problem difficulty across supervised, reinforcement, and other types of learning where we conclude, for example, that solving the inverted pendulum problem is 100 times easier than classifying digits from MNIST, and playing Atari Pong from pixels is about as hard as classifying CIFAR-10. In addition to providing new cartography of the objective landscapes wandered by parameterized models, the method is a simple technique for constructively obtaining an upper bound on the minimum description length of a solution. A byproduct of this construction is a simple approach for compressing networks, in some cases by more than 100 times.},
  eventtitle = {International {{Conference}} on {{Learning Representations}}},
  langid = {english},
  keywords = {⛔ No INSPIRE recid found},
  file = {D:\Zotero文献库\00人工智能\00机器学习\学习范式\迁移学习\PEFT\理论解释\Li et al_2018_Measuring the Intrinsic Dimension of Objective Landscapes.pdf}
}

@article{liNovelMethodCredit2021,
  title = {A Novel Method for Credit Scoring Based on Feature Transformation and Ensemble Model},
  author = {Li, Hongxiang and Feng, Ao and Lin, Bin and Su, Houcheng and Liu, Zixi and Duan, Xuliang and Pu, Haibo and Wang, Yifei},
  date = {2021-06-04},
  journaltitle = {PeerJ Computer Science},
  shortjournal = {PeerJ Comput. Sci.},
  volume = {7},
  pages = {e579},
  publisher = {PeerJ Inc.},
  issn = {2376-5992},
  doi = {10.7717/peerj-cs.579},
  url = {https://peerj.com/articles/cs-579},
  urldate = {2023-10-19},
  abstract = {Credit scoring is a very critical task for banks and other financial institutions, and it has become an important evaluation metric to distinguish potential defaulting users. In this paper, we propose a credit score prediction method based on feature transformation and ensemble model, which is essentially a cascade approach. The feature transformation process consisting of boosting trees (BT) and auto-encoders (AE) is employed to replace manual feature engineering and to solve the data imbalance problem. For the classification process, this paper designs a heterogeneous ensemble model by weighting the factorization machine (FM) and deep neural networks (DNN), which can efficiently extract low-order intersections and high-order intersections. Comprehensive experiments were conducted on two standard datasets and the results demonstrate that the proposed approach outperforms existing credit scoring models in accuracy.},
  langid = {english},
  file = {C:\Users\YeCanming\Documents\WPS Cloud Files\200995647\Zotero文献库\人工智能\医学人工智能\AI知识查缺补漏\特征工程\Li et al_2021_A novel method for credit scoring based on feature transformation and ensemble.pdf}
}

@online{Linuxshanganquanchuanshuwenjian14geSCPminglingshili,
  title = {Linux上安全传输文件14个SCP命令示例},
  url = {https://zhuanlan.zhihu.com/p/97803435},
  urldate = {2022-11-15},
  abstract = {SCP（安全复制Secure Copy）是 Linux 和 Unix 之类的系统中的命令行工具，用于通过网络安全地跨系统传输文件和目录。当我们使用 scp 命令将文件和目录从本地系统复制到远程系统时，则在后端与远程系统建立了 ssh …},
  langid = {chinese},
  organization = {知乎专栏}
}

@inproceedings{linWordNetElectronicLexical1998,
  title = {{{WordNet}}: {{An Electronic Lexical Database}}},
  shorttitle = {{{WordNet}}},
  author = {Lin, Dekang},
  date = {1998},
  url = {https://www.semanticscholar.org/paper/WordNet%3A-An-Electronic-Lexical-Database-Lin/d53bcbac7ea19173e95d3bd855b998fab765737d},
  urldate = {2024-06-17},
  abstract = {Semantic Scholar extracted view of "WordNet: An Electronic Lexical Database" by Dekang Lin},
  langid = {english},
  keywords = {⛔ No INSPIRE recid found}
}

@inproceedings{liPrefixTuningOptimizingContinuous2021,
  title = {Prefix-{{Tuning}}: {{Optimizing Continuous Prompts}} for {{Generation}}},
  shorttitle = {Prefix-{{Tuning}}},
  booktitle = {Proceedings of the 59th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} and the 11th {{International Joint Conference}} on {{Natural Language Processing}} ({{Volume}} 1: {{Long Papers}})},
  author = {Li, Xiang Lisa and Liang, Percy},
  editor = {Zong, Chengqing and Xia, Fei and Li, Wenjie and Navigli, Roberto},
  date = {2021-08},
  pages = {4582--4597},
  publisher = {Association for Computational Linguistics},
  location = {Online},
  doi = {10.18653/v1/2021.acl-long.353},
  url = {https://aclanthology.org/2021.acl-long.353},
  urldate = {2024-04-21},
  abstract = {Fine-tuning is the de facto way of leveraging large pretrained language models for downstream tasks. However, fine-tuning modifies all the language model parameters and therefore necessitates storing a full copy for each task. In this paper, we propose prefix-tuning, a lightweight alternative to fine-tuning for natural language generation tasks, which keeps language model parameters frozen and instead optimizes a sequence of continuous task-specific vectors, which we call the prefix. Prefix-tuning draws inspiration from prompting for language models, allowing subsequent tokens to attend to this prefix as if it were “virtual tokens”. We apply prefix-tuning to GPT-2 for table-to-text generation and to BART for summarization. We show that by learning only 0.1\% of the parameters, prefix-tuning obtains comparable performance in the full data setting, outperforms fine-tuning in low-data settings, and extrapolates better to examples with topics that are unseen during training.},
  eventtitle = {{{ACL-IJCNLP}} 2021},
  langid = {english},
  keywords = {⛔ No INSPIRE recid found},
  annotation = {352 citations (Crossref) [2024-04-22]},
  file = {D:\Zotero文献库\00人工智能\00机器学习\学习范式\迁移学习\PEFT\prompt\Li_Liang_2021_Prefix-Tuning.pdf}
}

@inproceedings{liuEndToEndMultiTaskLearning2019,
  title = {End-{{To-End Multi-Task Learning With Attention}}},
  booktitle = {2019 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Liu, Shikun and Johns, Edward and Davison, Andrew J.},
  date = {2019-06},
  pages = {1871--1880},
  publisher = {IEEE},
  location = {Long Beach, CA, USA},
  doi = {10.1109/CVPR.2019.00197},
  url = {https://ieeexplore.ieee.org/document/8954221/},
  urldate = {2023-05-28},
  abstract = {We propose a novel multi-task learning architecture, which allows learning of task-specific feature-level attention. Our design, the Multi-Task Attention Network (MTAN), consists of a single shared network containing a global feature pool, together with a soft-attention module for each task. These modules allow for learning of taskspecific features from the global features, whilst simultaneously allowing for features to be shared across different tasks. The architecture can be trained end-to-end and can be built upon any feed-forward neural network, is simple to implement, and is parameter efficient. We evaluate our approach on a variety of datasets, across both image-toimage predictions and image classification tasks. We show that our architecture is state-of-the-art in multi-task learning compared to existing methods, and is also less sensitive to various weighting schemes in the multi-task loss function. Code is available at https://github.com/ lorenmt/mtan.},
  eventtitle = {2019 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  isbn = {978-1-72813-293-8},
  langid = {english},
  file = {D:\ProgramFiles\Zotero\storage\2PQI2KPJ\Liu 等 - 2019 - End-To-End Multi-Task Learning With Attention.pdf}
}

@article{liuFewShotParameterEfficientFineTuning2022,
  title = {Few-{{Shot Parameter-Efficient Fine-Tuning}} Is {{Better}} and {{Cheaper}} than {{In-Context Learning}}},
  author = {Liu, Haokun and Tam, Derek and Muqeeth, Mohammed and Mohta, Jay and Huang, Tenghao and Bansal, Mohit and Raffel, Colin A.},
  date = {2022-12-06},
  journaltitle = {Advances in Neural Information Processing Systems},
  volume = {35},
  pages = {1950--1965},
  url = {https://proceedings.neurips.cc/paper_files/paper/2022/hash/0cde695b83bd186c1fd456302888454c-Abstract-Conference.html},
  urldate = {2024-09-17},
  langid = {english},
  keywords = {⛔ No INSPIRE recid found,Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {D\:\\ProgramFiles\\Zotero\\storage\\IG5F8ZLD\\Liu 等 - 2022 - Few-Shot Parameter-Efficient Fine-Tuning is Better.pdf;D\:\\Zotero文献库\\00人工智能\\00计算机视觉课程\\经典视觉任务的对应设计\\视觉地点定位\\创新点\\PEFT\\Liu et al_2022_Few-Shot Parameter-Efficient Fine-Tuning is Better and Cheaper than In-Context.pdf;D\:\\ProgramFiles\\Zotero\\storage\\GS99N3LR\\2205.html}
}

@online{liuGPTUnderstandsToo2023,
  title = {{{GPT Understands}}, {{Too}}},
  author = {Liu, Xiao and Zheng, Yanan and Du, Zhengxiao and Ding, Ming and Qian, Yujie and Yang, Zhilin and Tang, Jie},
  date = {2023-10-25},
  eprint = {2103.10385},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2103.10385},
  urldate = {2024-05-29},
  abstract = {Prompting a pretrained language model with natural language patterns has been proved effective for natural language understanding (NLU). However, our preliminary study reveals that manual discrete prompts often lead to unstable performance—e.g., changing a single word in the prompt might result in substantial performance drop. We propose a novel method P-Tuning that employs trainable continuous prompt embeddings in concatenation with discrete prompts. Empirically, P-Tuning not only stabilizes training by minimizing the gap between various discrete prompts, but also improves performance by a sizeable margin on a wide range of NLU tasks including LAMA and SuperGLUE. P-Tuning is generally effective for both frozen and tuned language models, under both the fully-supervised and few-shot settings.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {⛔ No INSPIRE recid found,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {D:\ProgramFiles\Zotero\storage\6XUURVUJ\Liu 等 - 2023 - GPT Understands, Too.pdf}
}

@article{liuParameterEfficientOrthogonalFinetuning2023,
  title = {Parameter-{{Efficient Orthogonal Finetuning}} via {{Butterfly Factorization}}},
  author = {Liu, Weiyang and Qiu, Zeju and Feng, Yao and Xiu, Yuliang and Xue, Yuxuan and Yu, Longhui and Feng, Haiwen and Liu, Zhen and Heo, Juyeon and Peng, Songyou and Wen, Yandong and Black, Michael J. and Weller, Adrian and Schölkopf, Bernhard},
  date = {2023},
  journaltitle = {arXiv.org},
  url = {https://www.semanticscholar.org/paper/Parameter-Efficient-Orthogonal-Finetuning-via-Liu-Qiu/46a81d17a4f1b4c4fc3f3215ccfb2fc2921cecaa},
  urldate = {2024-03-21},
  abstract = {Inspired by how the Cooley-Tukey fast Fourier transform algorithm enables efficient information transmission, an efficient orthogonal parameterization using butterfly structures is proposed, creating a novel parameter-efficient finetuning method, called Orthogonal Butterfly (BOFT). Large foundation models are becoming ubiquitous, but training them from scratch is prohibitively expensive. Thus, efficiently adapting these powerful models to downstream tasks is increasingly important. In this paper, we study a principled finetuning paradigm -- Orthogonal Finetuning (OFT) -- for downstream task adaptation. Despite demonstrating good generalizability, OFT still uses a fairly large number of trainable parameters due to the high dimensionality of orthogonal matrices. To address this, we start by examining OFT from an information transmission perspective, and then identify a few key desiderata that enable better parameter-efficiency. Inspired by how the Cooley-Tukey fast Fourier transform algorithm enables efficient information transmission, we propose an efficient orthogonal parameterization using butterfly structures. We apply this parameterization to OFT, creating a novel parameter-efficient finetuning method, called Orthogonal Butterfly (BOFT). By subsuming OFT as a special case, BOFT introduces a generalized orthogonal finetuning framework. Finally, we conduct an extensive empirical study of adapting large vision transformers, large language models, and text-to-image diffusion models to various downstream tasks in vision and language.},
  langid = {english},
  keywords = {⛔ No INSPIRE recid found,Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {D\:\\Zotero文献库\\00人工智能\\00计算机视觉课程\\经典视觉任务的对应设计\\视觉地点定位\\创新点\\PEFT\\Liu et al_2023_Parameter-Efficient Orthogonal Finetuning via Butterfly Factorization.pdf;D\:\\ProgramFiles\\Zotero\\storage\\9IT98XUX\\2311.html;D\:\\ProgramFiles\\Zotero\\storage\\9NVYVRBX\\46a81d17a4f1b4c4fc3f3215ccfb2fc2921cecaa.html}
}

@inproceedings{liuPTuningPromptTuning2022,
  title = {P-{{Tuning}}: {{Prompt Tuning Can Be Comparable}} to {{Fine-tuning Across Scales}} and {{Tasks}}},
  shorttitle = {P-{{Tuning}}},
  booktitle = {Proceedings of the 60th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 2: {{Short Papers}})},
  author = {Liu, Xiao and Ji, Kaixuan and Fu, Yicheng and Tam, Weng and Du, Zhengxiao and Yang, Zhilin and Tang, Jie},
  editor = {Muresan, Smaranda and Nakov, Preslav and Villavicencio, Aline},
  date = {2022-05},
  pages = {61--68},
  publisher = {Association for Computational Linguistics},
  location = {Dublin, Ireland},
  doi = {10.18653/v1/2022.acl-short.8},
  url = {https://aclanthology.org/2022.acl-short.8},
  urldate = {2024-05-12},
  abstract = {Prompt tuning, which only tunes continuous prompts with a frozen language model, substantially reduces per-task storage and memory usage at training. However, in the context of NLU, prior work reveals that prompt tuning does not perform well for normal-sized pretrained models. We also find that existing methods of prompt tuning cannot handle hard sequence labeling tasks, indicating a lack of universality. We present a novel empirical finding that properly optimized prompt tuning can be universally effective across a wide range of model scales and NLU tasks. It matches the performance of finetuning while having only 0.1\%-3\% tuned parameters. Our method P-Tuning v2 is an implementation of Deep Prompt Tuning (CITATION) optimized and adapted for NLU. Given the universality and simplicity of P-Tuning v2, we believe it can serve as an alternative to finetuning and a strong baseline for future research.},
  eventtitle = {{{ACL}} 2022},
  langid = {english},
  keywords = {⛔ No INSPIRE recid found},
  annotation = {141 citations (Crossref) [2024-05-12]},
  file = {D:\Zotero文献库\00人工智能\00机器学习\学习范式\迁移学习\PEFT\prompt\ptuning\Liu et al_2022_P-Tuning.pdf}
}

@online{LobZhuJieOpenJPA,
  title = {@{{Lob}} 注解 - {{OpenJPA}} 教程},
  url = {https://www.hxstrive.com/subject/open_jpa.htm?id=541},
  urldate = {2022-11-14},
  file = {D:\ProgramFiles\Zotero\storage\ZGEMH7BB\open_jpa.html}
}

@article{longDeepLearningbasedFeature2019,
  title = {Deep Learning-Based Feature Engineering for Stock Price Movement Prediction},
  author = {Long, Wen and Lu, Zhichen and Cui, Lingxiao},
  date = {2019-01},
  journaltitle = {Knowledge-Based Systems},
  shortjournal = {Knowledge-Based Systems},
  volume = {164},
  pages = {163--173},
  issn = {09507051},
  doi = {10.1016/j.knosys.2018.10.034},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0950705118305264},
  urldate = {2023-10-19},
  abstract = {Stock price modeling and prediction have been challenging objectives for researchers and speculators because of noisy and non-stationary characteristics of samples. With the growth in deep learning, the task of feature learning can be performed more effectively by purposely designed network. In this paper, we propose a novel end-to-end model named multi-filters neural network (MFNN) specifically for feature extraction on financial time series samples and price movement prediction task. Both convolutional and recurrent neurons are integrated to build the multi-filters structure, so that the information from different feature spaces and market views can be obtained. We apply our MFNN for extreme market prediction and signal-based trading simulation tasks on Chinese stock market index CSI 300. Experimental results show that our network outperforms traditional machine learning models, statistical models, and single-structure(convolutional, recurrent, and LSTM) networks in terms of the accuracy, profitability, and stability.},
  langid = {english},
  file = {C:\Users\YeCanming\Documents\WPS Cloud Files\200995647\Zotero文献库\人工智能\医学人工智能\AI知识查缺补漏\特征工程\Long et al_2019_Deep learning-based feature engineering for stock price movement prediction.pdf}
}

@online{longLearningMultipleTasks2017,
  title = {Learning {{Multiple Tasks}} with {{Multilinear Relationship Networks}}},
  author = {Long, Mingsheng and Cao, Zhangjie and Wang, Jianmin and Yu, Philip S.},
  date = {2017-11-06},
  eprint = {1506.02117},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1506.02117},
  url = {http://arxiv.org/abs/1506.02117},
  urldate = {2023-03-28},
  abstract = {Deep networks trained on large-scale data can learn transferable features to promote learning multiple tasks. Since deep features eventually transition from general to specific along deep networks, a fundamental problem of multi-task learning is how to exploit the task relatedness underlying parameter tensors and improve feature transferability in the multiple task-specific layers. This paper presents Multilinear Relationship Networks (MRN) that discover the task relationships based on novel tensor normal priors over parameter tensors of multiple task-specific layers in deep convolutional networks. By jointly learning transferable features and multilinear relationships of tasks and features, MRN is able to alleviate the dilemma of negative-transfer in the feature layers and under-transfer in the classifier layer. Experiments show that MRN yields state-of-the-art results on three multi-task learning datasets.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning},
  file = {C\:\\Users\\YeCanming\\Documents\\WPS Cloud Files\\200995647\\Zotero文献库\\人工智能\\创新实践\\多任务学习\\Long et al_2017_Learning Multiple Tasks with Multilinear Relationship Networks.pdf;D\:\\ProgramFiles\\Zotero\\storage\\T6F7U8XF\\1506.html}
}

@online{luoEfficientVisualAdaption2023,
  title = {Towards {{Efficient Visual Adaption}} via {{Structural Re-parameterization}}},
  author = {Luo, Gen and Huang, Minglang and Zhou, Yiyi and Sun, Xiaoshuai and Jiang, Guannan and Wang, Zhiyu and Ji, Rongrong},
  date = {2023-03-20},
  eprint = {2302.08106},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2302.08106},
  urldate = {2024-04-15},
  abstract = {Parameter-efficient transfer learning (PETL) is an emerging research spot aimed at inexpensively adapting large-scale pre-trained models to downstream tasks. Recent advances have achieved great success in saving storage costs for various pre-trained models by updating a small number of parameters instead of full tuning. However, we notice that most existing PETL methods still incur non-negligible latency during inference. In this paper, we propose a parameter-efficient and computational friendly adapter for giant vision models, called RepAdapter. Specifically, we first prove that common adaptation modules can also be seamlessly integrated into most giant vision models via our structural re-parameterization, thereby achieving zero-cost during inference. We then investigate the sparse design and effective placement of adapter structure, helping our RepAdaper obtain other advantages in terms of parameter efficiency and performance. To validate RepAdapter, we conduct extensive experiments on 27 benchmark datasets of three vision tasks, i.e., image and video classifications and semantic segmentation. Experimental results show the superior performance and efficiency of RepAdapter than the state-of-the-art PETL methods. For instance, RepAdapter outperforms full tuning by +7.2\% on average and saves up to 25\% training time, 20\% GPU memory, and 94.6\% storage cost of ViT-B/16 on VTAB-1k. The generalization ability of RepAdapter is also well validated by a bunch of vision models. Our source code is released at https://github.com/luogen1996/RepAdapter.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {⛔ No INSPIRE recid found,Computer Science - Computer Vision and Pattern Recognition},
  file = {D\:\\Zotero文献库\\00人工智能\\00机器学习\\学习范式\\迁移学习\\PEFT\\residual\\Luo et al_2023_Towards Efficient Visual Adaption via Structural Re-parameterization.pdf;D\:\\ProgramFiles\\Zotero\\storage\\RH3RH2UF\\2302.html}
}

@online{lyuUnivariateBoundArea2018,
  title = {A {{Univariate Bound}} of {{Area Under ROC}}},
  author = {Lyu, Siwei and Ying, Yiming},
  date = {2018-05-25},
  eprint = {1804.05981},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.1804.05981},
  url = {http://arxiv.org/abs/1804.05981},
  urldate = {2022-12-15},
  abstract = {Area under ROC (AUC) is an important metric for binary classification and bipartite ranking problems. However, it is difficult to directly optimizing AUC as a learning objective, so most existing algorithms are based on optimizing a surrogate loss to AUC. One significant drawback of these surrogate losses is that they require pairwise comparisons among training data, which leads to slow running time and increasing local storage for online learning. In this work, we describe a new surrogate loss based on a reformulation of the AUC risk, which does not require pairwise comparison but rankings of the predictions. We further show that the ranking operation can be avoided, and the learning objective obtained based on this surrogate enjoys linear complexity in time and storage. We perform experiments to demonstrate the effectiveness of the online and batch algorithms for AUC optimization based on the proposed surrogate loss.},
  pubstate = {prepublished},
  version = {2},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\YeCanming\\Documents\\WPS Cloud Files\\200995647\\Zotero文献库\\人工智能\\创新实践\\损失函数\\Lyu_Ying_2018_A Univariate Bound of Area Under ROC.pdf;D\:\\ProgramFiles\\Zotero\\storage\\7UFT2KRM\\1804.html}
}

@inproceedings{maoDoRAEnhancingParameterEfficient2024,
  title = {{{DoRA}}: {{Enhancing Parameter-Efficient Fine-Tuning}} with {{Dynamic Rank Distribution}}},
  shorttitle = {{{DoRA}}},
  booktitle = {Proceedings of the 62nd {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Mao, Yulong and Huang, Kaiyu and Guan, Changhao and Bao, Ganglin and Mo, Fengran and Xu, Jinan},
  editor = {Ku, Lun-Wei and Martins, Andre and Srikumar, Vivek},
  date = {2024-08},
  pages = {11662--11675},
  publisher = {Association for Computational Linguistics},
  location = {Bangkok, Thailand},
  doi = {10.18653/v1/2024.acl-long.626},
  url = {https://aclanthology.org/2024.acl-long.626},
  urldate = {2024-09-26},
  abstract = {Fine-tuning large-scale pre-trained models is inherently a resource-intensive task. While it can enhance the capabilities of the model, it also incurs substantial computational costs, posing challenges to the practical application of downstream tasks. Existing parameter-efficient fine-tuning (PEFT) methods such as Low-Rank Adaptation (LoRA) rely on a bypass framework that ignores the differential parameter budget requirements across weight matrices, which may lead to suboptimal fine-tuning outcomes. To address this issue, we introduce the Dynamic Low-Rank Adaptation (DoRA) method. DoRA decomposes high-rank LoRA layers into structured single-rank components, allowing for dynamic pruning of parameter budget based on their importance to specific tasks during training, which makes the most of the limited parameter budget. Experimental results demonstrate that DoRA can achieve competitive performance compared with LoRA and full model fine-tuning, and outperform various strong baselines with the same storage parameter budget. Our code is available at [github](https://github.com/MIkumikumi0116/DoRA)},
  eventtitle = {{{ACL}} 2024},
  langid = {english},
  keywords = {⛔ No INSPIRE recid found},
  annotation = {0 citations (Crossref) [2024-09-27]},
  file = {D:\Zotero文献库\00人工智能\00机器学习\学习范式\迁移学习\PEFT\基于参数\lora类\Mao et al_2024_DoRA.pdf}
}

@online{maoUniPELTUnifiedFramework2022,
  title = {{{UniPELT}}: {{A Unified Framework}} for {{Parameter-Efficient Language Model Tuning}}},
  shorttitle = {{{UniPELT}}},
  author = {Mao, Yuning and Mathias, Lambert and Hou, Rui and Almahairi, Amjad and Ma, Hao and Han, Jiawei and Yih, Wen-tau and Khabsa, Madian},
  date = {2022-09-04},
  eprint = {2110.07577},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2110.07577},
  url = {http://arxiv.org/abs/2110.07577},
  urldate = {2024-03-21},
  abstract = {Recent parameter-efficient language model tuning (PELT) methods manage to match the performance of fine-tuning with much fewer trainable parameters and perform especially well when training data is limited. However, different PELT methods may perform rather differently on the same task, making it nontrivial to select the most appropriate method for a specific task, especially considering the fast-growing number of new PELT methods and tasks. In light of model diversity and the difficulty of model selection, we propose a unified framework, UniPELT, which incorporates different PELT methods as submodules and learns to activate the ones that best suit the current data or task setup via gating mechanism. On the GLUE benchmark, UniPELT consistently achieves 1\textasciitilde 4\% gains compared to the best individual PELT method that it incorporates and even outperforms fine-tuning under different setups. Moreover, UniPELT generally surpasses the upper bound that takes the best performance of all its submodules used individually on each task, indicating that a mixture of multiple PELT methods may be inherently more effective than single methods.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {⛔ No INSPIRE recid found,Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {D\:\\Zotero文献库\\00人工智能\\00计算机视觉课程\\经典视觉任务的对应设计\\视觉地点定位\\创新点\\PEFT\\Mao et al_2022_UniPELT.pdf;D\:\\ProgramFiles\\Zotero\\storage\\EQV6QRMR\\2110.html}
}

@online{Matlablunwenchatuhuizhimobandi25qiZhenzhuangtu,
  title = {Matlab论文插图绘制模板第25期—针状图},
  url = {https://zhuanlan.zhihu.com/p/512523275},
  urldate = {2023-03-07},
  abstract = {在之前的文章中，分享了 Matlab折线图的绘制模板： 柱状图的绘制模板： 三维柱状图的绘制模板： 散点图的绘制模板： 三维散点图的绘制模板： 热图的绘制模板：这次再来分享一下 针状图的绘制模板。先来看一下成品…},
  langid = {chinese},
  organization = {知乎专栏},
  file = {D:\ProgramFiles\Zotero\storage\MAQGYRHE\512523275.html}
}

@inreference{MatrixCalculus2022,
  title = {Matrix Calculus},
  booktitle = {Wikipedia},
  date = {2022-05-29T14:55:26Z},
  url = {https://en.wikipedia.org/w/index.php?title=Matrix_calculus&oldid=1090443970},
  urldate = {2022-10-07},
  abstract = {In mathematics, matrix calculus is a specialized notation for doing multivariable calculus, especially over spaces of matrices.  It collects the various partial derivatives of a single function with respect to many variables, and/or of a multivariate function with respect to a single variable, into vectors and matrices that can be treated as single entities.  This greatly simplifies operations such as finding the maximum or minimum of a multivariate function and solving systems of differential equations. The notation used here is commonly used in statistics and engineering, while the tensor index notation is preferred in physics. Two competing notational conventions split the field of matrix calculus into two separate groups. The two groups can be distinguished by whether they write the derivative of a scalar with respect to a vector as a column vector or a row vector. Both of these conventions are possible even when the common assumption is made that vectors should be treated as column vectors when combined with matrices (rather than row vectors). A single convention can be somewhat standard throughout a single field that commonly uses matrix calculus (e.g. econometrics, statistics, estimation theory and machine learning). However, even within a given field different authors can be found using competing conventions. Authors of both groups often write as though their specific convention were standard. Serious mistakes can result when combining results from different authors without carefully verifying that compatible notations have been used. Definitions of these two conventions and comparisons between them are collected in the layout conventions section.},
  langid = {english},
  annotation = {Page Version ID: 1090443970},
  file = {D:\ProgramFiles\Zotero\storage\XG8H8LFC\Matrix_calculus.html}
}

@online{maxciciAnswerUnderstandingAlpha2023,
  title = {Answer to "{{Understanding}} Alpha Parameter Tuning in {{LORA}} Paper"},
  author = {Maxcici, Helena},
  date = {2023-09-09},
  url = {https://datascience.stackexchange.com/a/123630},
  urldate = {2024-09-09},
  langid = {english},
  organization = {Data Science Stack Exchange},
  keywords = {⛔ No INSPIRE recid found},
  file = {D:\ProgramFiles\Zotero\storage\T3QRL7RW\understanding-alpha-parameter-tuning-in-lora-paper.html}
}

@article{mehtaEnsembleTransferLearning2024,
  title = {Ensemble of Transfer Learning and Light-Weight Convolutional Neural Network Model for an Effective Ear Recognition System},
  author = {Mehta, Ravishankar and Singh, Koushlendra Kumar},
  date = {2024-02-01},
  journaltitle = {Evolving Systems},
  shortjournal = {Evolving Systems},
  volume = {15},
  number = {1},
  pages = {115--131},
  issn = {1868-6486},
  doi = {10.1007/s12530-023-09561-6},
  url = {https://doi.org/10.1007/s12530-023-09561-6},
  urldate = {2024-04-14},
  abstract = {The transfer learning and deep convolutional neural network-based recognition models have been merged to develop an efficient model in the contemporary time. The ear recognition system has major challenges in terms of the number of sample images, illumination changes, and other environmental challenges. To make the recognition system free from these challenges, the authors utilized the strength of both the existing three pre-trained models (e.g. VGG19, VGG16, DenseNet201) and three simple light-weight Convolutional Neural (CNN) models. The proposed method encompasses the transfer learning as well as simple deep convolution neural network model to design the ensemble model. The first three models are based on the transfer learning approach which uses VGG19, VGG16, and DenseNet201 whereas the last three models are light-weight Convolution Neural Networks (CNNs) that consist of comparatively less number of convolutional layers. The use of transfer learning in the proposed approach overcomes the limitation of small datasets whereas the use of lightweight CNN models reduces the overhead of the training time of the model. The proposed model is validated with the IITD-II ear dataset in which we achieved recognition accuracy of 95.96\% and 93.08\% through weighted ensemble and average ensemble techniques. The combined approach of transfer learning and deep CNN show improvements in performance accuracy by 2–4\% when compared to individual models. It achieves a precision of 96.74, recall of 96.37, and f-score of 95.96 using the weighted ensemble method which is an improvement over the other state-of-the-art methods.},
  langid = {english},
  keywords = {⛔ No INSPIRE recid found,Accuracy,Biometrics,Deep learning,Ear recognition,Ensemble,ROC},
  annotation = {0 citations (Crossref) [2024-04-14]},
  file = {D:\Zotero文献库\00人工智能\00机器学习\学习范式\迁移学习\PEFT\Mehta_Singh_2024_Ensemble of transfer learning and light-weight convolutional neural network.pdf}
}

@online{misraCrossstitchNetworksMultitask2016a,
  title = {Cross-Stitch {{Networks}} for {{Multi-task Learning}}},
  author = {Misra, Ishan and Shrivastava, Abhinav and Gupta, Abhinav and Hebert, Martial},
  date = {2016-04-12},
  eprint = {1604.03539},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1604.03539},
  url = {http://arxiv.org/abs/1604.03539},
  urldate = {2022-11-30},
  abstract = {Multi-task learning in Convolutional Networks has displayed remarkable success in the field of recognition. This success can be largely attributed to learning shared representations from multiple supervisory tasks. However, existing multi-task approaches rely on enumerating multiple network architectures specific to the tasks at hand, that do not generalize. In this paper, we propose a principled approach to learn shared representations in ConvNets using multi-task learning. Specifically, we propose a new sharing unit: "cross-stitch" unit. These units combine the activations from multiple networks and can be trained end-to-end. A network with cross-stitch units can learn an optimal combination of shared and task-specific representations. Our proposed method generalizes across multiple tasks and shows dramatically improved performance over baseline methods for categories with few training examples.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {C\:\\Users\\YeCanming\\Documents\\WPS Cloud Files\\200995647\\Zotero文献库\\人工智能\\创新实践\\多任务学习\\Misra et al_2016_Cross-stitch Networks for Multi-task Learning.pdf;D\:\\ProgramFiles\\Zotero\\storage\\WQZ5P658\\1604.html}
}

@article{mitraHyperHeuristicAlgorithmFinding2013,
  title = {Hyper-{{Heuristic Algorithm}} for {{Finding Efficient Features}} in {{Diagnose}} of {{Lung Cancer Disease}}},
  author = {Mitra, Montazeri and Baghshah, M. and Enhesari, A.},
  date = {2013-10-01},
  journaltitle = {ArXiv},
  url = {https://www.semanticscholar.org/paper/Hyper-Heuristic-Algorithm-for-Finding-Efficient-in-Mitra-Baghshah/78cb153c0ba409a92810bc9a9910356bb834ce44},
  urldate = {2023-07-29},
  abstract = {Background: Lung cancer was known as primary cancers and the survival rate of cancer is about 15\%. Early detection of lung cancer is the leading factor in survival rate. All symptoms (features) of lung cancer do not appear until the cancer spreads to other areas. It needs an accurate early detection of lung cancer, for increasing the survival rate. For accurate detection, it need characterizes efficient features and delete redundancy features among all features. Feature selection is the problem of selecting informative features among all features. Materials and Methods: Lung cancer database consist of 32 patient records with 57 features. This database collected by Hong and Youngand indexed in the University of California Irvine repository. Experimental contents include the extracted from the clinical data and X-ray data, etc. The data described 3 types of pathological lung cancers and all features are taking an integer value 0-3. In our study, new method is proposed for identify efficient features of lung cancer. It is based on Hyper-Heuristic. Results: We obtained an accuracy of 80.63\% using reduced 11 feature set. The proposed method compare to the accuracy of 5 machine learning feature selections. The accuracy of these 5 methods are 60.94, 57.81, 68.75, 60.94 and 68.75. Conclusions: The proposed method has better performance with the highest level of accuracy. Therefore, the proposed model is recommended for identifying an efficient symptom of Disease. These finding are very important in health research, particularly in allocation of medical resources for patients who predicted as high-risks},
  langid = {english},
  file = {C:\Users\YeCanming\Documents\WPS Cloud Files\200995647\Zotero文献库\人工智能\医学人工智能\UCI数据集论文\Mitra et al_2013_Hyper-Heuristic Algorithm for Finding Efficient Features in Diagnose of Lung.pdf}
}

@online{MixtureofExpertsMoEJingDianLunWenYiLan,
  title = {Mixture-of-{{Experts}} ({{MoE}}) 经典论文一览},
  url = {https://zhuanlan.zhihu.com/p/542465517},
  urldate = {2023-03-29},
  abstract = {最近接触到 Mixture-of-Experts (MoE) 这个概念，才发现这是一个已经有30多年历史、至今依然在被广泛应用的技术，所以读了相关的几篇经典论文，在这里总结一下。1. Adaptive mixtures of local experts, Neural Co…},
  langid = {english},
  organization = {知乎专栏},
  file = {D:\ProgramFiles\Zotero\storage\5BAIE6IV\542465517.html}
}

@online{MoFanShiMeShiL1L22017,
  type = {知乎专栏文章},
  title = {什么是 {{L1}}/{{L2}} 正则化 ({{Regularization}})},
  author = {莫烦},
  date = {2017-03-11T11:55:05},
  url = {https://zhuanlan.zhihu.com/p/25707761},
  urldate = {2023-07-16},
  abstract = {什么是 L1/L2 正则化 (Regularization) - 来自知乎专栏「莫烦」，作者: 莫烦 https://zhuanlan.zhihu.com/p/25707761},
  langid = {english},
  organization = {莫烦},
  keywords = {机器学习,深度学习（Deep Learning）},
  annotation = {赞数:484;},
  file = {D:\ProgramFiles\Zotero\storage\NQPEKJS2\25707761.html}
}

@inproceedings{moFocusingIntracranialAneurysm2023,
  title = {Focusing {{Intracranial Aneurysm Lesion Segmentation}} by {{Graph Mask2Former}} with {{Local Refinement}} in {{DSA Images}}},
  booktitle = {2023 {{IEEE International Conference}} on {{Bioinformatics}} and {{Biomedicine}} ({{BIBM}})},
  author = {Mo, Yancheng and Chen, Yanlin and Hu, Yan and Xu, Jiongfu and Wang, Hao and Dai, Limeng and Liu, Jiang},
  date = {2023-12},
  pages = {899--903},
  issn = {2156-1133},
  doi = {10.1109/BIBM58861.2023.10385276},
  url = {https://ieeexplore.ieee.org/document/10385276},
  urldate = {2024-03-13},
  abstract = {Intracranial Aneurysm (IA) lesion segmentation is significant for IA treatment, which is one of the high death rate and deformity cerebrovascular diseases. Segmenting the IA lesions accurately is still challenging in digital subtraction angiography (DSA) images due to blurred boundaries, imaging noise, and intracranial vascular morphologies. In this paper, we are the first time to propose a novel instance segmentation network architecture, Graph Mask2Former, to segment IA lesions automatically based on DSA images. Specifically, we apply a graph convolution module to reassign label information, aiming to adjust the confidence weight of error instances adaptively. Furthermore, we design a local refinement module to refine the coarse mask output. The extensive experiments on the clinical IA- DSA and LiTS datasets show that our method outperforms recent state-of-the-art methods. This paper also provides the visual analysis to explain the inherent behavior of our method.},
  eventtitle = {2023 {{IEEE International Conference}} on {{Bioinformatics}} and {{Biomedicine}} ({{BIBM}})},
  langid = {english},
  keywords = {⛔ No INSPIRE recid found,Aneurysm,Convolution,DSA image,Focusing,Instance segmentation,Intracranial aneurysm,Morphology,Network architecture,Visualization},
  file = {D:\Zotero文献库\00人工智能\00机器学习课程\医学人工智能\类似文章\Mo et al_2023_Focusing Intracranial Aneurysm Lesion Segmentation by Graph Mask2Former with.pdf}
}

@article{mostafaExaminingMultipleFeature2019,
  title = {Examining Multiple Feature Evaluation and Classification Methods for Improving the Diagnosis of {{Parkinson}}’s Disease},
  author = {Mostafa, Salama A. and Mustapha, Aida and Mohammed, Mazin Abed and Hamed, Raed Ibraheem and Arunkumar, N. and Abd Ghani, Mohd Khanapi and Jaber, Mustafa Musa and Khaleefah, Shihab Hamad},
  date = {2019-05},
  journaltitle = {Cognitive Systems Research},
  shortjournal = {Cognitive Systems Research},
  volume = {54},
  pages = {90--99},
  issn = {13890417},
  doi = {10.1016/j.cogsys.2018.12.004},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S1389041718308933},
  urldate = {2023-07-30},
  abstract = {An accurate diagnosis of Parkinson’s disease by specialists involves many neurological, psychological and physical examinations. The specialists investigate a number of symptoms and signs when examining the nervous system conditions of a person. The diagnosis involves reviewing the medical history and genetic factor of the person. The recent diagnosis methodology to Parkinson’s disease relies on voice disorders analysis. This methodology entails extracting feature sets of a recorded person’s voice then utilizing a machine learning technique to identify the healthy and Parkinson’s cases from the voice. This paper attempts to improve the diagnoses of Parkinson’s disease by testing multiple feature evaluation and classification machine learning methods based on the voice disorders analysis. The aim of this paper is to find the optimal solution to the problem by (i) proposing a new Multiple Feature Evaluation Approach (MFEA) of a multi-agent system (ii) implementing five independent classification schemas which are Decision Tree, Na¨ıve Bayes, Neural Network, Random Forests, and Support Vector Machine on the Parkinson’s diagnosis before and after applying the MFEA, and (iii) evaluating the diagnosis accuracy of the results. The methodology of the tests encompasses 10-fold cross-validation to evaluate the learning of methods and track variation in their performance. The test results show that the MFEA of the multi-agent system finds the best set of features and improves the performance of the classifiers. The average rate of improvement in the diagnostic accuracy of the classifiers are Decision Tree 10.51\%, Na¨ıve Bayes 15.22\%, Neural Network 9.19\%, Random Forests 12.75\%, and Support Vector Machine 9.13\%. These results show that the MFEA makes a significant improvement to the classifiers’ diagnosis results.},
  langid = {english},
  file = {D:\ProgramFiles\Zotero\storage\9CIF8R7R\Mostafa 等 - 2019 - Examining multiple feature evaluation and classifi.pdf}
}

@inproceedings{muellerFasterSimplerMore2020,
  title = {Faster, {{Simpler}}, {{More Accurate}}: {{Practical Automated Machine Learning}} with {{Tabular}}, {{Text}}, and {{Image Data}}},
  shorttitle = {Faster, {{Simpler}}, {{More Accurate}}},
  booktitle = {Proceedings of the 26th {{ACM SIGKDD International Conference}} on {{Knowledge Discovery}} \& {{Data Mining}}},
  author = {Mueller, Jonas and Shi, Xingjian and Smola, Alexander},
  date = {2020-08-23},
  pages = {3509--3510},
  publisher = {ACM},
  location = {Virtual Event CA USA},
  doi = {10.1145/3394486.3406706},
  url = {https://dl.acm.org/doi/10.1145/3394486.3406706},
  urldate = {2023-08-23},
  abstract = {Automated machine learning (AutoML) offers the promise of translating raw data into accurate predictions with just a few lines of code. Rather than relying on human time/effort and manual experimentation, models can be improved by simply letting the AutoML system run for more time. In this hands-on tutorial, we demonstrate fundamental techniques that enable powerful AutoML. We consider standard supervised learning tasks on various types of data including tables, text, images, as well as multi-modal data comprised of multiple types. Rather than technical descriptions of how individual ML models work, we emphasize how to best use models within an overall ML pipeline that takes in raw training data and outputs predictions for test data. A major focus of our tutorial is on automating deep learning, a class of powerful techniques that are cumbersome to manage manually. Despite this, hardly any educational material describes their successful automation. Each topic covered in the tutorial is accompanied by a hands-on Jupyter notebook that implements best practices (which will be available on Github before and after the tutorial). Most of this code is adopted from AutoGluon (autogluon.mxnet.io), a recent AutoML toolkit for automated deep learning that is both state-of-the-art and easy-to-use.},
  eventtitle = {{{KDD}} '20: {{The}} 26th {{ACM SIGKDD Conference}} on {{Knowledge Discovery}} and {{Data Mining}}},
  isbn = {978-1-4503-7998-4},
  langid = {english},
  file = {D:\ProgramFiles\Zotero\storage\3STTPSUU\Mueller 等 - 2020 - Faster, Simpler, More Accurate Practical Automate.pdf}
}

@online{nehcAnswerCanSomeone2020,
  title = {Answer to "{{Can}} Someone Give a Good Math/Stats Explanation as to What the Parameter Var\_smoothing Does for {{GaussianNB}} in Scikit Learn?"},
  shorttitle = {Answer to "{{Can}} Someone Give a Good Math/Stats Explanation as to What the Parameter Var\_smoothing Does for {{GaussianNB}} in Scikit Learn?},
  author = {Nehc, Y.},
  date = {2020-05-30},
  url = {https://stackoverflow.com/a/62098032/16306773},
  urldate = {2022-11-18},
  organization = {Stack Overflow}
}

@article{neymanMathematicalStatisticsProbability1962,
  title = {\emph{Mathematical }{{\emph{Statistics}}}\emph{ and }{{\emph{Probability}}}},
  author = {Neyman, Jerzy and Kac, Mark},
  date = {1962-11},
  journaltitle = {Physics Today},
  shortjournal = {Physics Today},
  volume = {15},
  number = {11},
  pages = {64--68},
  issn = {0031-9228, 1945-0699},
  doi = {10.1063/1.3057865},
  url = {http://physicstoday.scitation.org/doi/10.1063/1.3057865},
  urldate = {2023-05-03},
  langid = {english},
  file = {D:\ProgramFiles\Zotero\storage\443G3A9A\Neyman 和 Kac - 1962 - Mathematical Statistics and Probability.pdf}
}

@online{novackCHiLSZeroShotImage2023,
  title = {{{CHiLS}}: {{Zero-Shot Image Classification}} with {{Hierarchical Label Sets}}},
  shorttitle = {{{CHiLS}}},
  author = {Novack, Zachary and McAuley, Julian and Lipton, Zachary C. and Garg, Saurabh},
  date = {2023-05-31},
  eprint = {2302.02551},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2302.02551},
  urldate = {2023-06-28},
  abstract = {Open vocabulary models (e.g. CLIP) have shown strong performance on zero-shot classification through their ability generate embeddings for each class based on their (natural language) names. Prior work has focused on improving the accuracy of these models through prompt engineering or by incorporating a small amount of labeled downstream data (via finetuning). However, there has been little focus on improving the richness of the class names themselves, which can pose issues when class labels are coarselydefined and uninformative. We propose Classification with Hierarchical Label Sets (or CHiLS), an alternative strategy for zero-shot classification specifically designed for datasets with implicit semantic hierarchies. CHiLS proceeds in three steps: (i) for each class, produce a set of subclasses, using either existing label hierarchies or by querying GPT-3; (ii) perform the standard zero-shot CLIP procedure as though these subclasses were the labels of interest; (iii) map the predicted subclass back to its parent to produce the final prediction. Across numerous datasets with underlying hierarchical structure, CHiLS leads to improved accuracy in situations both with and without ground-truth hierarchical information. CHiLS is simple to implement within existing CLIP pipelines and requires no additional training cost. Code is available at: https://github.com/acmi-lab/CHILS.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {D:\ProgramFiles\Zotero\storage\GEH4AAUX\Novack 等 - 2023 - CHiLS Zero-Shot Image Classification with Hierarc.pdf}
}

@online{NumpyZhongstackHstackVstack,
  title = {Numpy中stack()，hstack()，vstack()函数详解\_neu\_张康的博客-{{CSDN博客}}\_hstack},
  url = {https://blog.csdn.net/csdn15698845876/article/details/73380803},
  urldate = {2022-09-29},
  file = {D:\ProgramFiles\Zotero\storage\XTNGTW47\73380803.html}
}

@article{ogunleyeXGBoostModelChronic2020,
  title = {{{XGBoost Model}} for {{Chronic Kidney Disease Diagnosis}}},
  author = {Ogunleye, Adeola and Wang, Qing-Guo},
  date = {2020-11-01},
  journaltitle = {IEEE/ACM Transactions on Computational Biology and Bioinformatics},
  shortjournal = {IEEE/ACM Trans. Comput. Biol. and Bioinf.},
  volume = {17},
  number = {6},
  pages = {2131--2140},
  issn = {1545-5963, 1557-9964, 2374-0043},
  doi = {10.1109/TCBB.2019.2911071},
  url = {https://ieeexplore.ieee.org/document/8693581/},
  urldate = {2023-03-04},
  abstract = {Chronic Kidney Disease (CKD) is a menace that is affecting 10\% of the world population and 15\% of the South African population. The early and cheap diagnosis of this disease with accuracy and reliability will save 20,000 lives in South Africa per year. Scientists are developing smart solutions with Artificial Intelligence (AI). In this paper, several typical and recent AI algorithms are studied in the context of CKD and the extreme gradient boosting (XGBoost) is chosen as our base model for its high performance. Then, the model is optimized and the optimal full model trained on all the features achieves a testing accuracy, sensitivity and specificity of 1.000, 1.000 and 1.000, respectively. Note that, to cover the widest range of people, the time and monetary costs of CKD diagnosis have to be minimized with fewest patient tests. Thus the reduced model using fewer features is desirable while it should still maintain high performance. To this end, the set-theory based rule is presented which combines a few feature selection methods with their collective strengths. The reduced model using about a half of the original full features performs better than the models based on individual feature selection methods and achieves accuracy, sensitivity and specificity of 1.000, 1.000 and 1.000, respectively.},
  langid = {english},
  file = {C:\Users\YeCanming\Documents\WPS Cloud Files\200995647\Zotero文献库\人工智能\医学人工智能\Ogunleye_Wang_2020_XGBoost Model for Chronic Kidney Disease Diagnosis.pdf}
}

@online{openbmb42PromptLearningHeDeltaTuningBeiJingHeGaiLan,
  title = {4-2 {{Prompt-Learning和Delta-Tuning--背景和概览}}\_哔哩哔哩\_bilibili},
  author = {OpenBMB},
  url = {https://www.bilibili.com/video/BV1UG411p7zv/},
  urldate = {2023-06-21},
  abstract = {4-2 Prompt-Learning和Delta-Tuning--背景和概览是【清华NLP】刘知远团队大模型公开课全网首发｜带你从入门到实战的第42集视频，该合集共计137集，视频收藏或关注UP主，及时了解更多相关视频内容。},
  langid = {english}
}

@online{openbmbBoDongDaMoXingDeQinXianDeltaTuning2023,
  type = {知乎专栏文章},
  title = {拨动大模型的琴弦｜{{Delta Tuning}} 成果登上 {{Nature子刊封面}}！},
  author = {OpenBMB},
  date = {2023-03-24T10:11:04},
  url = {https://zhuanlan.zhihu.com/p/616766413},
  urldate = {2024-04-06},
  abstract = {拨动大模型的琴弦｜Delta Tuning 成果登上 Nature子刊封面！ - 来自知乎专栏，作者: OpenBMB https://zhuanlan.zhihu.com/p/616766413},
  langid = {english},
  organization = {回答},
  keywords = {《自然》（Nature）,⛔ No INSPIRE recid found,学术,科研},
  annotation = {赞数:13;},
  file = {D\:\\ProgramFiles\\Zotero\\storage\\2EJSNQGU\\616766413.html;D\:\\ProgramFiles\\Zotero\\storage\\7ZAJ8RZP\\616766413.html}
}

@article{orlenkoModelSelectionMetabolomics2020,
  title = {Model Selection for Metabolomics: Predicting Diagnosis of Coronary Artery Disease Using Automated Machine Learning},
  shorttitle = {Model Selection for Metabolomics},
  author = {Orlenko, Alena and Kofink, Daniel and Lyytikäinen, Leo-Pekka and Nikus, Kjell and Mishra, Pashupati and Kuukasjärvi, Pekka and Karhunen, Pekka J and Kähönen, Mika and Laurikka, Jari O and Lehtimäki, Terho and Asselbergs, Folkert W and Moore, Jason H},
  editor = {Kelso., Janet},
  date = {2020-03-01},
  journaltitle = {Bioinformatics},
  volume = {36},
  number = {6},
  pages = {1772--1778},
  issn = {1367-4803, 1367-4811},
  doi = {10.1093/bioinformatics/btz796},
  url = {https://academic.oup.com/bioinformatics/article/36/6/1772/5614811},
  urldate = {2023-08-23},
  abstract = {Motivation: Selecting the optimal machine learning (ML) model for a given dataset is often challenging. Automated ML (AutoML) has emerged as a powerful tool for enabling the automatic selection of ML methods and parameter settings for the prediction of biomedical endpoints. Here, we apply the tree-based pipeline optimization tool (TPOT) to predict angiographic diagnoses of coronary artery disease (CAD). With TPOT, ML models are represented as expression trees and optimal pipelines discovered using a stochastic search method called genetic programing. We provide some guidelines for TPOT-based ML pipeline selection and optimization-based on various clinical phenotypes and high-throughput metabolic profiles in the Angiography and Genes Study (ANGES).},
  langid = {english},
  file = {D:\ProgramFiles\Zotero\storage\JANEFM3F\Orlenko 等 - 2020 - Model selection for metabolomics predicting diagn.pdf}
}

@article{panDomainAdaptationTransfer2011,
  title = {Domain {{Adaptation}} via {{Transfer Component Analysis}}},
  author = {Pan, Sinno Jialin and Tsang, Ivor W. and Kwok, James T. and Yang, Qiang},
  date = {2011-02},
  journaltitle = {IEEE Transactions on Neural Networks},
  shortjournal = {IEEE Trans. Neural Netw.},
  volume = {22},
  number = {2},
  pages = {199--210},
  issn = {1045-9227, 1941-0093},
  doi = {10.1109/TNN.2010.2091281},
  url = {http://ieeexplore.ieee.org/document/5640675/},
  urldate = {2023-08-28},
  abstract = {Domain adaptation solves a learning problem in a target domain by utilizing the training data in a different but related source domain. Intuitively, discovering a good feature representation across domains is crucial. In this paper, we propose to find such a representation through a new learning method, transfer component analysis (TCA), for domain adaptation. TCA tries to learn some transfer components across domains in a Reproducing Kernel Hilbert Space (RKHS) using Maximum Mean Discrepancy (MMD). In the subspace spanned by these transfer components, data distributions in different domains are close to each other. As a result, with the new representations in this subspace, we can apply standard machine learning methods to train classifiers or regression models in the source domain for use in the target domain. The main contribution of our work is that we propose a novel feature representation in which to perform domain adaptation via a new parametric kernel using feature extraction methods, which can dramatically minimize the distance between domain distributions by projecting data onto the learned transfer components. Furthermore, our approach can handle large datsets and naturally lead to out-of-sample generalization. The effectiveness and efficiency of our approach in are verified by experiments on two real-world applications: cross-domain indoor WiFi localization and cross-domain text classification.},
  langid = {english},
  file = {D:\ProgramFiles\Zotero\storage\KYJBVWN8\Pan 等 - 2011 - Domain Adaptation via Transfer Component Analysis.pdf}
}

@article{panSurveyTransferLearning2010,
  title = {A {{Survey}} on {{Transfer Learning}}},
  author = {Pan, Sinno Jialin and Yang, Qiang},
  date = {2010-10},
  journaltitle = {IEEE Transactions on Knowledge and Data Engineering},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  volume = {22},
  number = {10},
  pages = {1345--1359},
  issn = {1041-4347},
  doi = {10.1109/TKDE.2009.191},
  url = {http://ieeexplore.ieee.org/document/5288526/},
  urldate = {2022-08-08},
  abstract = {A major assumption in many machine learning and data mining algorithms is that the training and future data must be in the same feature space and have the same distribution. However, in many real-world applications, this assumption may not hold. For example, we sometimes have a classification task in one domain of interest, but we only have sufficient training data in another domain of interest, where the latter data may be in a different feature space or follow a different data distribution. In such cases, knowledge transfer, if done successfully, would greatly improve the performance of learning by avoiding much expensive data labeling efforts. In recent years, transfer learning has emerged as a new learning framework to address this problem. This survey focuses on categorizing and reviewing the current progress on transfer learning for classification, regression and clustering problems. In this survey, we discuss the relationship between transfer learning and other related machine learning techniques such as domain adaptation, multitask learning and sample selection bias, as well as co-variate shift. We also explore some potential future issues in transfer learning research.},
  langid = {english},
  file = {C:\Users\YeCanming\Documents\WPS Cloud Files\200995647\Zotero文献库\人工智能\Pan_Yang_2010_A Survey on Transfer Learning.pdf}
}

@online{Past_key_valuesPast_key_valuesIssue,
  title = {Past\_key\_values=past\_key\_values · {{Issue}} \#14 · {{THUDM}}/{{P-tuning-v2}}},
  url = {https://github.com/THUDM/P-tuning-v2/issues/14},
  urldate = {2024-05-29},
  abstract = {Hi, I found in the official huggingface documentation that this “past\_key\_values=past\_key\_values” parameter is only useful for accelerating precompute. Can you please explain which is p-tuning v2, ...},
  langid = {english},
  organization = {GitHub},
  keywords = {⛔ No INSPIRE recid found},
  file = {D:\ProgramFiles\Zotero\storage\T4LZRWUU\14.html}
}

@online{PDFConvergenceAlternating,
  title = {({{PDF}}) {{Convergence}} of Alternating Optimization},
  url = {https://www.researchgate.net/publication/262318598_Convergence_of_alternating_optimization},
  urldate = {2023-03-22},
  file = {D:\ProgramFiles\Zotero\storage\9NWAGLDL\262318598_Convergence_of_alternating_optimization.html}
}

@online{PeftHuggingfaceDaMoXingJiaZaiDuoGeLoRABingSuiShiQieHuanQxAIRobotDeBoKeCSDNBoKe,
  title = {【peft】{{huggingface大模型加载多个LoRA并随时切换}}\_{{QxAIRobot的博客-CSDN博客}}},
  url = {https://blog.csdn.net/liuqixuan1994/article/details/130664198?ops_request_misc=&request_id=&biz_id=102&spm=1018.2226.3001.4187},
  urldate = {2023-06-23},
  langid = {english},
  file = {D:\ProgramFiles\Zotero\storage\XB7QUFTD\130664198.html}
}

@online{pfeifferAdapterHubFrameworkAdapting2020,
  title = {{{AdapterHub}}: {{A Framework}} for {{Adapting Transformers}}},
  shorttitle = {{{AdapterHub}}},
  author = {Pfeiffer, Jonas and Rücklé, Andreas and Poth, Clifton and Kamath, Aishwarya and Vulić, Ivan and Ruder, Sebastian and Cho, Kyunghyun and Gurevych, Iryna},
  date = {2020-10-06},
  eprint = {2007.07779},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2007.07779},
  url = {http://arxiv.org/abs/2007.07779},
  urldate = {2024-05-10},
  abstract = {The current modus operandi in NLP involves downloading and fine-tuning pre-trained models consisting of millions or billions of parameters. Storing and sharing such large trained models is expensive, slow, and time-consuming, which impedes progress towards more general and versatile NLP methods that learn from and for many tasks. Adapters -- small learnt bottleneck layers inserted within each layer of a pre-trained model -- ameliorate this issue by avoiding full fine-tuning of the entire model. However, sharing and integrating adapter layers is not straightforward. We propose AdapterHub, a framework that allows dynamic "stitching-in" of pre-trained adapters for different tasks and languages. The framework, built on top of the popular HuggingFace Transformers library, enables extremely easy and quick adaptations of state-of-the-art pre-trained models (e.g., BERT, RoBERTa, XLM-R) across tasks and languages. Downloading, sharing, and training adapters is as seamless as possible using minimal changes to the training scripts and a specialized infrastructure. Our framework enables scalable and easy access to sharing of task-specific models, particularly in low-resource scenarios. AdapterHub includes all recent adapter architectures and can be found at https://AdapterHub.ml.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {⛔ No INSPIRE recid found,Computer Science - Computation and Language},
  file = {D\:\\Zotero文献库\\00人工智能\\00机器学习\\学习范式\\迁移学习\\PEFT\\prompt\\代码实现难点\\Pfeiffer et al_2020_AdapterHub.pdf;D\:\\ProgramFiles\\Zotero\\storage\\8TZD2YHB\\2007.html}
}

@online{popeEfficientlyScalingTransformer2022,
  title = {Efficiently {{Scaling Transformer Inference}}},
  author = {Pope, Reiner and Douglas, Sholto and Chowdhery, Aakanksha and Devlin, Jacob and Bradbury, James and Levskaya, Anselm and Heek, Jonathan and Xiao, Kefan and Agrawal, Shivani and Dean, Jeff},
  date = {2022-11-09},
  eprint = {2211.05102},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2211.05102},
  url = {http://arxiv.org/abs/2211.05102},
  urldate = {2023-08-11},
  abstract = {We study the problem of efficient generative inference for Transformer models, in one of its most challenging settings: large deep models, with tight latency targets and long sequence lengths. Better understanding of the engineering tradeoffs for inference for large Transformer-based models is important as use cases of these models are growing rapidly throughout application areas. We develop a simple analytical model for inference efficiency to select the best multi-dimensional partitioning techniques optimized for TPU v4 slices based on the application requirements. We combine these with a suite of low-level optimizations to achieve a new Pareto frontier on the latency and model FLOPS utilization (MFU) tradeoffs on 500B+ parameter models that outperforms the FasterTransformer suite of benchmarks. We further show that with appropriate partitioning, the lower memory requirements of multiquery attention (i.e. multiple query heads share single key/value head) enables scaling up to 32x larger context lengths. Finally, we achieve a low-batch-size latency of 29ms per token during generation (using int8 weight quantization) and a 76\% MFU during large-batch-size processing of input tokens, while supporting a long 2048-token context length on the PaLM 540B parameter model.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {C\:\\Users\\YeCanming\\Documents\\WPS Cloud Files\\200995647\\Zotero文献库\\人工智能\\创新实践\\prompt\\Pope et al_2022_Efficiently Scaling Transformer Inference.pdf;D\:\\ProgramFiles\\Zotero\\storage\\LGZFU64E\\2211.html}
}

@online{pothAdaptersUnifiedLibrary2023,
  title = {Adapters: {{A Unified Library}} for {{Parameter-Efficient}} and {{Modular Transfer Learning}}},
  shorttitle = {Adapters},
  author = {Poth, Clifton and Sterz, Hannah and Paul, Indraneil and Purkayastha, Sukannya and Engländer, Leon and Imhof, Timo and Vulić, Ivan and Ruder, Sebastian and Gurevych, Iryna and Pfeiffer, Jonas},
  date = {2023-11-18},
  eprint = {2311.11077},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2311.11077},
  url = {http://arxiv.org/abs/2311.11077},
  urldate = {2024-05-10},
  abstract = {We introduce Adapters, an open-source library that unifies parameter-efficient and modular transfer learning in large language models. By integrating 10 diverse adapter methods into a unified interface, Adapters offers ease of use and flexible configuration. Our library allows researchers and practitioners to leverage adapter modularity through composition blocks, enabling the design of complex adapter setups. We demonstrate the library's efficacy by evaluating its performance against full fine-tuning on various NLP tasks. Adapters provides a powerful tool for addressing the challenges of conventional fine-tuning paradigms and promoting more efficient and modular transfer learning. The library is available via https://adapterhub.ml/adapters.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {⛔ No INSPIRE recid found,Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {D\:\\Zotero文献库\\00人工智能\\00机器学习\\学习范式\\迁移学习\\PEFT\\prompt\\代码实现难点\\Poth et al_2023_Adapters.pdf;D\:\\ProgramFiles\\Zotero\\storage\\4E393Y8X\\2311.html}
}

@article{pourpanahReviewGeneralizedZeroShot2022,
  title = {A {{Review}} of {{Generalized Zero-Shot Learning Methods}}},
  author = {Pourpanah, Farhad and Abdar, Moloud and Luo, Yuxuan and Zhou, Xinlei and Wang, Ran and Lim, Chee Peng and Wang, Xi-Zhao and Wu, Q. M. Jonathan},
  date = {2022},
  journaltitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  eprint = {2011.08641},
  eprinttype = {arXiv},
  eprintclass = {cs},
  pages = {1--20},
  issn = {0162-8828, 2160-9292, 1939-3539},
  doi = {10.1109/TPAMI.2022.3191696},
  url = {http://arxiv.org/abs/2011.08641},
  urldate = {2024-02-23},
  abstract = {Generalized zero-shot learning (GZSL) aims to train a model for classifying data samples under the condition that some output classes are unknown during supervised learning. To address this challenging task, GZSL leverages semantic information of both seen (source) and unseen (target) classes to bridge the gap between both seen and unseen classes. Since its introduction, many GZSL models have been formulated. In this review paper, we present a comprehensive review of GZSL. Firstly, we provide an overview of GZSL including the problems and challenging issues. Then, we introduce a hierarchical categorization of the GZSL methods and discuss the representative methods of each category. In addition, we discuss several research directions for future studies.},
  langid = {english},
  keywords = {⛔ No INSPIRE recid found,Computer Science - Computer Vision and Pattern Recognition},
  annotation = {48 citations (Crossref) [2024-02-23]},
  file = {D:\ProgramFiles\Zotero\storage\MKHYGJWC\Pourpanah 等 - 2022 - A Review of Generalized Zero-Shot Learning Methods.pdf}
}

@article{prokhorenkovaCatBoostUnbiasedBoosting,
  title = {{{CatBoost}}: Unbiased Boosting with Categorical Features},
  author = {Prokhorenkova, Liudmila and Gusev, Gleb and Vorobev, Aleksandr and Dorogush, Anna Veronika and Gulin, Andrey},
  abstract = {This paper presents the key algorithmic techniques behind CatBoost, a new gradient boosting toolkit. Their combination leads to CatBoost outperforming other publicly available boosting implementations in terms of quality on a variety of datasets. Two critical algorithmic advances introduced in CatBoost are the implementation of ordered boosting, a permutation-driven alternative to the classic algorithm, and an innovative algorithm for processing categorical features. Both techniques were created to fight a prediction shift caused by a special kind of target leakage present in all currently existing implementations of gradient boosting algorithms. In this paper, we provide a detailed analysis of this problem and demonstrate that proposed algorithms solve it effectively, leading to excellent empirical results.},
  langid = {english},
  file = {D:\ProgramFiles\Zotero\storage\K7YXDVR9\Prokhorenkova et al. - CatBoost unbiased boosting with categorical featu.pdf}
}

@online{PuGuaJiKeYiWenDuDongEmbeddingDeGaiNianYiJiTaHeShenDuXueXiDeGuanXi2020,
  type = {知乎专栏文章},
  title = {一文读懂Embedding的概念，以及它和深度学习的关系},
  author = {普适极客},
  date = {2020-07-27},
  url = {https://zhuanlan.zhihu.com/p/164502624},
  urldate = {2022-10-25},
  abstract = {一文读懂Embedding的概念，以及它和深度学习的关系 - 来自知乎专栏「大数据+AI专栏」，作者: 普适极客 https://zhuanlan.zhihu.com/p/164502624},
  langid = {chinese},
  organization = {大数据+AI专栏},
  keywords = {稀疏矩阵,自然语言处理,计算机视觉},
  annotation = {赞数:360;},
  file = {D:\ProgramFiles\Zotero\storage\MLX4YS5F\164502624.html}
}

@online{PythonFanSheJiZhiChaKanShuXingHeFangFaLinBaBuXiangDongDeBoKeCSDNBoKe,
  title = {Python -反射机制查看属性和方法\_淋巴不想动的博客-{{CSDN博客}}},
  url = {https://blog.csdn.net/weixin_43067754/article/details/85693277},
  urldate = {2022-10-13},
  file = {D:\ProgramFiles\Zotero\storage\8PH86SAD\85693277.html}
}

@online{PythonHunHeGaoSiFenBuGMMDeYingYongJiKSJianYan_python,
  title = {Python-混合高斯分布({{GMM}}){{的应用及K-S检验}}\_python Gmm\_你的陈某某的博客-{{CSDN博客}}},
  url = {https://blog.csdn.net/weixin_45679938/article/details/119822556},
  urldate = {2023-08-28},
  langid = {english},
  file = {D:\ProgramFiles\Zotero\storage\LDNPL8FF\119822556.html}
}

@online{PythonQueShiZhiYuChuLiFuZaQueShiZhiTianBuFangFa,
  title = {Python缺失值预处理——复杂缺失值填补方法},
  url = {https://zhuanlan.zhihu.com/p/348974197},
  urldate = {2022-09-15},
  abstract = {复杂的缺失值填补方法，会考虑到数据的整体情况，然后在对有缺失值的数据进行填充，本小节将会介绍3种复杂的缺失值填补方法。 数据准备\#\# 输出高清图像 \%config InlineBackend.figure\_format = \&\#39;retina\&\#39; \%…},
  langid = {chinese},
  organization = {知乎专栏},
  file = {D:\ProgramFiles\Zotero\storage\5QHUPDFR\348974197.html}
}

@online{PyTorchXiJieJuanJiHouDebiasShiMeShiHouJiaShiMeShiHouBuJia,
  title = {【{{PyTorch细节}}】卷积后的bias什么时候加，什么时候不加\_是王同学呀的博客-{{CSDN博客}}\_卷积bias},
  url = {https://blog.csdn.net/wxd1233/article/details/117574603},
  urldate = {2022-11-15}
}

@book{QianYiXueXiDaoLun,
  title = {迁移学习导论},
  url = {https://weread.qq.com/web/reader/31732ca0724b923d3178545},
  urldate = {2024-04-05},
  abstract = {迁移学习作为机器学习和人工智能领域的重要方法，在计算机视觉、自然语言处理、语音识别等领域都得到了广泛的应用。本书的编写目的是帮助迁移学习及机器学习相关领域的初学者快速入门。全书主要分为背景与概念、方法与技术、扩展与探索及应用与展望四大部分。在这四大部分中，我们详尽介绍了迁移学习的背景、概念、方法和应用。除此之外，本书还配有相关的代码、数据和论文资料，最大限度地降低初学者的学习和使用门槛。本书适合对迁移学习感兴趣的读者阅读，也可以作为相关课程的配套教材。},
  langid = {chinese},
  keywords = {⛔ No INSPIRE recid found},
  file = {D\:\\Zotero文献库\\00人工智能\\00机器学习课程\\创新实践\\迁移学习\\迁移学习导论.pdf;D\:\\ProgramFiles\\Zotero\\storage\\6LDJSJA8\\31732ca0724b923d3178545.html}
}

@article{qiuControllingTexttoImageDiffusion2023,
  title = {Controlling {{Text-to-Image Diffusion}} by {{Orthogonal Finetuning}}},
  author = {Qiu, Zeju and Liu, Weiyang and Feng, Haiwen and Xue, Yuxuan and Feng, Yao and Liu, Zhen and Zhang, Dan and Weller, Adrian and Schölkopf, Bernhard},
  date = {2023-12-15},
  journaltitle = {Advances in Neural Information Processing Systems},
  volume = {36},
  pages = {79320--79362},
  url = {https://papers.nips.cc/paper_files/paper/2023/hash/faacb7a4827b4d51e201666b93ab5fa7-Abstract-Conference.html},
  urldate = {2024-05-10},
  langid = {english},
  keywords = {⛔ No INSPIRE recid found,Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics,Computer Science - Machine Learning},
  file = {D\:\\ProgramFiles\\Zotero\\storage\\IK6MJ57N\\Qiu 等 - 2023 - Controlling Text-to-Image Diffusion by Orthogonal .pdf;D\:\\Zotero文献库\\00人工智能\\00计算机视觉课程\\经典视觉任务的对应设计\\视觉地点定位\\创新点\\PEFT\\Qiu et al_2024_Controlling Text-to-Image Diffusion by Orthogonal Finetuning.pdf;D\:\\ProgramFiles\\Zotero\\storage\\KBXFYAZP\\2306.html}
}

@online{QQplotDeXiangXiJieShi,
  title = {{{QQplot的详细解释}}},
  url = {https://zhuanlan.zhihu.com/p/232271603},
  urldate = {2023-08-28},
  abstract = {最近接触到QQplot, 自学的过程中，发现各种资料上的解释都不够清晰全面，我在综合了各种资料之后，梳理出了一个比较清晰的解释，在这里分享出来，供有需要的朋友们参考 QQ plot也就是Quantile-Quantile Plots。是 …},
  langid = {english},
  organization = {知乎专栏}
}

@article{r.m.EffectiveFeatureEngineering2020,
  title = {An Effective Feature Engineering for {{DNN}} Using Hybrid {{PCA-GWO}} for Intrusion Detection in {{IoMT}} Architecture},
  author = {R.M., Swarna Priya and Maddikunta, Praveen Kumar Reddy and M., Parimala and Koppu, Srinivas and Gadekallu, Thippa Reddy and Chowdhary, Chiranji Lal and Alazab, Mamoun},
  date = {2020-07},
  journaltitle = {Computer Communications},
  shortjournal = {Computer Communications},
  volume = {160},
  pages = {139--149},
  issn = {01403664},
  doi = {10.1016/j.comcom.2020.05.048},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S014036642030298X},
  urldate = {2023-10-19},
  langid = {english},
  file = {D:\ProgramFiles\Zotero\storage\PHQP6USY\R.M. et al. - 2020 - An effective feature engineering for DNN using hyb.pdf}
}

@article{radfordImprovingLanguageUnderstanding,
  title = {Improving {{Language Understanding}} by {{Generative Pre-Training}}},
  author = {Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya},
  abstract = {Natural language understanding comprises a wide range of diverse tasks such as textual entailment, question answering, semantic similarity assessment, and document classification. Although large unlabeled text corpora are abundant, labeled data for learning these specific tasks is scarce, making it challenging for discriminatively trained models to perform adequately. We demonstrate that large gains on these tasks can be realized by generative pre-training of a language model on a diverse corpus of unlabeled text, followed by discriminative fine-tuning on each specific task. In contrast to previous approaches, we make use of task-aware input transformations during fine-tuning to achieve effective transfer while requiring minimal changes to the model architecture. We demonstrate the effectiveness of our approach on a wide range of benchmarks for natural language understanding. Our general task-agnostic model outperforms discriminatively trained models that use architectures specifically crafted for each task, significantly improving upon the state of the art in 9 out of the 12 tasks studied. For instance, we achieve absolute improvements of 8.9\% on commonsense reasoning (Stories Cloze Test), 5.7\% on question answering (RACE), and 1.5\% on textual entailment (MultiNLI).},
  langid = {english},
  file = {D:\ProgramFiles\Zotero\storage\GVQ6BUY4\Radford 等 - Improving Language Understanding by Generative Pre.pdf}
}

@article{radfordLanguageModelsArea,
  title = {Language {{Models}} Are {{Unsupervised Multitask Learners}}},
  author = {Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  abstract = {Natural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically approached with supervised learning on taskspecific datasets. We demonstrate that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called WebText. When conditioned on a document plus questions, the answers generated by the language model reach 55 F1 on the CoQA dataset - matching or exceeding the performance of 3 out of 4 baseline systems without using the 127,000+ training examples. The capacity of the language model is essential to the success of zero-shot task transfer and increasing it improves performance in a log-linear fashion across tasks. Our largest model, GPT-2, is a 1.5B parameter Transformer that achieves state of the art results on 7 out of 8 tested language modeling datasets in a zero-shot setting but still underfits WebText. Samples from the model reflect these improvements and contain coherent paragraphs of text. These findings suggest a promising path towards building language processing systems which learn to perform tasks from their naturally occurring demonstrations.},
  langid = {english},
  file = {C:\Users\YeCanming\Documents\WPS Cloud Files\200995647\Zotero文献库\人工智能\创新实践\多任务学习\多任务学习\Radford et al_Language Models are Unsupervised Multitask Learners.pdf}
}

@online{rapaioAnswerItReasonable2014,
  title = {Answer to "{{Is}} It Reasonable for a Classifier to Obtain a High {{AUC}} and a Low {{MCC}}? {{Or}} the Opposite?"},
  shorttitle = {Answer to "{{Is}} It Reasonable for a Classifier to Obtain a High {{AUC}} and a Low {{MCC}}?},
  author = {{rapaio}},
  date = {2014-08-11},
  url = {https://stats.stackexchange.com/a/111485},
  urldate = {2023-08-25},
  langid = {english},
  organization = {Cross Validated},
  file = {D:\ProgramFiles\Zotero\storage\GK9492G3\is-it-reasonable-for-a-classifier-to-obtain-a-high-auc-and-a-low-mcc-or-the-opp.html}
}

@article{razaviPracticalFeatureengineeringFramework2019,
  title = {A Practical Feature-Engineering Framework for Electricity Theft Detection in Smart Grids},
  author = {Razavi, Rouzbeh and Gharipour, Amin and Fleury, Martin and Akpan, Ikpe Justice},
  date = {2019-03},
  journaltitle = {Applied Energy},
  shortjournal = {Applied Energy},
  volume = {238},
  pages = {481--494},
  issn = {03062619},
  doi = {10.1016/j.apenergy.2019.01.076},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0306261919300753},
  urldate = {2023-10-19},
  abstract = {Despite many potential advantages, Advanced Metering Infrastructures have introduced new ways to falsify meter readings and commit electricity theft. This study contributes a new model-agnostic, feature-engineering framework for theft detection in smart grids. The framework introduces a combination of Finite Mixture Model clustering for customer segmentation and a Genetic Programming algorithm for identifying new features suitable for prediction. Utilizing demand data from more than 4000 households, a Gradient Boosting Machine algorithm is applied within the framework, significantly outperforming the results of prior machine-learning, theft-detection methods. This study further examines some important practical aspects of deploying theft detection including: the detection delay; the required size of historical demand data; the accuracy in detecting thefts of various types and intensity; detecting irregular and unseen attacks; and the computational complexity of the detection algorithm.},
  langid = {english},
  file = {D:\ProgramFiles\Zotero\storage\ADFPQGKF\Razavi et al. - 2019 - A practical feature-engineering framework for elec.pdf}
}

@inproceedings{rebuffiLearningMultipleVisual2017,
  title = {Learning Multiple Visual Domains with Residual Adapters},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Rebuffi, Sylvestre-Alvise and Bilen, Hakan and Vedaldi, Andrea},
  date = {2017},
  volume = {30},
  publisher = {Curran Associates, Inc.},
  url = {https://proceedings.neurips.cc/paper/2017/hash/e7b24b112a44fdd9ee93bdf998c6ca0e-Abstract.html},
  urldate = {2023-12-10},
  abstract = {There is a growing interest in learning data representations that work well for many different types of problems and data. In this paper, we look in particular at the task of learning a single visual representation that can be successfully utilized in the analysis of very different types of images, from dog breeds to stop signs and digits. Inspired by recent work on learning networks that predict the parameters of another, we develop a tunable deep network architecture that, by means of adapter residual modules, can be steered on the fly to diverse visual domains. Our method achieves a high degree of parameter sharing while maintaining or even improving the accuracy of domain-specific representations. We also introduce the Visual Decathlon Challenge, a benchmark that evaluates the ability of  representations to capture simultaneously ten very different visual domains and measures their ability to recognize well uniformly.},
  langid = {english},
  keywords = {⛔ No INSPIRE recid found},
  file = {D:\Zotero文献库\00人工智能\00计算机视觉课程\经典视觉任务的对应设计\视觉地点定位\创新点\PEFT\Rebuffi et al_2017_Learning multiple visual domains with residual adapters.pdf}
}

@online{RecommendedReadingEvolutionary,
  title = {--{{Recommended}} Reading – {{Evolutionary Computation}} and {{Its}}...},
  url = {https://bb.sustech.edu.cn/webapps/blackboard/content/listContent.jsp?course_id=_4808_1&content_id=_347249_1&mode=reset},
  urldate = {2023-02-26},
  file = {D:\ProgramFiles\Zotero\storage\E23GBLBF\listContent.html}
}

@online{RongHeGaoSiFenBuRonnieHuDeBoKeCSDNBoKe,
  title = {融合高斯分布\_{{Ronnie}}\_{{Hu的博客-CSDN博客}}\_融合高斯分布},
  url = {https://blog.csdn.net/Ronnie_Hu/article/details/111378283},
  urldate = {2022-09-29}
}

@online{RuntimeErrorExpectedScalar2022,
  title = {{{RuntimeError}}: Expected Scalar Type {{Byte}} but Found {{Float}} - Tensorboard},
  shorttitle = {{{RuntimeError}}},
  date = {2022-01-18T02:10:01+00:00},
  url = {https://discuss.pytorch.org/t/runtimeerror-expected-scalar-type-byte-but-found-float/141897},
  urldate = {2022-10-31},
  abstract = {I want to implement GCN on my data.  the data looks like this:  Data(pos=[4, 2], x=[4, 1080, 1920, 3], y=[4], edge\_index=[2, 12], edge\_attr=[12, 1])  my code:  \# Set optimizer (adam) optimizer = torch.optim.Adam(model.parameters(), lr=0.01) \# Define loss function criterion = torch.nn.CrossEntropyLoss()   \# Initialize train function def train():     model.train()       for data in trainset:         out = model(data.x.view(-1, 3), data.edge\_index, data.batch)         loss = criterion(out, data.y) ...},
  langid = {english},
  organization = {PyTorch Forums}
}

@inproceedings{senerMultiTaskLearningMultiObjective2018,
  title = {Multi-{{Task Learning}} as {{Multi-Objective Optimization}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Sener, Ozan and Koltun, Vladlen},
  date = {2018},
  volume = {31},
  publisher = {Curran Associates, Inc.},
  url = {https://proceedings.neurips.cc/paper/2018/hash/432aca3a1e345e339f35a30c8f65edce-Abstract.html},
  urldate = {2022-10-27},
  abstract = {In multi-task learning, multiple tasks are solved jointly, sharing inductive bias between them. Multi-task learning is inherently a multi-objective problem because different tasks may conflict, necessitating a trade-off. A common compromise is to optimize a proxy objective that minimizes a weighted linear combination of per-task losses. However, this workaround is only valid when the tasks do not compete, which is rarely the case. In this paper, we explicitly cast multi-task learning as multi-objective optimization, with the overall objective of finding a Pareto optimal solution. To this end, we use algorithms developed in the gradient-based multi-objective optimization literature. These algorithms are not directly applicable to large-scale learning problems since they scale poorly with the dimensionality of the gradients and the number of tasks. We therefore propose an upper bound for the multi-objective loss and show that it can be optimized efficiently. We further prove that optimizing this upper bound yields a Pareto optimal solution under realistic assumptions. We apply our method to a variety of multi-task deep learning problems including digit classification, scene understanding (joint semantic segmentation, instance segmentation, and depth estimation), and multi-label classification. Our method produces higher-performing models than recent multi-task learning formulations or per-task training.},
  file = {C:\Users\YeCanming\Documents\WPS Cloud Files\200995647\Zotero文献库\人工智能\创新实践\多任务学习\Sener_Koltun_2018_Multi-Task Learning as Multi-Objective Optimization.pdf}
}

@article{seoHierarchicalConvolutionalNeural2019,
  title = {Hierarchical Convolutional Neural Networks for Fashion Image Classification},
  author = {Seo, Yian and Shin, Kyung-shik},
  date = {2019-02-01},
  journaltitle = {Expert Systems with Applications},
  shortjournal = {Expert Systems with Applications},
  volume = {116},
  pages = {328--339},
  issn = {0957-4174},
  doi = {10.1016/j.eswa.2018.09.022},
  url = {https://www.sciencedirect.com/science/article/pii/S0957417418305992},
  urldate = {2024-04-05},
  abstract = {Deep learning can be applied in various business fields for better performance. Especially, fashion-related businesses have started to apply deep learning techniques on their e-commerce such as apparel recognition, apparel search and retrieval engine, and automatic product recommendation. The most important backbone of these applications is the image classification task. However, apparel classification can be difficult due to its various apparel properties, and complexity in the depth of categorization. In other words, multi-class apparel classification can be hard and ambiguous to separate among similar classes. Here, we find the need of image classification reflecting hierarchical structure of apparel categories. In most of the previous studies, hierarchy has not been considered in image classification when using Convolutional Neural Networks (CNN), and not even in fashion image classification using other methodologies. In this paper, we propose to apply Hierarchical Convolutional Neural Networks (HCNN) on apparel classification. This study has contribution in that this is the first trial to apply hierarchical classification of apparel using CNN and has significance in that the proposed model is a knowledge embedded classifier outputting hierarchical information. We implement HCNN using VGGNet on Fashion-MNIST dataset. Results have shown that when using HCNN model, the loss gets decreased and the accuracy gets improved than the base model without hierarchical structure. We conclude that HCNN brings better performance in classifying apparel.},
  langid = {english},
  keywords = {⛔ No INSPIRE recid found,Classification,Convolutional neural networks,Fashion image,Hierarchy},
  annotation = {112 citations (Crossref) [2024-04-06]}
}

@online{ShenDuLiaoJieTeZhengGongCheng,
  title = {深度了解特征工程},
  url = {https://zhuanlan.zhihu.com/p/111296130},
  urldate = {2023-11-08},
  abstract = {一 特征工程介绍（Feature Engineering）什么是特征工程？ 特征工程解决了什么问题？ 为什么特征工程对机器学习很重要？ 怎么做特征工程？ 怎么做好特征工程？ 集众多博友智慧，一文全面了解并应用特征工程。 1 定…},
  langid = {chinese},
  organization = {知乎专栏},
  file = {D:\ProgramFiles\Zotero\storage\9WEC9ZBB\111296130.html}
}

@inproceedings{shiMultimodalAutoMLStructured2021,
  title = {Multimodal {{AutoML}} on {{Structured Tables}} with {{Text Fields}}},
  author = {Shi, Xingjian and Mueller, Jonas and Erickson, Nick and Li, Mu and Smola, Alex},
  date = {2021-07-14},
  url = {https://openreview.net/forum?id=OHAIVOOl7Vl},
  urldate = {2024-09-15},
  abstract = {We design automated supervised learning systems for data tables that not only contain numeric/categorical columns, but text fields as well. Here we assemble 15 multimodal data tables that each contain some text fields and stem from a real business application. Over this benchmark, we evaluate numerous multimodal AutoML strategies, including a standard two-stage approach where NLP is used to featurize the text such that AutoML for tabular data can then be applied. We propose various practically superior strategies based on multimodal adaptations of Transformer networks and stack ensembling of these networks with classical tabular models. Beyond performing the best in our benchmark, our proposed (fully automated) methodology manages to rank 1st place (against human data scientists) when fit to the raw tabular/text data in two MachineHack prediction competitions and 2nd place (out of 2380 teams) in Kaggle’s Mercari Price Suggestion Challenge.},
  eventtitle = {8th {{ICML Workshop}} on {{Automated Machine Learning}} ({{AutoML}})},
  langid = {english},
  keywords = {⛔ No INSPIRE recid found},
  file = {D:\Zotero文献库\00人工智能\00机器学习\医学人工智能\AI知识查缺补漏\automl\Shi et al_2021_Multimodal AutoML on Structured Tables with Text Fields.pdf}
}

@online{ShuJuZhongDeYiChangZhiYingGaiZhiJieShanChuMa,
  title = {数据中的异常值，应该直接删除吗？},
  url = {https://zhuanlan.zhihu.com/p/66391643},
  urldate = {2023-11-09},
  abstract = {文章转载自“小白学统计”公众号，感谢作者授权。 不少人曾问我：我的数据中有异常值，是不是应该删除？要回答这个问题，我们必须从异常值的概念来谈起。可能有的人觉得异常值很好理解，不就是“异常”的值吗？关…},
  langid = {chinese},
  organization = {知乎专栏},
  file = {D:\ProgramFiles\Zotero\storage\QADW2GVC\66391643.html}
}

@online{siFLoRALowRankCore2024,
  title = {{{FLoRA}}: {{Low-Rank Core Space}} for {{N-dimension}}},
  shorttitle = {{{FLoRA}}},
  author = {Si, Chongjie and Wang, Xuehui and Yang, Xue and Xu, Zhengqin and Li, Qingyun and Dai, Jifeng and Qiao, Yu and Yang, Xiaokang and Shen, Wei},
  date = {2024-05-23},
  eprint = {2405.14739},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2405.14739},
  urldate = {2024-08-29},
  abstract = {Adapting pre-trained foundation models for various downstream tasks has been prevalent in artificial intelligence. Due to the vast number of tasks and high costs, adjusting all parameters becomes unfeasible. To mitigate this, several finetuning techniques have been developed to update the pre-trained model weights in a more resource-efficient manner, such as through low-rank adjustments. Yet, almost all of these methods focus on linear weights, neglecting the intricacies of parameter spaces in higher dimensions like 4D. Alternatively, some methods can be adapted for high-dimensional parameter space by compressing changes in the original space into two dimensions and then employing low-rank matrix decomposition. However, these approaches destructs the structural integrity of the involved high-dimensional spaces. To tackle the diversity of dimensional spaces across different foundation models and provide a more precise representation of the changes within these spaces, this paper introduces a generalized parameter-efficient fine-tuning framework, FLoRA, designed for various dimensional parameter space. Specifically, utilizing Tucker decomposition, FLoRA asserts that changes in each dimensional parameter space are based on a low-rank core space which maintains the consistent topological structure with the original space. It then models the changes through this core space alongside corresponding weights to reconstruct alterations in the original space. FLoRA effectively preserves the structural integrity of the change of original N-dimensional parameter space, meanwhile decomposes it via low-rank tensor decomposition. Extensive experiments on computer vision, natural language processing and multi-modal tasks validate FLoRA’s effectiveness. Codes are available at https://github.com/SJTU-DeepVisionLab/FLoRA.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {⛔ No INSPIRE recid found,Computer Science - Computer Vision and Pattern Recognition},
  file = {D:\ProgramFiles\Zotero\storage\A9GHDPUN\Si 等 - 2024 - FLoRA Low-Rank Core Space for N-dimension.pdf}
}

@article{sillaSurveyHierarchicalClassification2011,
  title = {A Survey of Hierarchical Classification across Different Application Domains},
  author = {Silla, Carlos N. and Freitas, Alex A.},
  date = {2011-01-01},
  journaltitle = {Data Mining and Knowledge Discovery},
  shortjournal = {Data Min Knowl Disc},
  volume = {22},
  number = {1},
  pages = {31--72},
  issn = {1573-756X},
  doi = {10.1007/s10618-010-0175-9},
  url = {https://doi.org/10.1007/s10618-010-0175-9},
  urldate = {2024-04-05},
  abstract = {In this survey we discuss the task of hierarchical classification. The literature about this field is scattered across very different application domains and for that reason research in one domain is often done unaware of methods developed in other domains. We define what is the task of hierarchical classification and discuss why some related tasks should not be considered hierarchical classification. We also present a new perspective about some existing hierarchical classification approaches, and based on that perspective we propose a new unifying framework to classify the existing approaches. We also present a review of empirical comparisons of the existing methods reported in the literature as well as a conceptual comparison of those methods at a high level of abstraction, discussing their advantages and disadvantages.},
  langid = {english},
  keywords = {⛔ No INSPIRE recid found,DAG-structured class hierarchies,Hierarchical classification,Tree-structured class hierarchies},
  annotation = {628 citations (Crossref) [2024-04-06]},
  file = {D:\Zotero文献库\00人工智能\00计算机视觉课程\经典视觉任务的对应设计\视觉分类\Silla_Freitas_2011_A survey of hierarchical classification across different application domains.pdf}
}

@online{SklearnKuFit_transformDeHanYi_PiQiaQiuHuangLiaoBaJiPieDeBoKeCSDNBoKe_encoder,
  title = {【sklearn库】fit\_transform()的含义\_皮卡丘黄了吧唧丿的博客-{{CSDN博客}}\_encoder.Fit\_transform},
  url = {https://blog.csdn.net/weixin_47125742/article/details/115449648},
  urldate = {2022-09-15},
  file = {D:\ProgramFiles\Zotero\storage\AFBWCADB\115449648.html}
}

@online{SklearnMetricsZhongDePingGuFangFaJieShaoAccuracy,
  title = {sklearn.metrics中的评估方法介绍（accuracy\_score, recall\_score, roc\_curve, roc\_auc\_score, confusion\_matrix） - limingqi - 博客园},
  url = {https://www.cnblogs.com/limingqi/p/11729572.html},
  urldate = {2022-09-29},
  abstract = {1 accuracy\_score：分类准确率分数是指所有分类正确的百分比。分类准确率这一衡量分类器的标准比较容易理解，但是它不能告诉你响应值的潜在分布，并且它也不能告诉你分类器犯错的类型。常常误导初学},
  langid = {chinese},
  file = {D:\ProgramFiles\Zotero\storage\XNJMRSVJ\11729572.html}
}

@inproceedings{smithCyclicalLearningRates2017,
  title = {Cyclical {{Learning Rates}} for {{Training Neural Networks}}},
  booktitle = {2017 {{IEEE Winter Conference}} on {{Applications}} of {{Computer Vision}} ({{WACV}})},
  author = {Smith, Leslie N.},
  date = {2017-03},
  pages = {464--472},
  doi = {10.1109/WACV.2017.58},
  url = {https://ieeexplore.ieee.org/abstract/document/7926641},
  urldate = {2024-09-05},
  abstract = {It is known that the learning rate is the most important hyper-parameter to tune for training deep neural networks. This paper describes a new method for setting the learning rate, named cyclical learning rates, which practically eliminates the need to experimentally find the best values and schedule for the global learning rates. Instead of monotonically decreasing the learning rate, this method lets the learning rate cyclically vary between reasonable boundary values. Training with cyclical learning rates instead of fixed values achieves improved classification accuracy without a need to tune and often in fewer iterations. This paper also describes a simple way to estimate "reasonable bounds" - linearly increasing the learning rate of the network for a few epochs. In addition, cyclical learning rates are demonstrated on the CIFAR-10 and CIFAR-100 datasets with ResNets, Stochastic Depth networks, and DenseNets, and the ImageNet dataset with the AlexNet and GoogLeNet architectures. These are practical tools for everyone who trains neural networks.},
  eventtitle = {2017 {{IEEE Winter Conference}} on {{Applications}} of {{Computer Vision}} ({{WACV}})},
  langid = {english},
  keywords = {⛔ No INSPIRE recid found,Computational efficiency,Computer architecture,Neural networks,Schedules,Training,Tuning},
  annotation = {1317 citations (Crossref) [2024-09-05]},
  file = {D\:\\Zotero文献库\\00人工智能\\00机器学习\\优化器\\学习率\\Smith_2017_Cyclical Learning Rates for Training Neural Networks.pdf;D\:\\ProgramFiles\\Zotero\\storage\\I8GK69FK\\7926641.html}
}

@online{SongHuaGuanYuAutoMLNiXiangZhiDaoDeDuZaiZheLi2019,
  type = {知乎专栏文章},
  title = {关于AutoML，你想知道的都在这里！},
  author = {松桦},
  date = {2019-11-24},
  url = {https://zhuanlan.zhihu.com/p/93109455},
  urldate = {2022-10-16},
  abstract = {关于AutoML，你想知道的都在这里！ - 来自知乎专栏「Auto\_Me\_Learning」，作者: 松桦 https://zhuanlan.zhihu.com/p/93109455},
  langid = {chinese},
  organization = {Auto\_Me\_Learning},
  keywords = {AutoML,开源项目,机器学习},
  annotation = {赞数:361;},
  file = {D:\ProgramFiles\Zotero\storage\TLXQL83W\93109455.html}
}

@inreference{SpaceMathematics2022,
  title = {Space (Mathematics)},
  booktitle = {Wikipedia},
  date = {2022-09-13T01:49:42Z},
  url = {https://en.wikipedia.org/w/index.php?title=Space_(mathematics)&oldid=1110006381},
  urldate = {2022-10-04},
  abstract = {In mathematics, a space is a set (sometimes called a universe) with some added structure. While modern mathematics uses many types of spaces, such as Euclidean spaces, linear spaces, topological spaces, Hilbert spaces, or probability spaces, it does not define the notion of "space" itself.A space consists of selected mathematical objects that are treated as points, and selected relationships between these points.  The nature of the points can vary widely: for example, the points can be elements of a set, functions on another space, or subspaces of another space. It is the relationships that define the nature of the space. More precisely, isomorphic spaces are considered identical, where an isomorphism between two spaces is a one-to-one correspondence between their points that preserves the relationships. For example, the relationships between the points of a three-dimensional Euclidean space are uniquely determined by Euclid's axioms, and all three-dimensional Euclidean spaces are considered identical. Topological notions such as continuity have natural definitions in every Euclidean space.  However, topology does not distinguish straight lines from curved lines, and the relation between Euclidean and topological spaces is thus "forgetful". Relations of this kind are  treated in more detail in the Section "Types of spaces".  It is not always clear whether a given mathematical object should be considered as a geometric "space", or an  algebraic "structure". A general definition of "structure", proposed by Bourbaki, embraces all common types of spaces, provides a general definition of isomorphism, and justifies the transfer of properties between isomorphic structures.},
  langid = {english},
  annotation = {Page Version ID: 1110006381},
  file = {D:\ProgramFiles\Zotero\storage\UQ3F9A6F\Space_(mathematics).html}
}

@online{sparklegoTuiJianXiTongZhiAUCPingJieZhiBiao2019,
  type = {知乎专栏文章},
  title = {推荐系统之AUC评价指标},
  author = {{sparklego}},
  date = {2019-07-12},
  url = {https://zhuanlan.zhihu.com/p/73335362},
  urldate = {2022-10-26},
  abstract = {推荐系统之AUC评价指标 - 来自知乎专栏，作者: sparklego https://zhuanlan.zhihu.com/p/73335362},
  langid = {chinese},
  organization = {回答},
  keywords = {推荐系统},
  annotation = {赞数:36;},
  file = {D:\ProgramFiles\Zotero\storage\ULU998HU\73335362.html}
}

@online{SpringDataJpaZhengHePostgreSQL,
  title = {Spring {{Data Jpa整合PostgreSQL}}（二）\_有道无德的博客-{{CSDN博客}}},
  url = {https://blog.csdn.net/carry1beyond/article/details/79879971},
  urldate = {2022-11-14},
  file = {D:\ProgramFiles\Zotero\storage\RRHU2ETF\79879971.html}
}

@article{stojadinovicImprovedPredictivePerformance2021,
  title = {Improved Predictive Performance of Prostate Biopsy Collaborative Group Risk Calculator When Based on Automated Machine Learning},
  author = {Stojadinovic, Miroslav and Milicevic, Bogdan and Jankovic, Slobodan},
  date = {2021-11-01},
  journaltitle = {Computers in Biology and Medicine},
  shortjournal = {Comput. Biol. Med.},
  volume = {138},
  number = {C},
  issn = {0010-4825},
  doi = {10.1016/j.compbiomed.2021.104903},
  url = {https://doi.org/10.1016/j.compbiomed.2021.104903},
  urldate = {2023-08-23},
  abstract = {The Prostate Biopsy Collaborative Group risk calculator (PBCG RC) has a moderate discriminatory capability. This study aimed to create automated machine learning (AutoML) PBCG RC for predicting the probability of any-grade and high-grade prostate cancer (PCa). This retrospective, single-center study was carried out using the database with 832 patients who were subject to transrectal ultrasound-guided prostate biopsy with prostate-specific antigen (PSA) values from 2 to 50 ng/ml. Information about PBCG RC predictors was gathered for all patients. We used H2O, as an open-source platform for AutoML, where the set of 20 base learning algorithms were trained. The AutoML PBCG RC was compared in terms of discrimination, calibration, and clinical utility with the original PBCG RC. PCa was detected in 341 (41\%) men, and 159 (19.1\%) of them had high-grade PCa. Our AutoML models demonstrated better discriminative ability than the original PBCG RC for detection of PCa (area under the curve [AUC]: 0.703 vs 0.628; P = 0.023) and high-grade PCa (AUC: 0.990 vs 0.717; P {$<$} 0.001). The decision curve analyses showed that AutoML models performed better. For high-grade PCa the PSA was the most important feature. We applied ensemble techniques to create a freely available online PCa risk tool based on PBCG RC predictors and AutoML algorithms. The AutoML models drastically improved original model performance and the predictions of high-grade PCa were nearly perfect. However, new models should be used with a reserve, because external validation has not been performed yet. • The Prostate Biopsy Collaborative Group risk calculator has moderate accuracy. • The automated machine learning was not previously used to improve its performance. • We improved the accuracy of predicting cancer with ensemble learning techniques. • We deployed the models in a web-embedded structure.},
  langid = {english},
  keywords = {Automated machine learning,Prostate biopsy,Prostate cancer,Risk calculator,Webbased model}
}

@inreference{StopWord2022,
  title = {Stop Word},
  booktitle = {Wikipedia},
  date = {2022-09-21T10:50:48Z},
  url = {https://en.wikipedia.org/w/index.php?title=Stop_word&oldid=1111514175},
  urldate = {2022-09-29},
  abstract = {Stop words are the words in a stop list (or stoplist or negative dictionary) which are filtered out (i.e. stopped) before or after processing of natural language data (text) because they are insignificant. There is no single universal list of stop words used by all natural language processing tools, nor any agreed upon rules for identifying stop words, and indeed not all tools even use such a list. Therefore, any group of words can be chosen as the stop words for a given purpose. The "general trend in [information retrieval]  systems over time has been from standard use of quite large stop lists (200–300 terms) to very small stop lists (7–12 terms) to no stop list whatsoever"},
  langid = {english},
  annotation = {Page Version ID: 1111514175},
  file = {D:\ProgramFiles\Zotero\storage\STJZF3AA\Stop_word.html}
}

@online{StrategyTransferringImageNet21k,
  title = {The Strategy of Transferring {{ImageNet-21k ViT}} Model to Cifar100 · {{Issue}} \#40 · {{Alibaba-MIIL}}/{{ImageNet21K}}},
  url = {https://github.com/Alibaba-MIIL/ImageNet21K/issues/40},
  urldate = {2023-08-07},
  abstract = {Hi @mrT23, thanks for your great work! Currently I use timm train.py to finetune the 'vit\_base\_patch16\_224\_miil\_in21k' model on cifar100, however I can't get the reported result 94.2\%. Here is my r...},
  langid = {english},
  organization = {GitHub},
  file = {D:\ProgramFiles\Zotero\storage\CDHT6ZBY\40.html}
}

@online{sungLSTLadderSideTuning2022,
  title = {{{LST}}: {{Ladder Side-Tuning}} for {{Parameter}} and {{Memory Efficient Transfer Learning}}},
  shorttitle = {{{LST}}},
  author = {Sung, Yi-Lin and Cho, Jaemin and Bansal, Mohit},
  date = {2022-10-31},
  eprint = {2206.06522},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2206.06522},
  urldate = {2024-08-29},
  abstract = {Fine-tuning large pre-trained models on downstream tasks has been adopted in a variety of domains recently. However, it is costly to update the entire parameter set of large pre-trained models. Although recently proposed parameter-efficient transfer learning (PETL) techniques allow updating a small subset of parameters (e.g. only using 2\% of parameters) inside a pre-trained backbone network for a new task, they only reduce the training memory requirement by up to 30\%. This is because the gradient computation for the trainable parameters still requires backpropagation through the large pre-trained backbone model. To address this, we propose Ladder Side-Tuning (LST), a new PETL technique that can reduce training memory requirements by more substantial amounts. Unlike existing parameter-efficient methods that insert additional parameters inside backbone networks, we train a ladder side network, a small and separate network that takes intermediate activations as input via shortcut connections (called ladders) from backbone networks and makes predictions. LST has significantly lower memory requirements than previous methods, because it does not require backpropagation through the backbone network, but instead only through the side network and ladder connections. We evaluate our method with various models (T5 and CLIP-T5) on both NLP (GLUE) and vision-and-language (VQA, GQA, NLVR2 , MSCOCO) tasks. LST saves 69\% of the memory costs to fine-tune the whole network, while other methods only save 26\% of that in similar parameter usages (hence, 2.7x more memory savings). Moreover, LST achieves higher accuracy than Adapter and LoRA in a low-memory regime. To further show the advantage of this better memory efficiency, we also apply LST to larger T5 models, attaining better GLUE performance than full fine-tuning and other PETL methods. The accuracy-efficiency trade-off also holds on VL tasks.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {⛔ No INSPIRE recid found,Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition},
  file = {D\:\\ProgramFiles\\Zotero\\storage\\M5768PWD\\Sung 等 - 2022 - LST Ladder Side-Tuning for Parameter and Memory E.pdf;D\:\\Zotero文献库\\00人工智能\\00机器学习\\学习范式\\迁移学习\\PEFT\\residual\\Sung et al_2022_LST.pdf}
}

@online{superli3CanSomeoneGive2019,
  type = {Forum post},
  title = {Can Someone Give a Good Math/Stats Explanation as to What the Parameter Var\_smoothing Does for {{GaussianNB}} in Scikit Learn?},
  author = {{superli3}},
  date = {2019-09-22},
  url = {https://stackoverflow.com/q/58046129/16306773},
  urldate = {2022-11-18},
  organization = {Stack Overflow}
}

@online{SurveyTransferLearning,
  title = {《A Survey on Transfer Learning》迁移学习研究综述 翻译 - 进击的学徒 - 博客园},
  url = {https://www.cnblogs.com/jinjidexuetu/p/11097626.html},
  urldate = {2022-08-08},
  abstract = {迁移学习研究综述 Sinno Jialin Pan and Qiang Yang,Fellow, IEEE 摘要： \&emsp;\&emsp;在许多机器学习和数据挖掘算法中，一个重要的假设就是目前的训练},
  langid = {chinese},
  file = {D:\ProgramFiles\Zotero\storage\7X5C2ZHT\11097626.html}
}

@online{TorchHubPyTorch,
  title = {Torch.Hub — {{PyTorch}} 1.13 Documentation},
  url = {https://pytorch.org/docs/stable/hub.html},
  urldate = {2022-11-14},
  file = {D:\ProgramFiles\Zotero\storage\CGLE7TDR\hub.html}
}

@online{TrainerOptimizerTransformers2020,
  title = {Trainer Optimizer - 🤗{{Transformers}}},
  date = {2020-11-20T10:19:39+00:00},
  url = {https://discuss.huggingface.co/t/trainer-optimizer/2146},
  urldate = {2023-08-07},
  abstract = {Hi everyone,  in my code I instantiate a trainer as follows:  trainer = Trainer(     model=model,     args=training\_args,     train\_dataset=train\_dataset,     eval\_dataset=eval\_dataset,     compute\_metrics=compute\_metrics, )  I don’t specify anything in the “optimizers” field as I’ve always used the default one (AdamW).  I tried to create an optimizer instance similar to the default one so I could try to change the learning rate (lr).  The code I used to simulate the default optimizer is the fol...},
  langid = {english},
  organization = {Hugging Face Forums}
}

@online{TuJieSwinTransformer,
  title = {{{图解Swin Transformer}}},
  url = {https://zhuanlan.zhihu.com/p/367111046},
  urldate = {2023-07-15},
  abstract = {引言目前Transformer应用到图像领域主要有两大挑战： 视觉实体变化大，在不同场景下视觉Transformer性能未必很好图像分辨率高，像素点多，Transformer基于全局自注意力的计算导致计算量较大针对上述两个问题，我们…},
  langid = {english},
  organization = {知乎专栏}
}

@online{TuXiangFenLeiResNetV150,
  title = {图像分类-{{ResNet}}\_v1\_50},
  url = {https://developer.huaweicloud.com/develop/aigallery/algo/detail?id=40b66195-5bbe-463d-b8a2-03e57073538d},
  urldate = {2022-11-14},
  file = {D:\ProgramFiles\Zotero\storage\UHYLMX5E\detail.html}
}

@online{user__42AnswerSupportVector2014,
  title = {Answer to "{{Support Vector Machines}} ({{SVM}}) for Large/Very Large Datasets"},
  author = {{user\_\_42}},
  date = {2014-02-20},
  url = {https://stackoverflow.com/a/21909587},
  urldate = {2023-08-28},
  langid = {english},
  organization = {Stack Overflow}
}

@online{user2551507SupportVectorMachines2017,
  type = {Forum post},
  title = {Support {{Vector Machines}} ({{SVM}}) for Large/Very Large Datasets},
  author = {{user2551507}},
  date = {2017-09-22},
  url = {https://stackoverflow.com/q/19640175},
  urldate = {2023-08-28},
  langid = {english},
  organization = {Stack Overflow}
}

@online{UstcjingggAwesomePEFTVLMAim,
  title = {Ustcjinggg/{{Awesome-PEFT-VLM}}: {{Aim}} to Paramter-Efficient Fine-Tuning (Peft) Vision -Language Model},
  shorttitle = {Ustcjinggg/{{Awesome-PEFT-VLM}}},
  url = {https://github.com/ustcjinggg/Awesome-PEFT-VLM},
  urldate = {2024-03-15},
  abstract = {Aim to paramter-efficient fine-tuning (peft) vision -language model - ustcjinggg/Awesome-PEFT-VLM},
  langid = {english},
  organization = {GitHub},
  keywords = {⛔ No INSPIRE recid found},
  file = {D:\ProgramFiles\Zotero\storage\XYWF9VLC\Awesome-PEFT-VLM.html}
}

@article{vellidoImportanceInterpretabilityVisualization2020,
  title = {The Importance of Interpretability and Visualization in Machine Learning for Applications in Medicine and Health Care},
  author = {Vellido, Alfredo},
  date = {2020-12},
  journaltitle = {Neural Computing and Applications},
  shortjournal = {Neural Comput \& Applic},
  volume = {32},
  number = {24},
  pages = {18069--18083},
  issn = {0941-0643, 1433-3058},
  doi = {10.1007/s00521-019-04051-w},
  url = {http://link.springer.com/10.1007/s00521-019-04051-w},
  urldate = {2023-07-30},
  abstract = {In a short period of time, many areas of science have made a sharp transition towards data-dependent methods. In some cases, this process has been enabled by simultaneous advances in data acquisition and the development of networked system technologies. This new situation is particularly clear in the life sciences, where data overabundance has sparked a flurry of new methodologies for data management and analysis. This can be seen as a perfect scenario for the use of machine learning and computational intelligence techniques to address problems in which more traditional data analysis approaches might struggle. But, this scenario also poses some serious challenges. One of them is model interpretability and explainability, especially for complex nonlinear models. In some areas such as medicine and health care, not addressing such challenge might seriously limit the chances of adoption, in real practice, of computer-based systems that rely on machine learning and computational intelligence methods for data analysis. In this paper, we reflect on recent investigations about the interpretability and explainability of machine learning methods and discuss their impact on medicine and health care. We pay specific attention to one of the ways in which interpretability and explainability in this context can be addressed, which is through data and model visualization. We argue that, beyond improving model interpretability as a goal in itself, we need to integrate the medical experts in the design of data analysis interpretation strategies. Otherwise, machine learning is unlikely to become a part of routine clinical and health care practice.},
  langid = {english},
  file = {D:\ProgramFiles\Zotero\storage\3AANWKL2\Vellido - 2020 - The importance of interpretability and visualizati.pdf}
}

@article{wangClinicalAssistantDecisionmaking2023,
  title = {Clinical Assistant Decision-Making Model of Tuberculosis Based on Electronic Health Records},
  author = {Wang, Mengying and Lee, Cuixia and Wei, Zhenhao and Ji, Hong and Yang, Yingyun and Yang, Cheng},
  date = {2023-03-16},
  journaltitle = {BioData Mining},
  shortjournal = {BioData Mining},
  volume = {16},
  number = {1},
  pages = {11},
  issn = {1756-0381},
  doi = {10.1186/s13040-023-00328-y},
  url = {https://doi.org/10.1186/s13040-023-00328-y},
  urldate = {2023-07-18},
  abstract = {Tuberculosis is a dangerous infectious disease with the largest number of reported cases in China every year. Preventing missed diagnosis has an important impact on the prevention, treatment, and recovery of tuberculosis. The earliest pulmonary tuberculosis prediction models mainly used traditional image data combined with neural network models. However, a single data source tends to miss important information, such as primary symptoms and laboratory test results, that is available in multi-source data like medical records and tests. In this study, we propose a multi-stream integrated pulmonary tuberculosis diagnosis model based on structured and unstructured multi-source data from electronic health records. With the limited number of lung specialists and the high prevalence of tuberculosis, the application of this auxiliary diagnosis model can make substantial contributions to clinical settings.},
  langid = {english},
  keywords = {Clinical decision support,Electronic health records,Predictive model,Tuberculosis},
  file = {C\:\\Users\\YeCanming\\Documents\\WPS Cloud Files\\200995647\\Zotero文献库\\人工智能\\医学人工智能\\类似文章\\Wang et al_2023_Clinical assistant decision-making model of tuberculosis based on electronic.pdf;D\:\\ProgramFiles\\Zotero\\storage\\3HLTNJNF\\s13040-023-00328-y.html}
}

@article{wangFullyAutomaticDeep2020,
  title = {A Fully Automatic Deep Learning System for {{COVID-19}} Diagnostic and Prognostic Analysis},
  author = {Wang, Shuo and Zha, Yunfei and Li, Weimin and Wu, Qingxia and Li, Xiaohu and Niu, Meng and Wang, Meiyun and Qiu, Xiaoming and Li, Hongjun and Yu, He and Gong, Wei and Bai, Yan and Li, Li and Zhu, Yongbei and Wang, Liusu and Tian, Jie},
  date = {2020-08},
  journaltitle = {European Respiratory Journal},
  shortjournal = {Eur Respir J},
  volume = {56},
  number = {2},
  pages = {2000775},
  issn = {0903-1936, 1399-3003},
  doi = {10.1183/13993003.00775-2020},
  url = {http://erj.ersjournals.com/lookup/doi/10.1183/13993003.00775-2020},
  urldate = {2023-03-01},
  abstract = {Coronavirus disease 2019 (COVID-19) has spread globally, and medical resources become insufficient in many regions. Fast diagnosis of COVID-19 and finding high-risk patients with worse prognosis for early prevention and medical resource optimisation is important. Here, we proposed a fully automatic deep learning system for COVID-19 diagnostic and prognostic analysis by routinely used computed tomography.             We retrospectively collected 5372 patients with computed tomography images from seven cities or provinces. Firstly, 4106 patients with computed tomography images were used to pre-train the deep learning system, making it learn lung features. Following this, 1266 patients (924 with COVID-19 (471 had follow-up for {$>$}5\>days) and 342 with other pneumonia) from six cities or provinces were enrolled to train and externally validate the performance of the deep learning system.             In the four external validation sets, the deep learning system achieved good performance in identifying COVID-19 from other pneumonia (AUC 0.87 and 0.88, respectively) and viral pneumonia (AUC 0.86). Moreover, the deep learning system succeeded to stratify patients into high- and low-risk groups whose hospital-stay time had significant difference (p=0.013 and p=0.014, respectively). Without human assistance, the deep learning system automatically focused on abnormal areas that showed consistent characteristics with reported radiological findings.             Deep learning provides a convenient tool for fast screening of COVID-19 and identifying potential high-risk patients, which may be helpful for medical resource optimisation and early prevention before patients show severe symptoms.},
  langid = {english}
}

@article{wangGeneralizingUnseenDomains2022,
  title = {Generalizing to Unseen Domains: {{A}} Survey on Domain Generalization},
  shorttitle = {Generalizing to Unseen Domains},
  author = {Wang, Jindong and Lan, Cuiling and Liu, Chang and Ouyang, Yidong and Qin, Tao and Lu, Wang and Chen, Yiqiang and Zeng, Wenjun and Philip, S. Yu},
  date = {2022},
  journaltitle = {IEEE transactions on knowledge and data engineering},
  volume = {35},
  number = {8},
  pages = {8052--8072},
  publisher = {IEEE},
  url = {https://ieeexplore.ieee.org/abstract/document/9782500/},
  urldate = {2024-05-22},
  langid = {english},
  keywords = {⛔ No INSPIRE recid found,Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {D\:\\Zotero文献库\\00人工智能\\00计算机视觉\\经典视觉任务的对应设计\\视觉地点定位\\深度学习方法\\跨domain泛化\\Wang et al_2022_Generalizing to unseen domains.pdf;D\:\\ProgramFiles\\Zotero\\storage\\J3HMHABQ\\2103.html}
}

@article{wangHybridDecisionMaking2018,
  title = {Hybrid {{Decision Making}}: {{When Interpretable Models Collaborate With Black-Box Models}}},
  shorttitle = {Hybrid {{Decision Making}}},
  author = {Wang, Tong},
  date = {2018-02-12},
  journaltitle = {ArXiv},
  url = {https://www.semanticscholar.org/paper/Hybrid-Decision-Making%3A-When-Interpretable-Models-Wang/15e9d4517aa8f0e6b4cb13300a51e52e16359cb5},
  urldate = {2023-07-29},
  abstract = {Interpretable machine learning models have received increasing interest in recent years, especially in domains where humans are involved in the decision-making process. However, the possible loss of the task performance for gaining interpretability is often inevitable. This performance downgrade puts practitioners in a dilemma of choosing between a top-performing black-box model with no explanations and an interpretable model with unsatisfying task performance.  In this work, we propose a novel framework for building a Hybrid Decision Model that integrates an interpretable model with any black-box model to introduce explanations in the decision making process while preserving or possibly improving the predictive accuracy. We propose a novel metric, explainability, to measure the percentage of data that are sent to the interpretable model for decision. We also design a principled objective function that considers predictive accuracy, model interpretability, and data explainability. Under this framework, we develop Collaborative Black-box and RUle Set Hybrid (CoBRUSH) model that combines logic rules and any black-box model into a joint decision model. An input instance is first sent to the rules for decision. If a rule is satisfied, a decision will be directly generated. Otherwise, the black-box model is activated to decide on the instance. To train a hybrid model, we design an efficient search algorithm that exploits theoretically grounded strategies to reduce computation. Experiments show that CoBRUSH models are able to achieve same or better accuracy than their black-box collaborator working alone while gaining explainability. They also have smaller model complexity than interpretable baselines.},
  langid = {english}
}

@book{wangIntroductionTransferLearning2023,
  title = {Introduction to {{Transfer Learning}}: {{Algorithms}} and {{Practice}}},
  shorttitle = {Introduction to {{Transfer Learning}}},
  author = {Wang, Jindong and Chen, Yiqiang},
  date = {2023},
  series = {Machine {{Learning}}: {{Foundations}}, {{Methodologies}}, and {{Applications}}},
  publisher = {Springer Nature Singapore},
  location = {Singapore},
  doi = {10.1007/978-981-19-7584-4},
  url = {https://link.springer.com/10.1007/978-981-19-7584-4},
  urldate = {2024-04-05},
  isbn = {978-981-19758-3-7 978-981-19758-4-4},
  langid = {english},
  keywords = {⛔ No INSPIRE recid found},
  file = {D:\Zotero文献库\00人工智能\00机器学习课程\创新实践\迁移学习\Wang_Chen_2023_Introduction to Transfer Learning.pdf}
}

@online{wangLoRAGALowRankAdaptation2024,
  title = {{{LoRA-GA}}: {{Low-Rank Adaptation}} with {{Gradient Approximation}}},
  shorttitle = {{{LoRA-GA}}},
  author = {Wang, Shaowen and Yu, Linxi and Li, Jian},
  date = {2024-07-06},
  url = {https://arxiv.org/abs/2407.05000v2},
  urldate = {2024-09-26},
  abstract = {Fine-tuning large-scale pretrained models is prohibitively expensive in terms of computational and memory costs. LoRA, as one of the most popular Parameter-Efficient Fine-Tuning (PEFT) methods, offers a cost-effective alternative by fine-tuning an auxiliary low-rank model that has significantly fewer parameters. Although LoRA reduces the computational and memory requirements significantly at each iteration, extensive empirical evidence indicates that it converges at a considerably slower rate compared to full fine-tuning, ultimately leading to increased overall compute and often worse test performance. In our paper, we perform an in-depth investigation of the initialization method of LoRA and show that careful initialization (without any change of the architecture and the training algorithm) can significantly enhance both efficiency and performance. In particular, we introduce a novel initialization method, LoRA-GA (Low Rank Adaptation with Gradient Approximation), which aligns the gradients of low-rank matrix product with those of full fine-tuning at the first step. Our extensive experiments demonstrate that LoRA-GA achieves a convergence rate comparable to that of full fine-tuning (hence being significantly faster than vanilla LoRA as well as various recent improvements) while simultaneously attaining comparable or even better performance. For example, on the subset of the GLUE dataset with T5-Base, LoRA-GA outperforms LoRA by 5.69\% on average. On larger models such as Llama 2-7B, LoRA-GA shows performance improvements of 0.34, 11.52\%, and 5.05\% on MT-bench, GSM8K, and Human-eval, respectively. Additionally, we observe up to 2-4 times convergence speed improvement compared to vanilla LoRA, validating its effectiveness in accelerating convergence and enhancing model performance. Code is available at https://github.com/Outsider565/LoRA-GA.},
  langid = {english},
  organization = {arXiv.org},
  keywords = {⛔ No INSPIRE recid found},
  file = {D:\Zotero文献库\00人工智能\00机器学习\学习范式\迁移学习\PEFT\基于参数\lora类\Wang et al_2024_LoRA-GA.pdf}
}

@online{wangRevisitingPowerPrompt2024,
  title = {Revisiting the {{Power}} of {{Prompt}} for {{Visual Tuning}}},
  author = {Wang, Yuzhu and Cheng, Lechao and Fang, Chaowei and Zhang, Dingwen and Duan, Manni and Wang, Meng},
  date = {2024-02-04},
  eprint = {2402.02382},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2402.02382},
  url = {http://arxiv.org/abs/2402.02382},
  urldate = {2024-03-25},
  abstract = {Visual prompt tuning (VPT) is a promising solution incorporating learnable prompt tokens to customize pre-trained models for downstream tasks. However, VPT and its variants often encounter challenges like prompt initialization, prompt length, and subpar performance in self-supervised pretraining, hindering successful contextual adaptation. This study commences by exploring the correlation evolvement between prompts and patch tokens during proficient training. Inspired by the observation that the prompt tokens tend to share high mutual information with patch tokens, we propose initializing prompts with downstream token prototypes. The strategic initialization, a stand-in for the previous initialization, substantially improves performance in fine-tuning. To refine further, we optimize token construction with a streamlined pipeline that maintains excellent performance with almost no increase in computational expenses compared to VPT. Exhaustive experiments show our proposed approach outperforms existing methods by a remarkable margin. For instance, it surpasses full fine-tuning in 19 out of 24 tasks, using less than 0.4\% of learnable parameters on the FGVC and VTAB-1K benchmarks. Notably, our method significantly advances the adaptation for self-supervised pretraining, achieving impressive task performance gains of at least 10\% to 30\%. Besides, the experimental results demonstrate the proposed SPT is robust to prompt lengths and scales well with model capacity and training data size. We finally provide an insightful exploration into the amount of target data facilitating the adaptation of pre-trained models to downstream tasks.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {⛔ No INSPIRE recid found,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {D\:\\Zotero文献库\\00人工智能\\00计算机视觉课程\\经典视觉任务的对应设计\\视觉地点定位\\创新点\\PEFT\\prompt类\\Wang et al_2024_Revisiting the Power of Prompt for Visual Tuning.pdf;D\:\\ProgramFiles\\Zotero\\storage\\7GGHGLUR\\2402.html}
}

@article{wangTransHPImageClassification2023,
  title = {{{TransHP}}: {{Image Classification}} with {{Hierarchical Prompting}}},
  shorttitle = {{{TransHP}}},
  author = {Wang, Wenhao and Sun, Yifan and Li, Wei and Yang, Yi},
  date = {2023-12-15},
  journaltitle = {Advances in Neural Information Processing Systems},
  volume = {36},
  pages = {28187--28200},
  url = {https://proceedings.neurips.cc/paper_files/paper/2023/hash/59b7c1e1716c4feadefd6c70b1dd4630-Abstract-Conference.html},
  urldate = {2024-04-05},
  langid = {english},
  keywords = {⛔ No INSPIRE recid found},
  file = {D:\Zotero文献库\00人工智能\00机器学习\学习范式\多任务学习\多层级多标签分类\深度学习方法\Wang et al_2023_TransHP.pdf}
}

@online{WanQuanSuiJiSheJiLiangYangBenDeWilcoxonJianYanYuKSJianYanGongXiaoBiJiaoAMiner,
  title = {{{完全随机设计两样本的Wilcoxon检验与K-S检验功效比较}} - {{AMiner}}},
  url = {https://www.aminer.cn/pub/53e9bb08b7602d9704741c4f/a-comparison-of-the-power-of-the-wilcoxon-rank-sum-test-and},
  urldate = {2023-03-07}
}

@article{warnat-herresthalSwarmLearningDecentralized2021,
  title = {Swarm {{Learning}} for Decentralized and Confidential Clinical Machine Learning},
  author = {Warnat-Herresthal, Stefanie and Schultze, Hartmut and Shastry, Krishnaprasad Lingadahalli and Manamohan, Sathyanarayanan and Mukherjee, Saikat and Garg, Vishesh and Sarveswara, Ravi and Händler, Kristian and Pickkers, Peter and Aziz, N. Ahmad and Ktena, Sofia and Tran, Florian and Bitzer, Michael and Ossowski, Stephan and Casadei, Nicolas and Herr, Christian and Petersheim, Daniel and Behrends, Uta and Kern, Fabian and Fehlmann, Tobias and Schommers, Philipp and Lehmann, Clara and Augustin, Max and Rybniker, Jan and Altmüller, Janine and Mishra, Neha and Bernardes, Joana P. and Krämer, Benjamin and Bonaguro, Lorenzo and Schulte-Schrepping, Jonas and De Domenico, Elena and Siever, Christian and Kraut, Michael and Desai, Milind and Monnet, Bruno and Saridaki, Maria and Siegel, Charles Martin and Drews, Anna and Nuesch-Germano, Melanie and Theis, Heidi and Heyckendorf, Jan and Schreiber, Stefan and Kim-Hellmuth, Sarah and {COVID-19 Aachen Study (COVAS)} and Balfanz, Paul and Eggermann, Thomas and Boor, Peter and Hausmann, Ralf and Kuhn, Hannah and Isfort, Susanne and Stingl, Julia Carolin and Schmalzing, Günther and Kuhl, Christiane K. and Röhrig, Rainer and Marx, Gernot and Uhlig, Stefan and Dahl, Edgar and Müller-Wieland, Dirk and Dreher, Michael and Marx, Nikolaus and Nattermann, Jacob and Skowasch, Dirk and Kurth, Ingo and Keller, Andreas and Bals, Robert and Nürnberg, Peter and Rieß, Olaf and Rosenstiel, Philip and Netea, Mihai G. and Theis, Fabian and Mukherjee, Sach and Backes, Michael and Aschenbrenner, Anna C. and Ulas, Thomas and {Deutsche COVID-19 Omics Initiative (DeCOI)} and Angelov, Angel and Bartholomäus, Alexander and Becker, Anke and Bezdan, Daniela and Blumert, Conny and Bonifacio, Ezio and Bork, Peer and Boyke, Bunk and Blum, Helmut and Clavel, Thomas and Colome-Tatche, Maria and Cornberg, Markus and De La Rosa Velázquez, Inti Alberto and Diefenbach, Andreas and Dilthey, Alexander and Fischer, Nicole and Förstner, Konrad and Franzenburg, Sören and Frick, Julia-Stefanie and Gabernet, Gisela and Gagneur, Julien and Ganzenmueller, Tina and Gauder, Marie and Geißert, Janina and Goesmann, Alexander and Göpel, Siri and Grundhoff, Adam and Grundmann, Hajo and Hain, Torsten and Hanses, Frank and Hehr, Ute and Heimbach, André and Hoeper, Marius and Horn, Friedemann and Hübschmann, Daniel and Hummel, Michael and Iftner, Thomas and Iftner, Angelika and Illig, Thomas and Janssen, Stefan and Kalinowski, Jörn and Kallies, René and Kehr, Birte and Keppler, Oliver T. and Klein, Christoph and Knop, Michael and Kohlbacher, Oliver and Köhrer, Karl and Korbel, Jan and Kremsner, Peter G. and Kühnert, Denise and Landthaler, Markus and Li, Yang and Ludwig, Kerstin U. and Makarewicz, Oliwia and Marz, Manja and McHardy, Alice C. and Mertes, Christian and Münchhoff, Maximilian and Nahnsen, Sven and Nöthen, Markus and Ntoumi, Francine and Overmann, Jörg and Peter, Silke and Pfeffer, Klaus and Pink, Isabell and Poetsch, Anna R. and Protzer, Ulrike and Pühler, Alfred and Rajewsky, Nikolaus and Ralser, Markus and Reiche, Kristin and Ripke, Stephan and Da Rocha, Ulisses Nunes and Saliba, Antoine-Emmanuel and Sander, Leif Erik and Sawitzki, Birgit and Scheithauer, Simone and Schiffer, Philipp and Schmid-Burgk, Jonathan and Schneider, Wulf and Schulte, Eva-Christina and Sczyrba, Alexander and Sharaf, Mariam L. and Singh, Yogesh and Sonnabend, Michael and Stegle, Oliver and Stoye, Jens and Vehreschild, Janne and Velavan, Thirumalaisamy P. and Vogel, Jörg and Volland, Sonja and Von Kleist, Max and Walker, Andreas and Walter, Jörn and Wieczorek, Dagmar and Winkler, Sylke and Ziebuhr, John and Breteler, Monique M. B. and Giamarellos-Bourboulis, Evangelos J. and Kox, Matthijs and Becker, Matthias and Cheran, Sorin and Woodacre, Michael S. and Goh, Eng Lim and Schultze, Joachim L.},
  date = {2021-06-10},
  journaltitle = {Nature},
  shortjournal = {Nature},
  volume = {594},
  number = {7862},
  pages = {265--270},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/s41586-021-03583-3},
  url = {https://www.nature.com/articles/s41586-021-03583-3},
  urldate = {2023-07-30},
  abstract = {Abstract                            Fast and reliable detection of patients with severe and heterogeneous illnesses is a major goal of precision medicine               1,2               . Patients with leukaemia can be identified using machine learning on the basis of their blood transcriptomes               3               . However, there is an increasing divide between what is technically possible and what is allowed, because of privacy legislation               4,5               . Here, to facilitate the integration of any medical data from any data owner worldwide without violating privacy laws, we introduce Swarm Learning—a decentralized machine-learning approach that unites edge computing, blockchain-based peer-to-peer networking and coordination while maintaining confidentiality without the need for a central coordinator, thereby going beyond federated learning. To illustrate the feasibility of using Swarm Learning to develop disease classifiers using distributed data, we chose four use cases of heterogeneous diseases (COVID-19, tuberculosis, leukaemia and lung pathologies). With more than 16,400 blood transcriptomes derived from 127 clinical studies with non-uniform distributions of cases and controls and substantial study biases, as well as more than 95,000 chest X-ray images, we show that Swarm Learning classifiers outperform those developed at individual sites. In addition, Swarm Learning completely fulfils local confidentiality regulations by design. We believe that this approach will notably accelerate the introduction of precision medicine.},
  langid = {english},
  file = {D:\ProgramFiles\Zotero\storage\NUAT7Y6R\Warnat-Herresthal 等 - 2021 - Swarm Learning for decentralized and confidential .pdf}
}

@article{wehrmannHierarchicalMultiLabelClassification,
  title = {Hierarchical {{Multi-Label Classification Networks}}},
  author = {Wehrmann, Jônatas and Cerri, Ricardo and Barros, Rodrigo C},
  abstract = {One of the most challenging machine learning problems is a particular case of data classification in which classes are hierarchically structured and objects can be assigned to multiple paths of the class hierarchy at the same time. This task is known as hierarchical multi-label classification (HMC), with applications in text classification, image annotation, and in bioinformatics problems such as protein function prediction. In this paper, we propose novel neural network architectures for HMC called HMCN, capable of simultaneously optimizing local and global loss functions for discovering local hierarchical class-relationships and global information from the entire class hierarchy while penalizing hierarchical violations. We evaluate its performance in 21 datasets from four distinct domains, and we compare it against the current HMC state-of-the-art approaches. Results show that HMCN substantially outperforms all baselines with statistical significance, arising as the novel state-of-the-art for HMC.},
  langid = {english},
  keywords = {⛔ No INSPIRE recid found},
  file = {D:\ProgramFiles\Zotero\storage\4DHW2P5R\Wehrmann 等 - Hierarchical Multi-Label Classification Networks.pdf}
}

@online{WeiRuanZhiHuiaiAIXinChongPromptLearning,
  title = {{{AI新宠}}：{{Prompt Learning}},用提示学习调教大模型| {{微软ATP Public}} 100公益演讲\#026\_哔哩哔哩\_bilibili},
  author = {微软智汇AI},
  url = {https://www.bilibili.com/video/BV1ax4y1T7x4/},
  urldate = {2023-06-21},
  abstract = {本系列直播课由苏州人工智能产业创新中心、图灵社区 @图灵社区 联合主办，苏州国际科技园和蒲公英孵化器协办，同时也感谢 @微软Reactor\_SH 的支持！目前，人工智能新技术与元宇宙新赛道的硬核新科技不断涌现，众多企业都在思考——如何在产业新集群、新场景和新治理中，通过AI技术赋能，实现智能时代下的数字化转型。系列课程致力于能够让所有参与者进一步了解，无论企业规模大小都能获得先进的AI体验，以, 视频播放量 15479、弹幕量 11、点赞数 689、投硬币枚数 219、收藏人数 1662、转发人数 346, 视频作者 微软智汇AI, 作者简介 【微软ATP】Microsoft AI Talent Program是微软（亚洲）互联网工程院下属提供专业AI知识培训、企业级咨询服务的项目，相关视频：B站首推！官方精品【完整版九集】中文字幕，ChatGPT Prompt提示词课程 斯坦福吴恩达 | OpenAl官方联合出品，【清华NLP】刘知远团队大模型公开课全网首发｜带你从入门到实战，Prompt Learning（提示/模板学习），ChatGPT提示工程师\&AI大神吴恩达教你写提示词｜prompt engineering【完整中字九集全】，大模型prompt learning研讨会 智源/清华大学，我们做了一个免费AI口语陪练 Oral Craft，【45分钟系统学习】ChatGPT Prompt提示词工程 基础{$>$}少样本{$>$}思维链{$>$}联网{$>$}认知搜索{$>$}ReAct实现ChatGPT插件，用AI工具生成我奶奶的虚拟数字人，用ChatGPT学英语！托福雅思四六级无痛拿下！！，ChatGPT爆火！最全prompt工程指南称霸GitHub热榜，7天标星7k！},
  langid = {english},
  file = {D:\ProgramFiles\Zotero\storage\A8933GX4\BV1ax4y1T7x4.html}
}

@online{XianJinDeAutoMLJiShu2021,
  title = {先进的 {{AutoML}} 技术，现在使用 {{NVIDIA GPU}} 和 {{RAPIDS}} 速度提高了 10 倍},
  date = {2021-06-09T09:19+00:00},
  url = {https://developer.nvidia.com/zh-cn/blog/advancing-the-state-of-the-art-in-automl-now-10x-faster-with-nvidia-gpus-and-rapids/},
  urldate = {2023-08-25},
  abstract = {为了获得最先进的机器学习（ ML ）解决方案，数据科学家通常建立复杂的 ML 模型。然而， 这些技术的计算成本很高，直到最近还需要广泛的背景知识、经验和人力。 最近，在 GTC21 ， AWS 高级数据科学家 尼克·埃里克森 给出了一个 session 分享如何结合 AutoGluon ， RAPIDS 和 NVIDIA GPU 计算简化实现最先进的 ML 精度，同时提高性能和降低成本。},
  langid = {english},
  organization = {NVIDIA 技术博客},
  file = {D:\ProgramFiles\Zotero\storage\7N47DTYK\advancing-the-state-of-the-art-in-automl-now-10x-faster-with-nvidia-gpus-and-rapids.html}
}

@online{xinParameterEfficientFineTuningPreTrained2024,
  title = {Parameter-{{Efficient Fine-Tuning}} for {{Pre-Trained Vision Models}}: {{A Survey}}},
  shorttitle = {Parameter-{{Efficient Fine-Tuning}} for {{Pre-Trained Vision Models}}},
  author = {Xin, Yi and Luo, Siqi and Zhou, Haodi and Du, Junlong and Liu, Xiaohong and Fan, Yue and Li, Qing and Du, Yuntao},
  date = {2024-02-08},
  eprint = {2402.02242},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2402.02242},
  urldate = {2024-09-11},
  abstract = {Large-scale pre-trained vision models (PVMs) have shown great potential for adaptability across various downstream vision tasks. However, with stateof-the-art PVMs growing to billions or even trillions of parameters, the standard full fine-tuning paradigm is becoming unsustainable due to high computational and storage demands. In response, researchers are exploring parameter-efficient finetuning (PEFT), which seeks to exceed the performance of full fine-tuning with minimal parameter modifications. This survey provides a comprehensive overview and future directions for visual PEFT, offering a systematic review of the latest advancements. First, we provide a formal definition of PEFT and discuss model pre-training methods. We then categorize existing methods into three categories: addition-based, partial-based, and unified-based. Finally, we introduce the commonly used datasets and applications and suggest potential future research challenges. A comprehensive collection of resources is available at https://github.com/synbol/ Awesome-Parameter-Efficient-Transfer-Learning.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {⛔ No INSPIRE recid found,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {D:\ProgramFiles\Zotero\storage\FVV2RGYJ\Xin 等 - 2024 - Parameter-Efficient Fine-Tuning for Pre-Trained Vi.pdf}
}

@online{yangFeatureLearningInfiniteWidth2022,
  title = {Feature {{Learning}} in {{Infinite-Width Neural Networks}}},
  author = {Yang, Greg and Hu, Edward J.},
  date = {2022-07-15},
  eprint = {2011.14522},
  eprinttype = {arXiv},
  eprintclass = {cond-mat},
  doi = {10.48550/arXiv.2011.14522},
  url = {http://arxiv.org/abs/2011.14522},
  urldate = {2024-09-09},
  abstract = {As its width tends to infinity, a deep neural network's behavior under gradient descent can become simplified and predictable (e.g. given by the Neural Tangent Kernel (NTK)), if it is parametrized appropriately (e.g. the NTK parametrization). However, we show that the standard and NTK parametrizations of a neural network do not admit infinite-width limits that can learn features, which is crucial for pretraining and transfer learning such as with BERT. We propose simple modifications to the standard parametrization to allow for feature learning in the limit. Using the *Tensor Programs* technique, we derive explicit formulas for such limits. On Word2Vec and few-shot learning on Omniglot via MAML, two canonical tasks that rely crucially on feature learning, we compute these limits exactly. We find that they outperform both NTK baselines and finite-width networks, with the latter approaching the infinite-width feature learning performance as width increases. More generally, we classify a natural space of neural network parametrizations that generalizes standard, NTK, and Mean Field parametrizations. We show 1) any parametrization in this space either admits feature learning or has an infinite-width training dynamics given by kernel gradient descent, but not both; 2) any such infinite-width limit can be computed using the Tensor Programs technique. Code for our experiments can be found at github.com/edwardjhu/TP4.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {⛔ No INSPIRE recid found,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Condensed Matter - Disordered Systems and Neural Networks},
  file = {D\:\\Zotero文献库\\00人工智能\\00机器学习\\学习范式\\迁移学习\\PEFT\\基于参数\\lora类\\Yang_Hu_2022_Feature Learning in Infinite-Width Neural Networks.pdf;D\:\\ProgramFiles\\Zotero\\storage\\2S6P3DNH\\2011.html}
}

@article{yangRiskPredictionDiabetes2021,
  title = {Risk {{Prediction}} of {{Diabetes}}: {{Big}} Data Mining with Fusion of Multifarious Physical Examination Indicators},
  shorttitle = {Risk {{Prediction}} of {{Diabetes}}},
  author = {Yang, Hui and Luo, Yamei and Ren, Xiaolei and Wu, Ming and He, Xiaolin and Peng, Bowen and Deng, Kejun and Yan, Dan and Tang, Hua and Lin, Hao},
  date = {2021-11},
  journaltitle = {Information Fusion},
  shortjournal = {Information Fusion},
  volume = {75},
  pages = {140--149},
  issn = {15662535},
  doi = {10.1016/j.inffus.2021.02.015},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S1566253521000397},
  urldate = {2023-07-30},
  abstract = {Diabetes is a global epidemic. Long-term exposure to hyperglycemia can cause chronic damage to various tissues. Thus, early diagnosis of diabetes is crucial. In this study, we designed a computational system to predict diabetes risk by fusing multifarious types of physical examination data. We collected 1,507,563 physical examination data of healthy people and diabetes patients, as well as 387,076 physical examination data from the follow-up records from 2011 to 2017 of diabetes patients in Luzhou City in China. Three types of physical examination indexes were statistically analyzed: demographics, vital signs, and laboratory values. To distinguish diabetes patients from healthy people, a model based on eXtreme Gradient Boosting (XGBoost) was developed, which could produce an area under the receiver operating characteristic curve (AUC) of 0.8768. Moreover, to improve the convenience and flexibility of the model in clinical and real-life scenarios, a diabetes risk scorecard was established based on logistic regression, which could evaluate human health. Lastly, we statistically analyzed the data from the follow-up records to identify the key factors influencing patient control of their conditions. To improve the diabetes cascade screening and personal lifestyle management, an online diabetes risk assessment system was established, which can be freely accessed at http://lin-group.cn/server/DRSC/index.html. This system is expected to provide guidance for human health management.},
  langid = {english},
  file = {D:\ProgramFiles\Zotero\storage\7R5HH2U6\Yang 等 - 2021 - Risk Prediction of Diabetes Big data mining with .pdf}
}

@article{yangRiskPredictionDiabetes2021a,
  title = {Risk {{Prediction}} of {{Diabetes}}: {{Big}} Data Mining with Fusion of Multifarious Physical Examination Indicators},
  shorttitle = {Risk {{Prediction}} of {{Diabetes}}},
  author = {Yang, Hui and Luo, Yamei and Ren, Xiaolei and Wu, Ming and He, Xiaolin and Peng, Bowen and Deng, Kejun and Yan, Dan and Tang, Hua and Lin, Hao},
  date = {2021-11},
  journaltitle = {Information Fusion},
  shortjournal = {Information Fusion},
  volume = {75},
  pages = {140--149},
  issn = {15662535},
  doi = {10.1016/j.inffus.2021.02.015},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S1566253521000397},
  urldate = {2023-08-16},
  abstract = {Diabetes is a global epidemic. Long-term exposure to hyperglycemia can cause chronic damage to various tissues. Thus, early diagnosis of diabetes is crucial. In this study, we designed a computational system to predict diabetes risk by fusing multifarious types of physical examination data. We collected 1,507,563 physical examination data of healthy people and diabetes patients, as well as 387,076 physical examination data from the follow-up records from 2011 to 2017 of diabetes patients in Luzhou City in China. Three types of physical examination indexes were statistically analyzed: demographics, vital signs, and laboratory values. To distinguish diabetes patients from healthy people, a model based on eXtreme Gradient Boosting (XGBoost) was developed, which could produce an area under the receiver operating characteristic curve (AUC) of 0.8768. Moreover, to improve the convenience and flexibility of the model in clinical and real-life scenarios, a diabetes risk scorecard was established based on logistic regression, which could evaluate human health. Lastly, we statistically analyzed the data from the follow-up records to identify the key factors influencing patient control of their conditions. To improve the diabetes cascade screening and personal lifestyle management, an online diabetes risk assessment system was established, which can be freely accessed at http://lin-group.cn/server/DRSC/index.html. This system is expected to provide guidance for human health management.},
  langid = {english},
  file = {C:\Users\YeCanming\Documents\WPS Cloud Files\200995647\Zotero文献库\人工智能\医学人工智能\类似文章\Yang et al_2021_Risk Prediction of Diabetes.pdf}
}

@inproceedings{yangTensorProgramsIV2021,
  title = {Tensor {{Programs IV}}: {{Feature Learning}} in {{Infinite-Width Neural Networks}}},
  shorttitle = {Tensor {{Programs IV}}},
  booktitle = {Proceedings of the 38th {{International Conference}} on {{Machine Learning}}},
  author = {Yang, Greg and Hu, Edward J.},
  date = {2021-07-01},
  pages = {11727--11737},
  publisher = {PMLR},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v139/yang21c.html},
  urldate = {2024-09-09},
  abstract = {As its width tends to infinity, a deep neural network’s behavior under gradient descent can become simplified and predictable (e.g. given by the Neural Tangent Kernel (NTK)), if it is parametrized appropriately (e.g. the NTK parametrization). However, we show that the standard and NTK parametrizations of a neural network do not admit infinite-width limits that can *learn* features, which is crucial for pretraining and transfer learning such as with BERT. We propose simple modifications to the standard parametrization to allow for feature learning in the limit. Using the *Tensor Programs* technique, we derive explicit formulas for such limits. On Word2Vec and few-shot learning on Omniglot via MAML, two canonical tasks that rely crucially on feature learning, we compute these limits exactly. We find that they outperform both NTK baselines and finite-width networks, with the latter approaching the infinite-width feature learning performance as width increases.},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  langid = {english},
  keywords = {⛔ No INSPIRE recid found},
  file = {D\:\\ProgramFiles\\Zotero\\storage\\Y4X6T762\\Yang 和 Hu - 2021 - Tensor Programs IV Feature Learning in Infinite-W.pdf;D\:\\Zotero文献库\\00人工智能\\00机器学习\\学习范式\\迁移学习\\PEFT\\基于参数\\lora类\\Yang_Hu_2021_Tensor Programs IV.pdf}
}

@book{yangTransferLearning2020,
  title = {Transfer {{Learning}}},
  author = {Yang, Qiang and Zhang, Yu and Dai, Wenyuan and Pan, Sinno Jialin},
  date = {2020-01-31},
  edition = {1},
  publisher = {Cambridge University Press},
  doi = {10.1017/9781139061773},
  url = {https://www.cambridge.org/core/product/identifier/9781139061773/type/book},
  urldate = {2024-04-05},
  isbn = {978-1-139-06177-3 978-1-107-01690-3},
  langid = {english},
  keywords = {⛔ No INSPIRE recid found},
  file = {D\:\\ProgramFiles\\Zotero\\storage\\5JHEITAH\\迁移学习(优化有书签) (杨强  张宇  戴文渊  潘嘉林) (Z-Library).pdf;D\:\\Zotero文献库\\00人工智能\\00机器学习课程\\创新实践\\迁移学习\\Yang et al_2020_Transfer Learning.pdf}
}

@article{yangUnboxBlackboxMedical2022,
  title = {Unbox the Black-Box for the Medical Explainable {{AI}} via Multi-Modal and Multi-Centre Data Fusion: {{A}} Mini-Review, Two Showcases and Beyond},
  shorttitle = {Unbox the Black-Box for the Medical Explainable {{AI}} via Multi-Modal and Multi-Centre Data Fusion},
  author = {Yang, Guang and Ye, Qinghao and Xia, Jun},
  date = {2022-01},
  journaltitle = {Information Fusion},
  shortjournal = {Information Fusion},
  volume = {77},
  pages = {29--52},
  issn = {15662535},
  doi = {10.1016/j.inffus.2021.07.016},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S1566253521001597},
  urldate = {2023-07-30},
  abstract = {Explainable Artificial Intelligence (XAI) is an emerging research topic of machine learning aimed at unboxing how AI systems’ black-box choices are made. This research field inspects the measures and models involved in decision-making and seeks solutions to explain them explicitly. Many of the machine learning algorithms cannot manifest how and why a decision has been cast. This is particularly true of the most popular deep neural network approaches currently in use. Consequently, our confidence in AI systems can be hindered by the lack of explainability in these black-box models. The XAI becomes more and more crucial for deep learning powered applications, especially for medical and healthcare studies, although in general these deep neural networks can return an arresting dividend in performance. The insufficient explainability and transparency in most existing AI systems can be one of the major reasons that successful implementation and integration of AI tools into routine clinical practice are uncommon. In this study, we first surveyed the current progress of XAI and in particular its advances in healthcare applications. We then introduced our solutions for XAI leveraging multi-modal and multi-centre data fusion, and subsequently validated in two showcases following real clinical scenarios. Comprehensive quantitative and qualitative analyses can prove the efficacy of our proposed XAI solutions, from which we can envisage successful applications in a broader range of clinical questions.},
  langid = {english},
  file = {D:\ProgramFiles\Zotero\storage\MFDGHNB6\Yang 等 - 2022 - Unbox the black-box for the medical explainable AI.pdf}
}

@online{yanHDCNNHierarchicalDeep2015,
  title = {{{HD-CNN}}: {{Hierarchical Deep Convolutional Neural Network}} for {{Large Scale Visual Recognition}}},
  shorttitle = {{{HD-CNN}}},
  author = {Yan, Zhicheng and Zhang, Hao and Piramuthu, Robinson and Jagadeesh, Vignesh and DeCoste, Dennis and Di, Wei and Yu, Yizhou},
  date = {2015-05-15},
  eprint = {1410.0736},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.1410.0736},
  url = {http://arxiv.org/abs/1410.0736},
  urldate = {2024-05-08},
  abstract = {In image classification, visual separability between different object categories is highly uneven, and some categories are more difficult to distinguish than others. Such difficult categories demand more dedicated classifiers. However, existing deep convolutional neural networks (CNN) are trained as flat N-way classifiers, and few efforts have been made to leverage the hierarchical structure of categories. In this paper, we introduce hierarchical deep CNNs (HD-CNNs) by embedding deep CNNs into a category hierarchy. An HD-CNN separates easy classes using a coarse category classifier while distinguishing difficult classes using fine category classifiers. During HD-CNN training, component-wise pretraining is followed by global finetuning with a multinomial logistic loss regularized by a coarse category consistency term. In addition, conditional executions of fine category classifiers and layer parameter compression make HD-CNNs scalable for large-scale visual recognition. We achieve state-of-the-art results on both CIFAR100 and large-scale ImageNet 1000-class benchmark datasets. In our experiments, we build up three different HD-CNNs and they lower the top-1 error of the standard CNNs by 2.65\%, 3.1\% and 1.1\%, respectively.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {⛔ No INSPIRE recid found,Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {D\:\\Zotero文献库\\00人工智能\\00机器学习\\学习范式\\多任务学习\\多层级多标签分类\\深度学习方法\\Yan et al_2015_HD-CNN.pdf;D\:\\ProgramFiles\\Zotero\\storage\\X43I65Z2\\1410.html}
}

@inproceedings{yanOptimizingClassifierPerformance2003,
  title = {Optimizing {{Classifier Performance}} via an {{Approximation}} to the {{Wilcoxon-Mann-Whitney Statistic}}},
  author = {Yan, Lian and Dodier, R. and Mozer, M. and Wolniewicz, R.},
  date = {2003-08-21},
  url = {https://www.semanticscholar.org/paper/Optimizing-Classifier-Performance-via-an-to-the-Yan-Dodier/df27dde10589455d290eeee6d0ae6ceeb83d0c6b?p2df},
  urldate = {2022-12-15},
  abstract = {When the goal is to achieve the best correct classification rate, cross entropy and mean squared error are typical cost functions used to optimize classifier performance. However, for many real-world classification problems, the ROC curve is a more meaningful performance measure. We demonstrate that minimizing cross entropy or mean squared error does not necessarily maximize the area under the ROC curve (AUC). We then consider alternative objective functions for training a classifier to maximize the AUC directly. We propose an objective function that is an approximation to the Wilcoxon-Mann-Whitney statistic, which is equivalent to the AUC. The proposed objective function is differentiable, so gradient-based methods can be used to train the classifier. We apply the new objective function to real-world customer behavior prediction problems for a wireless service provider and a cable service provider, and achieve reliable improvements in the ROC curve.},
  eventtitle = {International {{Conference}} on {{Machine Learning}}}
}

@software{YeCanMingMachineLearning2022,
  title = {Machine {{Learning}}},
  author = {叶璨铭},
  date = {2022-09-08T03:03:33Z},
  origdate = {2022-09-07T09:43:10Z},
  url = {https://github.com/2catycm/SUSTech-Machine-Learning-Lab-Solutions},
  urldate = {2022-09-28},
  abstract = {Machine-Learning}
}

@inreference{YiChangZhi2022,
  title = {异常值},
  booktitle = {维基百科，自由的百科全书},
  date = {2022-12-23T15:17:43Z},
  url = {https://zh.wikipedia.org/w/index.php?title=%E5%BC%82%E5%B8%B8%E5%80%BC&oldid=75185824},
  urldate = {2023-11-09},
  abstract = {在统计学中，异常值（又称离群值）是指与其他观测值有显著差异的数据点。异常值可能是由实验误差造成；后者有时会从数据集中排除。异常值可能会导致统计分析中出现严重问题。 能妥善处理异常值的估计量，称为“稳健”。例如，中位数是集中趋势的稳健统计量，但平均数则不然。},
  langid = {chinese},
  annotation = {Page Version ID: 75185824},
  file = {D:\ProgramFiles\Zotero\storage\ZW7K2E7Q\异常值.html}
}

@online{YiCongLombokQianYiDaoKotlinALiYunKaiFaZheSheQu,
  title = {[译]{{从Lombok迁移到Kotlin-阿里云开发者社区}}},
  url = {https://developer.aliyun.com/article/769984},
  urldate = {2022-11-14},
  abstract = {{$>$} 原文地址: https://dzone.com/articles/migrating-from-lombok-to-kotlin {$>$} 更短的代码不是目的，只有更可读的代码才是 作为一个Java开发者，最常见的抱怨是对Java语言冗长的抱怨。而其中出现最多的就是数据类。 数据类，或者元祖，或者record记录类，未来在Java语言可能会消失，但在那天之前，任何时间创建一个rest d},
  file = {D:\ProgramFiles\Zotero\storage\ZFI4JW3H\769984.html}
}

@online{YiWenKanDongTuiJianXiTongGateWangLuo2BaiDuGemNN,
  title = {一文看懂推荐系统：{{Gate网络2}}：{{百度GemNN}}（{{Gating-Enhanced Multi-Task Neural Networks}}）\_冰露可乐的博客-{{CSDN博客}}},
  url = {https://blog.csdn.net/weixin_46838716/article/details/126567049},
  urldate = {2022-10-16},
  file = {D:\ProgramFiles\Zotero\storage\FUIGRAMD\126567049.html}
}

@online{yuanLargescaleRobustDeep2021,
  title = {Large-Scale {{Robust Deep AUC Maximization}}: {{A New Surrogate Loss}} and {{Empirical Studies}} on {{Medical Image Classification}}},
  shorttitle = {Large-Scale {{Robust Deep AUC Maximization}}},
  author = {Yuan, Zhuoning and Yan, Yan and Sonka, Milan and Yang, Tianbao},
  date = {2021-09-07},
  eprint = {2012.03173},
  eprinttype = {arXiv},
  eprintclass = {cs, math, stat},
  doi = {10.48550/arXiv.2012.03173},
  url = {http://arxiv.org/abs/2012.03173},
  urldate = {2023-01-11},
  abstract = {Deep AUC Maximization (DAM) is a new paradigm for learning a deep neural network by maximizing the AUC score of the model on a dataset. Most previous works of AUC maximization focus on the perspective of optimization by designing efficient stochastic algorithms, and studies on generalization performance of large-scale DAM on difficult tasks are missing. In this work, we aim to make DAM more practical for interesting real-world applications (e.g., medical image classification). First, we propose a new margin-based min-max surrogate loss function for the AUC score (named as AUC min-max-margin loss or simply AUC margin loss for short). It is more robust than the commonly used AUC square loss, while enjoying the same advantage in terms of large-scale stochastic optimization. Second, we conduct extensive empirical studies of our DAM method on four difficult medical image classification tasks, namely (i) classification of chest x-ray images for identifying many threatening diseases, (ii) classification of images of skin lesions for identifying melanoma, (iii) classification of mammogram for breast cancer screening, and (iv) classification of microscopic images for identifying tumor tissue. Our studies demonstrate that the proposed DAM method improves the performance of optimizing cross-entropy loss by a large margin, and also achieves better performance than optimizing the existing AUC square loss on these medical image classification tasks. Specifically, our DAM method has achieved the 1st place on Stanford CheXpert competition on Aug. 31, 2020. To the best of our knowledge, this is the first work that makes DAM succeed on large-scale medical image datasets. We also conduct extensive ablation studies to demonstrate the advantages of the new AUC margin loss over the AUC square loss on benchmark datasets. The proposed method is implemented in our open-sourced library LibAUC (www.libauc.org).},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Mathematics - Optimization and Control,Statistics - Machine Learning},
  file = {C\:\\Users\\YeCanming\\Documents\\WPS Cloud Files\\200995647\\Zotero文献库\\人工智能\\创新实践\\损失函数\\Yuan et al_2021_Large-scale Robust Deep AUC Maximization.pdf;D\:\\ProgramFiles\\Zotero\\storage\\D4TLHSZH\\2012.html}
}

@online{YuanXueXiMAMLSuanFaXiangJiePinnShanLiWaDeBoKeCSDNBoKeMamlSuanFa,
  title = {{{元学习MAML算法详解}}\_pinn山里娃的博客-{{CSDN博客}}\_maml算法},
  url = {https://blog.csdn.net/weixin_45521594/article/details/107980137},
  urldate = {2022-10-26},
  file = {D:\ProgramFiles\Zotero\storage\F4F6Q3ZY\107980137.html}
}

@online{yuGradientSurgeryMultiTask2020,
  title = {Gradient {{Surgery}} for {{Multi-Task Learning}}},
  author = {Yu, Tianhe and Kumar, Saurabh and Gupta, Abhishek and Levine, Sergey and Hausman, Karol and Finn, Chelsea},
  date = {2020-12-21},
  eprint = {2001.06782},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.2001.06782},
  url = {http://arxiv.org/abs/2001.06782},
  urldate = {2023-04-30},
  abstract = {While deep learning and deep reinforcement learning (RL) systems have demonstrated impressive results in domains such as image classification, game playing, and robotic control, data efficiency remains a major challenge. Multi-task learning has emerged as a promising approach for sharing structure across multiple tasks to enable more efficient learning. However, the multi-task setting presents a number of optimization challenges, making it difficult to realize large efficiency gains compared to learning tasks independently. The reasons why multi-task learning is so challenging compared to single-task learning are not fully understood. In this work, we identify a set of three conditions of the multi-task optimization landscape that cause detrimental gradient interference, and develop a simple yet general approach for avoiding such interference between task gradients. We propose a form of gradient surgery that projects a task's gradient onto the normal plane of the gradient of any other task that has a conflicting gradient. On a series of challenging multi-task supervised and multi-task RL problems, this approach leads to substantial gains in efficiency and performance. Further, it is model-agnostic and can be combined with previously-proposed multi-task architectures for enhanced performance.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Robotics,Statistics - Machine Learning},
  file = {C\:\\Users\\YeCanming\\Documents\\WPS Cloud Files\\200995647\\Zotero文献库\\人工智能\\创新实践\\多任务学习\\Yu et al_2020_Gradient Surgery for Multi-Task Learning.pdf;D\:\\ProgramFiles\\Zotero\\storage\\TYQ8CFNR\\2001.html}
}

@online{zakenBitFitSimpleParameterefficient2022,
  title = {{{BitFit}}: {{Simple Parameter-efficient Fine-tuning}} for {{Transformer-based Masked Language-models}}},
  shorttitle = {{{BitFit}}},
  author = {Zaken, Elad Ben and Ravfogel, Shauli and Goldberg, Yoav},
  date = {2022-09-05},
  eprint = {2106.10199},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2106.10199},
  url = {http://arxiv.org/abs/2106.10199},
  urldate = {2024-05-12},
  abstract = {We introduce BitFit, a sparse-finetuning method where only the bias-terms of the model (or a subset of them) are being modified. We show that with small-to-medium training data, applying BitFit on pre-trained BERT models is competitive with (and sometimes better than) fine-tuning the entire model. For larger data, the method is competitive with other sparse fine-tuning methods. Besides their practical utility, these findings are relevant for the question of understanding the commonly-used process of finetuning: they support the hypothesis that finetuning is mainly about exposing knowledge induced by language-modeling training, rather than learning new task-specific linguistic knowledge.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {⛔ No INSPIRE recid found,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {D\:\\Zotero文献库\\00人工智能\\00机器学习\\学习范式\\迁移学习\\PEFT\\selection\\Zaken et al_2022_BitFit.pdf;D\:\\ProgramFiles\\Zotero\\storage\\BTID526V\\2106.html}
}

@inproceedings{zhangAdaptiveBudgetAllocation2022,
  title = {Adaptive {{Budget Allocation}} for {{Parameter-Efficient Fine-Tuning}}},
  author = {Zhang, Qingru and Chen, Minshuo and Bukharin, Alexander and He, Pengcheng and Cheng, Yu and Chen, Weizhu and Zhao, Tuo},
  date = {2022-09-29},
  url = {https://openreview.net/forum?id=lq62uWRJjiY},
  urldate = {2024-05-10},
  abstract = {Fine-tuning large pre-trained language models on downstream tasks has become an important paradigm in NLP. However, common practice fine-tunes all of the parameters in a pre-trained model, which becomes prohibitive when a large number of downstream tasks are present. Therefore, many fine-tuning methods are proposed to learn incremental updates of pre-trained weights in a parameter efficient way, e.g., low-rank increments. These methods often evenly distribute the budget of incremental updates across all pre-trained weight matrices, and overlook the varying importance of different weight parameters. As a consequence, the fine-tuning performance is suboptimal. To bridge this gap, we propose AdaLoRA, which adaptively allocates the parameter budget among weight matrices according to their importance score. In particular, AdaLoRA parameterizes the incremental updates in the form of singular value decomposition. Such a novel approach allows us to effectively prune the singular values of unimportant updates, which is essentially to reduce their parameter budget but circumvent intensive exact SVD computations. We conduct extensive experiments with several pre-trained models on natural language processing, question answering, and natural language generation to validate the effectiveness of AdaLoRA. Results demonstrate that AdaLoRA manifests notable improvement over baselines, especially in the low budget settings. Our code is publicly available at https://github.com/QingruZhang/AdaLoRA .},
  eventtitle = {The {{Eleventh International Conference}} on {{Learning Representations}}},
  langid = {english},
  keywords = {⛔ No INSPIRE recid found},
  file = {D:\Zotero文献库\00人工智能\00机器学习\学习范式\迁移学习\PEFT\lora类\Zhang et al_2022_Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning.pdf}
}

@article{zhangConvexFormulationLearning,
  title = {A {{Convex Formulation}} for {{Learning Task Relationships}} in {{Multi-Task Learning}}},
  author = {Zhang, Yu and Yeung, Dit-Yan},
  abstract = {Multi-task learning is a learning paradigm which seeks to improve the generalization performance of a learning task with the help of some other related tasks. In this paper, we propose a regularization formulation for learning the relationships between tasks in multi-task learning. This formulation can be viewed as a novel generalization of the regularization framework for single-task learning. Besides modeling positive task correlation, our method, called multi-task relationship learning (MTRL), can also describe negative task correlation and identify outlier tasks based on the same underlying principle. Under this regularization framework, the objective function of MTRL is convex. For efficiency, we use an alternating method to learn the optimal model parameters for each task as well as the relationships between tasks. We study MTRL in the symmetric multi-task learning setting and then generalize it to the asymmetric setting as well. We also study the relationships between MTRL and some existing multi-task learning methods. Experiments conducted on a toy problem as well as several benchmark data sets demonstrate the effectiveness of MTRL.},
  langid = {english},
  file = {C:\Users\YeCanming\Documents\WPS Cloud Files\200995647\Zotero文献库\人工智能\创新实践\多任务学习\Zhang_Yeung_A Convex Formulation for Learning Task Relationships in Multi-Task Learning.pdf}
}

@online{zhangDynamicMalwareAnalysis2020,
  title = {Dynamic {{Malware Analysis}} with {{Feature Engineering}} and {{Feature Learning}}},
  author = {Zhang, Zhaoqi and Qi, Panpan and Wang, Wei},
  date = {2020-01-23},
  eprint = {1907.07352},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1907.07352},
  urldate = {2023-10-19},
  abstract = {Dynamic malware analysis executes the program in an isolated environment and monitors its run-time behaviour (e.g. system API calls) for malware detection. This technique has been proven to be effective against various code obfuscation techniques and newly released (“zero-day”) malware. However, existing works typically only consider the API name while ignoring the arguments, or require complex feature engineering operations and expert knowledge to process the arguments. In this paper, we propose a novel and low-cost feature extraction approach, and an effective deep neural network architecture for accurate and fast malware detection. Specifically, the feature representation approach utilizes a feature hashing trick to encode the API call arguments associated with the API name. The deep neural network architecture applies multiple Gated-CNNs (convolutional neural networks) to transform the extracted features of each API call. The outputs are further processed through bidirectional LSTM (long-short term memory networks) to learn the sequential correlation among API calls. Experiments show that our solution outperforms baselines significantly on a large real dataset. Valuable insights about feature engineering and architecture design are derived from the ablation study.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Computer Science - Cryptography and Security,Computer Science - Machine Learning},
  file = {C:\Users\YeCanming\Documents\WPS Cloud Files\200995647\Zotero文献库\人工智能\医学人工智能\AI知识查缺补漏\特征工程\Zhang et al_2020_Dynamic Malware Analysis with Feature Engineering and Feature Learning.pdf}
}

@article{zhangHOBANovelFeature2021,
  title = {{{HOBA}}: {{A}} Novel Feature Engineering Methodology for Credit Card Fraud Detection with a Deep Learning Architecture},
  shorttitle = {{{HOBA}}},
  author = {Zhang, Xinwei and Han, Yaoci and Xu, Wei and Wang, Qili},
  date = {2021-05},
  journaltitle = {Information Sciences},
  shortjournal = {Information Sciences},
  volume = {557},
  pages = {302--316},
  issn = {00200255},
  doi = {10.1016/j.ins.2019.05.023},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S002002551930427X},
  urldate = {2023-10-19},
  abstract = {Credit card transaction fraud costs billions of dollars to card issuers every year. A well-developed fraud detection system with a state-of-the-art fraud detection model is regarded as essential to reducing fraud losses. The main contribution of our work is the development of a fraud detection system that employs a deep learning architecture together with an advanced feature engineering process based on homogeneity-oriented behavior analysis (HOBA). Based on a real-life dataset from one of the largest commercial banks in China, we conduct a comparative study to assess the effectiveness of the proposed framework. The experimental results illustrate that our proposed methodology is an effective and feasible mechanism for credit card fraud detection. From a practical perspective, our proposed method can identify relatively more fraudulent transactions than the benchmark methods under an acceptable false positive rate. The managerial implication of our work is that credit card issuers can apply the proposed methodology to efficiently identify fraudulent transactions to protect customers’ interests and reduce fraud losses and regulatory costs.},
  langid = {english},
  file = {C:\Users\YeCanming\Documents\WPS Cloud Files\200995647\Zotero文献库\人工智能\医学人工智能\AI知识查缺补漏\特征工程\Zhang et al_2021_HOBA.pdf}
}

@online{zhangMakingPretrainedLanguage2022,
  title = {Making {{Pretrained Language Models Good Long-tailed Learners}}},
  author = {Zhang, Chen and Ren, Lei and Wang, Jingang and Wu, Wei and Song, Dawei},
  date = {2022-10-10},
  eprint = {2205.05461},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2205.05461},
  urldate = {2023-04-27},
  abstract = {Prompt-tuning has shown appealing performance in few-shot classification by virtue of its capability in effectively exploiting pretrained knowledge. This motivates us to check the hypothesis that prompt-tuning is also a promising choice for long-tailed classification, since the tail classes are intuitively fewshot ones. To achieve this aim, we conduct empirical studies to examine the hypothesis. The results demonstrate that prompt-tuning exactly makes pre-trained language models at least good long-tailed learners. For intuitions on why prompt-tuning can achieve good performance in long-tailed classification, we carry out an in-depth analysis by progressively bridging the gap between prompt-tuning and commonly used fine-tuning. The summary is that the classifier structure and parameterization form the key to making good long-tailed learners, in comparison with the less important input structure. Finally, we verify the applicability of our finding to few-shot classification.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {D:\ProgramFiles\Zotero\storage\U2TAMERK\Zhang 等 - 2022 - Making Pretrained Language Models Good Long-tailed.pdf}
}

@online{zhangSideTuningBaselineNetwork2020,
  title = {Side-{{Tuning}}: {{A Baseline}} for {{Network Adaptation}} via {{Additive Side Networks}}},
  shorttitle = {Side-{{Tuning}}},
  author = {Zhang, Jeffrey O. and Sax, Alexander and Zamir, Amir and Guibas, Leonidas and Malik, Jitendra},
  date = {2020-07-30},
  eprint = {1912.13503},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1912.13503},
  urldate = {2024-08-29},
  abstract = {When training a neural network for a desired task, one may prefer to adapt a pre-trained network rather than starting from randomly initialized weights. Adaptation can be useful in cases when training data is scarce, when a single learner needs to perform multiple tasks, or when one wishes to encode priors in the network. The most commonly employed approaches for network adaptation are fine-tuning and using the pre-trained network as a fixed feature extractor, among others.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {⛔ No INSPIRE recid found,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Computer Science - Robotics},
  file = {D:\ProgramFiles\Zotero\storage\6HH923SU\Zhang 等 - 2020 - Side-Tuning A Baseline for Network Adaptation via.pdf}
}

@online{zhangSurveyMultiTaskLearning2021,
  title = {A {{Survey}} on {{Multi-Task Learning}}},
  author = {Zhang, Yu and Yang, Qiang},
  date = {2021-03-29},
  eprint = {1707.08114},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1707.08114},
  urldate = {2022-09-10},
  abstract = {Multi-Task Learning (MTL) is a learning paradigm in machine learning and its aim is to leverage useful information contained in multiple related tasks to help improve the generalization performance of all the tasks. In this paper, we give a survey for MTL. First, we classify different MTL algorithms into several categories, including feature learning approach, low-rank approach, task clustering approach, task relation learning approach, and decomposition approach, and then discuss the characteristics of each approach. In order to improve the performance of learning tasks further, MTL can be combined with other learning paradigms including semi-supervised learning, active learning, unsupervised learning, reinforcement learning, multi-view learning and graphical models. When the number of tasks is large or the data dimensionality is high, batch MTL models are difficult to handle this situation and online, parallel and distributed MTL models as well as dimensionality reduction and feature hashing are reviewed to reveal their computational and storage advantages. Many real-world applications use MTL to boost their performance and we review representative works.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {C:\Users\YeCanming\Documents\WPS Cloud Files\200995647\Zotero文献库\人工智能\创新实践\迁移学习\Zhang_Yang_2021_A Survey on Multi-Task Learning.pdf}
}

@online{ZhangTianPingOpenFEQuanZiDongTeZhengShengChengQiZhuLiKaggleJingSaiQuDeGengHaoChengJi2022,
  type = {知乎专栏文章},
  title = {【OpenFE】全自动特征生成器 --- 助力Kaggle竞赛取得更好成绩},
  author = {张天平},
  date = {2022-11-29T06:01:37},
  url = {https://zhuanlan.zhihu.com/p/587075465},
  urldate = {2023-11-22},
  abstract = {【OpenFE】全自动特征生成器 --- 助力Kaggle竞赛取得更好成绩 - 来自知乎专栏「FHZ的机器学习笔记」，作者: 张天平 https://zhuanlan.zhihu.com/p/587075465},
  langid = {chinese},
  organization = {FHZ的机器学习笔记},
  keywords = {Kaggle,特征工程,特征提取},
  annotation = {赞数:329;},
  file = {D:\ProgramFiles\Zotero\storage\FYXXQW96\587075465.html}
}

@inproceedings{zhaoOnlineAUCMaximization2011,
  title = {Online {{AUC}} Maximization},
  author = {Zhao, Peilin and Hoi, Steven and Jin, Rong and Yang, Tianbao},
  date = {2011-01-01},
  pages = {233--240},
  abstract = {Most studies of online learning measure the performance of a learner by classification accuracy, which is inappropriate for applications where the data are unevenly distributed among different classes. We address this limitation by developing online learning algorithm for maximizing Area Under the ROC curve (AUC), a metric that is widely used for measuring the classification performance for imbalanced data distributions. The key challenge of online AUC maximization is that it needs to optimize the pairwise loss between two instances from different classes. This is in contrast to the classical setup of online learning where the overall loss is a sum of losses over individual training examples. We address this challenge by exploiting the reservoir sampling technique, and present two algorithms for online AUC maximization with theoretic performance guarantee. Extensive experimental studies confirm the effectiveness and the efficiency of the proposed algorithms for maximizing AUC.},
  eventtitle = {Proceedings of the 28th {{International Conference}} on {{Machine Learning}}, {{ICML}} 2011},
  file = {C:\Users\YeCanming\Documents\WPS Cloud Files\200995647\Zotero文献库\人工智能\创新实践\损失函数\Zhao et al_2011_Online AUC maximization.pdf}
}

@software{zhaoPythonOutlierDetection2023,
  title = {Python {{Outlier Detection}} ({{PyOD}})},
  author = {Zhao, Yue},
  date = {2023-11-09T14:39:52Z},
  origdate = {2017-10-03T20:29:04Z},
  url = {https://github.com/yzhao062/pyod},
  urldate = {2023-11-09},
  abstract = {A Comprehensive and Scalable Python Library for Outlier Detection (Anomaly Detection)},
  keywords = {anomaly,anomaly-detection,autoencoder,data-analysis,data-mining,data-science,deep-learning,fraud-detection,machine-learning,neural-networks,novelty-detection,out-of-distribution-detection,outlier-detection,outlier-ensembles,outliers,python,python2,python3,unsupervised-learning}
}

@inproceedings{zhongConvolutionMeetsLoRA2023,
  title = {Convolution {{Meets LoRA}}: {{Parameter Efficient Finetuning}} for {{Segment Anything Model}}},
  shorttitle = {Convolution {{Meets LoRA}}},
  author = {Zhong, Zihan and Tang, Zhiqiang and He, Tong and Fang, Haoyang and Yuan, Chun},
  date = {2023-10-13},
  url = {https://openreview.net/forum?id=ezscMer8L0},
  urldate = {2024-03-11},
  abstract = {The Segment-Anything Model (SAM) stands as a foundational framework for image segmentation. While it exhibits remarkable zero-shot generalization in typical scenarios, its advantage diminishes when applied to specialized domains like medical imagery and remote sensing. To address this limitation, this paper introduces Conv-LoRA, a simple yet effective parameter-efficient fine-tuning approach. By integrating ultra-lightweight convolutional parameters into Low-Rank Adaptation (LoRA), Conv-LoRA can inject image-related inductive biases into the plain ViT encoder, further reinforcing SAM’s local prior assumption. Notably, Conv-LoRA not only preserves SAM’s extensive segmentation knowledge but also revives its capacity of learning high-level image semantics, which is constrained by SAM’s foreground-background segmentation pretraining. Comprehensive experimentation across diverse benchmarks spanning multiple domains underscores Conv-LoRA’s superiority in adapting SAM to real-world semantic segmentation tasks.},
  eventtitle = {The {{Twelfth International Conference}} on {{Learning Representations}}},
  langid = {english},
  keywords = {⛔ No INSPIRE recid found},
  file = {D:\Zotero文献库\00人工智能\00计算机视觉课程\经典视觉任务的对应设计\视觉地点定位\创新点\PEFT\Zhong et al_2023_Convolution Meets LoRA.pdf}
}

@book{zhouMachineLearning2021,
  title = {Machine {{Learning}}},
  author = {Zhou, Zhi-Hua},
  date = {2021},
  publisher = {Springer},
  url = {http://gen.lib.rus.ec/book/index.php?md5=2A726599FA108C7DD223FA8D32E84F4B},
  urldate = {2024-03-03},
  isbn = {9789811519666},
  langid = {english},
  keywords = {⛔ No INSPIRE recid found}
}

@book{ZhouZhiHuaJiQiXueXi2016,
  title = {机器学习},
  shorttitle = {《机器学习》},
  author = {周志华},
  date = {2016-01-01},
  series = {清华社人工智能系列},
  publisher = {清华大学出版社},
  url = {https://book.douban.com/subject/26708119/},
  urldate = {2022-07-16},
  abstract = {作者简介: 周志华，南京大学教授，计算机科学与技术系副主任，软件新技术国家重点实验室常务副主任，机器学习与数据挖掘研究所（LAMDA）所长，校、系学术委员会委员；ACM杰出科学家，IEEE Fellow，IAPR Fellow，中国计算机学会会士；长江学者特聘教授，国家杰出青年基金获得者。2007年创建南京大学机器学习与数据挖掘研究所（LAMDA），2010年11月任软件新技术国家重点实验室常务副主任，2013年5月任计算机系副主任。 内容简介: 机器学习是计算机科学与人工智能的重要分支领域。本书作为该领域的入门教材，在内容上尽可能涵盖机器学习基础知识的各方面。为了使尽可能多的读者通过本书对机器学习有所了解，作者试图尽可能少地使用数学知识。然而，少量的概率、统计、代数、优化、逻辑知识似乎不可避免。因此，本书更适合大学三年级以上的理工科本科生和研究生，以及具有类似背景的对机器学习感兴趣的人士。为方便读者，本书附录给出了一些相关数学基础知识简介。 全书共16章，大致分为3个部分：第1部分（第1～3章）介绍机器学习的基础知识；第2部分（第4～10章）讨论一些经典而常用的机器学习方法（决策树、神经网络、支持向量机、贝叶斯分类器、集成学习、聚类、降维与度量学习）；第3部分（第11～16章）为进阶知识，内容涉及特征选择与稀疏学习、计算学习理论、半监督学习、概率图模型、规则学习以及强化学习等。前3章之外的后续各章均相对独立，读者可根据自己的兴趣和时间情况选择使用。根据课时情况，一个学期的本科生课程可考虑讲授前9章或前10章;研究生课程则不妨使用全书。 书中除第1章外，每章都给出了十道习题。有的习题是帮助读者巩固本章学习，有的是为了引导读者扩展相关知识。一学期的一般课程可使用这些习题，再辅以两到三个针对具体数据集的大作业。带星号的习题则有相当难度，有些并无现成答案，谨供富有进取心的读者启发思考。 本书可作为高等院校计算机、自动化及相关专业的本科生或研究生教材，也可供对机器学习感兴趣的研究人员和工程技术人员阅读参考。},
  isbn = {978-7-302-42328-7},
  langid = {chinese},
  pagetotal = {425},
  annotation = {《机器学习》;👩‍⚖️3239;🔟8.6;88.00 元;},
  file = {C:\Users\YeCanming\Documents\WPS Cloud Files\200995647\Zotero文献库\人工智能\机器学习课程\周志华_2016_机器学习.pdf}
}
